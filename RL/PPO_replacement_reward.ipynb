{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "PJfT8e2MPYnj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PJfT8e2MPYnj",
        "outputId": "3089a506-2028-4702-b8b9-9855d4aec0aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypsa in /usr/local/lib/python3.12/dist-packages (0.35.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pypsa) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pypsa) (1.16.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from pypsa) (2.2.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (from pypsa) (2025.8.0)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.12/dist-packages (from pypsa) (1.7.2)\n",
            "Requirement already satisfied: linopy>=0.4 in /usr/local/lib/python3.12/dist-packages (from pypsa) (0.5.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pypsa) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from pypsa) (5.24.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from pypsa) (0.13.2)\n",
            "Requirement already satisfied: geopandas>=0.9 in /usr/local/lib/python3.12/dist-packages (from pypsa) (1.1.1)\n",
            "Requirement already satisfied: shapely<2.1 in /usr/local/lib/python3.12/dist-packages (from pypsa) (2.0.7)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.12/dist-packages (from pypsa) (3.5)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.12/dist-packages (from pypsa) (2.1.0)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.12/dist-packages (from pypsa) (0.35.0)\n",
            "Requirement already satisfied: highspy in /usr/local/lib/python3.12/dist-packages (from pypsa) (1.11.0)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.9->pypsa) (0.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.9->pypsa) (25.0)\n",
            "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.9->pypsa) (3.7.2)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (1.4.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (0.12.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (2.11.0)\n",
            "Requirement already satisfied: dask>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (2025.5.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->pypsa) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->pypsa) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->pypsa) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (3.2.3)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.12/dist-packages (from netcdf4->pypsa) (1.6.4.post1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netcdf4->pypsa) (2025.8.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->pypsa) (8.5.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (2025.3.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (6.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24->pypsa) (1.17.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask>=0.18.0->linopy>=0.4->pypsa) (1.0.0)\n",
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.12/dist-packages (1.14.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (3.1.45)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (11.3.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.12/dist-packages (from neptune-client) (2.10.1)\n",
            "Requirement already satisfied: boto3>=1.28.0 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (1.40.21)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (11.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (8.2.1)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (1.0.0)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (3.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from neptune-client) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from neptune-client) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from neptune-client) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (2.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (1.17.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (4.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (2.5.0)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.12/dist-packages (from neptune-client) (1.8.0)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.21 in /usr/local/lib/python3.12/dist-packages (from boto3>=1.28.0->neptune-client) (1.40.21)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3>=1.28.0->neptune-client) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from boto3>=1.28.0->neptune-client) (0.13.1)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.12/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.1.1)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.12/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.6)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.0.2)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.12/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.20.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.12)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->neptune-client) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->neptune-client) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->neptune-client) (2025.8.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.25.1)\n",
            "Requirement already satisfied: importlib-resources>=1.3 in /usr/local/lib/python3.12/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->neptune-client) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->neptune-client) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->neptune-client) (2025.2)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.12/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.27.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (0.1.1)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (24.11.1)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.2.2)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.12/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (2.9.0.20250822)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypsa\n",
        "!pip install neptune-client\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ce4a5972",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce4a5972",
        "outputId": "e4d48b87-4fed-4ff4-c2d1-277631fa6a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[neptune] [warning] NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs-legacy.neptune.ai/setup/upgrading/\n"
          ]
        }
      ],
      "source": [
        "import pypsa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "import gc\n",
        "import psutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import neptune\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a3ee85e6",
      "metadata": {
        "id": "a3ee85e6"
      },
      "outputs": [],
      "source": [
        "def calculate_offset_k_initialization(network_file, k_method='mean', k_samples=1000, **env_kwargs):\n",
        "  \"\"\"\n",
        "  Calculate the offset k for replacement reward method.\n",
        "  Creates a temporary environment to perform the calculation.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  network_file : str\n",
        "      Path to the PyPSA network file\n",
        "  input_dir : str\n",
        "      Directory containing constraint mappings\n",
        "  k_method : str\n",
        "      Method to calculate k: 'mean', 'worst_case', or 'percentile'\n",
        "  k_samples : int\n",
        "      Number of random samples to use for estimation\n",
        "  **env_kwargs : dict\n",
        "      Additional keyword arguments for environment creation\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  float: Offset value k\n",
        "  \"\"\"\n",
        "  print(f\"Sampling {k_samples} random states to calculate offset k...\")\n",
        "\n",
        "  # Create temporary environment without offset_k (uses default)\n",
        "  temp_env = EnvDispatchReplacement(network_file=network_file, **env_kwargs)\n",
        "  #this initializes episode_length to number of snapshots and constraint_penalty_factor to None.\n",
        "  #I'm just making this env to access certain attributes/ methods; which should be fine since none of these attributes/methods reference these two parameters.\n",
        "\n",
        "  action_dim = temp_env.action_space.shape[0]\n",
        "\n",
        "  objective_values = []\n",
        "  successful_samples = 0\n",
        "  try:\n",
        "        for i in range(k_samples):\n",
        "            try:\n",
        "                # Reset environment to start fresh\n",
        "                temp_env.reset(seed=42 + i)  # Use different seeds for variety\n",
        "                # Sample random action\n",
        "                random_action = temp_env.action_space.sample()  # This ensures [0,1] range\n",
        "                # Take step - this handles all action scaling and application\n",
        "                # make sure to get snapshot before the step to use when call evaluate_objective_direc() again\n",
        "                #i increase the current snapshot_idx by executing step() if do this line after step, when evaluate_objective_direct() is run it evaluates the objective for the next step!\n",
        "                current_snapshot = temp_env.network.snapshots[temp_env.snapshot_idx]\n",
        "                obs, reward, terminated, truncated, info = temp_env.step(random_action)\n",
        "                # Get the base objective value (the -J(s) part, before any penalties or offsets)\n",
        "                obj_value = temp_env.evaluate_objective_direct(current_snapshot)\n",
        "                objective_values.append(obj_value)\n",
        "                successful_samples += 1\n",
        "                #print(info[\"constraint_violations\"])\n",
        "\n",
        "                # Progress indicator every 200 samples\n",
        "                if (i + 1) % 200 == 0:\n",
        "                    print(f\"  Completed {i + 1}/{k_samples} samples...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # Skip failed samples but continue\n",
        "                if i < 5:  # Only print first few errors to avoid spam\n",
        "                    print(f\"  Sample {i} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate offset based on method\n",
        "        if objective_values:\n",
        "            if k_method == 'worst_case':\n",
        "                k = abs(max(objective_values))\n",
        "                print(f\"  Using worst-case method: k = |{max(objective_values):.2f}| = {k:.2f}\")\n",
        "            else:  # method == 'mean'\n",
        "                mean_val = np.mean(objective_values)\n",
        "                k = abs(mean_val)\n",
        "                print(f\"  Using mean method: k = |{mean_val:.2f}| = {k:.2f}\")\n",
        "\n",
        "            print(f\"  Successfully sampled {successful_samples}/{k_samples} states\")\n",
        "            print(f\"  Objective value range: [{min(objective_values):.2f}, {max(objective_values):.2f}]\")\n",
        "        else:\n",
        "            print(\"  Warning: No successful samples, using default k value\")\n",
        "            k = 2500  # Default fallback value\n",
        "\n",
        "  except Exception as e:\n",
        "          print(f\"Error in offset calculation: {e}\")\n",
        "          import traceback\n",
        "          traceback.print_exc()\n",
        "          k = 2500  # Default fallback value\n",
        "  return k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9db9ca87",
      "metadata": {
        "id": "9db9ca87"
      },
      "outputs": [],
      "source": [
        "def fix_artificial_lines_reasonable(network):\n",
        "    \"\"\"\n",
        "    Fix artificial lines with reasonable capacity values:\n",
        "    - s_nom = based on connected bus demand (with safety factor)\n",
        "    - s_nom_extendable = False (non-extendable)\n",
        "    - Keep capacity high enough to meet demand\n",
        "    \"\"\"\n",
        "    print(\"=== FIXING ARTIFICIAL LINES WITH REASONABLE CAPACITY ===\")\n",
        "\n",
        "    # Find artificial lines\n",
        "    artificial_lines = [line for line in network.lines.index\n",
        "                       if any(keyword in str(line).lower() for keyword in ['new', '<->', 'artificial'])]\n",
        "\n",
        "    if not artificial_lines:\n",
        "        # If no artificial lines found by name, look for lines with s_nom=0\n",
        "        # which is often a sign of artificial lines\n",
        "        zero_capacity_lines = network.lines[network.lines.s_nom == 0].index.tolist()\n",
        "        if zero_capacity_lines:\n",
        "            artificial_lines = zero_capacity_lines\n",
        "\n",
        "    print(f\"Found {len(artificial_lines)} artificial lines to fix:\")\n",
        "\n",
        "    # Get maximum demand per bus across all snapshots\n",
        "    bus_max_demand = {}\n",
        "    for bus in network.buses.index:\n",
        "        bus_demand = 0\n",
        "        for load_name, load in network.loads.iterrows():\n",
        "            if load.bus == bus and load_name in network.loads_t.p_set.columns:\n",
        "                bus_demand = max(bus_demand, network.loads_t.p_set[load_name].max())\n",
        "        bus_max_demand[bus] = bus_demand\n",
        "\n",
        "    # Fix each artificial line with reasonable capacity\n",
        "    for line_name in artificial_lines:\n",
        "        # Get connected buses\n",
        "        bus0 = network.lines.loc[line_name, 'bus0']\n",
        "        bus1 = network.lines.loc[line_name, 'bus1']\n",
        "\n",
        "        # Get maximum demand at these buses\n",
        "        bus0_demand = bus_max_demand.get(bus0, 0)\n",
        "        bus1_demand = bus_max_demand.get(bus1, 0)\n",
        "\n",
        "        # Calculate required capacity with safety factor\n",
        "        # Use 3x the higher demand to ensure adequate capacity\n",
        "        safety_factor = 3.0\n",
        "        required_capacity = max(bus0_demand, bus1_demand) * safety_factor\n",
        "\n",
        "        # Ensure minimum reasonable capacity (1000 MW)\n",
        "        required_capacity = max(required_capacity, 1000)\n",
        "\n",
        "        print(f\"\\n🔧 Fixing: {line_name}\")\n",
        "        print(f\"    Connected buses: {bus0} ↔ {bus1}\")\n",
        "        print(f\"    Bus demands: {bus0}: {bus0_demand:.1f} MW, {bus1}: {bus1_demand:.1f} MW\")\n",
        "\n",
        "        # Set s_nom to required capacity\n",
        "        old_s_nom = network.lines.loc[line_name, 's_nom']\n",
        "        network.lines.loc[line_name, 's_nom'] = required_capacity\n",
        "        print(f\"    s_nom: {old_s_nom} → {required_capacity:.1f} MW\")\n",
        "\n",
        "        # Make sure line is not extendable\n",
        "        if 's_nom_extendable' not in network.lines.columns:\n",
        "            network.lines['s_nom_extendable'] = False\n",
        "        network.lines.loc[line_name, 's_nom_extendable'] = False\n",
        "        print(f\"    s_nom_extendable: → False\")\n",
        "\n",
        "    return network\n",
        "\n",
        "def create_pypsa_network(network_file):\n",
        "    \"\"\"Create a PyPSA network from the .nc file.\"\"\"\n",
        "    # Initialize network\n",
        "    network = pypsa.Network(network_file)\n",
        "    for storage_name in network.storage_units.index:\n",
        "        # Use .loc for direct assignment to avoid SettingWithCopyWarning\n",
        "        network.storage_units.loc[storage_name, 'cyclic_state_of_charge'] = False\n",
        "\n",
        "        # Set marginal_cost to 0.01\n",
        "        network.storage_units.loc[storage_name, 'marginal_cost'] = 0.01\n",
        "\n",
        "        # Set marginal_cost_storage to 0.01\n",
        "        network.storage_units.loc[storage_name, 'marginal_cost_storage'] = 0.01\n",
        "\n",
        "        # Set spill_cost to 0.1\n",
        "        network.storage_units.loc[storage_name, 'spill_cost'] = 0.1\n",
        "\n",
        "        # Fix unrealistic max_hours values\n",
        "        current_max_hours = network.storage_units.loc[storage_name, 'max_hours']\n",
        "\n",
        "        if 'PHS' in storage_name:\n",
        "            # PHS with missing data - set to typical range\n",
        "            network.storage_units.loc[storage_name, 'max_hours'] = 8.0\n",
        "            print(f\"Fixed {storage_name}: set max_hours to 8.0\")\n",
        "\n",
        "        elif 'hydro' in storage_name:\n",
        "            # Hydro with unrealistic data - set to validated range\n",
        "            network.storage_units.loc[storage_name, 'max_hours'] = 6.0\n",
        "            print(f\"Fixed {storage_name}: corrected max_hours from {current_max_hours} to 6.0\")\n",
        "\n",
        "\n",
        "    fix_artificial_lines_reasonable(network)\n",
        "\n",
        "    return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dbo-BPT6uEJC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbo-BPT6uEJC",
        "outputId": "d2da3a08-ffb6-406c-f056-48198b89dfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.12/dist-packages (0.13.13)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.12/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb) (0.19.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "#Install debugger\n",
        "!pip install ipdb\n",
        "import ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b8f1d307",
      "metadata": {
        "id": "b8f1d307"
      },
      "outputs": [],
      "source": [
        "import ipdb\n",
        "class EnvDispatchConstr(gym.Env):\n",
        "    \"\"\"\n",
        "    OpenAI Gym environment for Optimal Power Flow using PyPSA.\n",
        "    Enhanced to handle dispatchable generators, renewable generators, and storage units.\n",
        "\n",
        "    Action Space: Continuous setpoints for all controllable components within their capacity limits\n",
        "    - Dispatchable generators: scaled between p_min_pu*p_nom and p_max_pu*p_nom\n",
        "    - Renewable generators: scaled between 0 and current p_max_pu*p_nom (time-varying)\n",
        "    - Storage units: scaled between -p_nom (charging) and +p_nom (discharging)\n",
        "    (This follows http://arxiv.org/abs/2403.17831.)\n",
        "\n",
        "    Has train/test split functionality\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,network_file, constraint_penalty_factor=100, test_start_date='2013-12-01 00:00:00',\n",
        "                 fixed_episode_length=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.network_file = network_file # Store network file path\n",
        "\n",
        "        # Use provided network or create new one\n",
        "        self.network =create_pypsa_network(network_file)\n",
        "\n",
        "        self.test_start_date = pd.Timestamp(test_start_date)\n",
        "        self._train_test_snapshots()\n",
        "\n",
        "        #self._initialize_optimization_components()\n",
        "        self.penalty_factor=constraint_penalty_factor\n",
        "        self.reward_method = \"summation\" # Default reward method for the base class\n",
        "\n",
        "        # Handle episode length configuration\n",
        "        self.fixed_episode_length = fixed_episode_length\n",
        "        if self.fixed_episode_length is not None:\n",
        "            # Use fixed episode length, ensure it doesn't exceed training data\n",
        "            self.episode_length = min(self.fixed_episode_length, self.train_snapshots)\n",
        "            self.variable_episodes = False\n",
        "            print(f\"Using fixed episode length: {self.episode_length}\")\n",
        "        else:\n",
        "            # Use legacy episode_length_factor or default to full training set\n",
        "            self.episode_length = None\n",
        "            self.variable_episodes = True\n",
        "            print(f\"Using variable episode length, max: {self.episode_length}\")\n",
        "\n",
        "        # Episode management\n",
        "        self.current_step = 0  # Steps within current episode\n",
        "        self.snapshot_idx = 0  # Current snapshot index (cycles through all snapshots)\n",
        "\n",
        "        # Initialize component categorization\n",
        "        self._categorize_components()\n",
        "\n",
        "        # Create action space\n",
        "        self._create_action_space()\n",
        "\n",
        "        # Initialize the network state\n",
        "        self.reset()\n",
        "\n",
        "        # Create observation space\n",
        "        low_bounds, high_bounds = self.create_observation_bounds()\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=low_bounds,\n",
        "            high=high_bounds,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        #TO DO: If use .pf instead of .lpf, add another of each term for active power AND reactive power\n",
        "\n",
        "    # def _initialize_optimization_components(self):\n",
        "    #     \"\"\"\n",
        "    #     Initialize all optimization components in one pass to avoid creating multiple models.\n",
        "    #     This method:\n",
        "    #     1. Creates the optimization model once\n",
        "    #     2. Extracts objective components (vars, coeffs, const)\n",
        "    #     3. Creates the variable ID to name mapping\n",
        "    #     4. Extracts constraints\n",
        "    #     5. Cleans up the model\n",
        "    #     \"\"\"\n",
        "    #     # Create model once - this is an expensive operation\n",
        "    #     temp_model = self.network.optimize.create_model()\n",
        "\n",
        "    #     # Extract objective components\n",
        "    #     obj_expr = temp_model.objective\n",
        "    #     objective_coeffs = obj_expr.coeffs.copy()\n",
        "    #     self.coeffs_flat = objective_coeffs.values.flatten()\n",
        "    #     self.objective_const = obj_expr.const.copy() if hasattr(obj_expr, 'const') else 0\n",
        "\n",
        "    #     self.variable_names_dict= load_dictionary(network_file=self.network_file, dict_name=\"variable_names_dict\", input_dir=self.input_dir)\n",
        "    #     self.var_indices_dict= load_dictionary(network_file=self.network_file, dict_name=\"var_indices_dict\", input_dir=self.input_dir)\n",
        "\n",
        "    #     # Clean up to free memory\n",
        "    #     del temp_model, obj_expr\n",
        "    #     gc.collect()\n",
        "\n",
        "    def _train_test_snapshots(self):\n",
        "      # Calculate train/test split based on specific start date\n",
        "        self.total_snapshots = len(self.network.snapshots)\n",
        "\n",
        "        self.train_snapshots = self.network.snapshots.get_loc(self.test_start_date)\n",
        "        nearest_idx = self.network.snapshots.get_indexer([self.test_start_date], method='nearest')[0]\n",
        "        self.train_snapshots = nearest_idx\n",
        "        actual_test_start = self.network.snapshots[nearest_idx]\n",
        "        print(f\"Exact test start date not found. Using nearest: {actual_test_start}\")\n",
        "\n",
        "        self.test_snapshots = self.total_snapshots - self.train_snapshots\n",
        "\n",
        "        # Ensure we have enough data\n",
        "        if self.train_snapshots <= 0:\n",
        "            raise ValueError(f\"Test start date {self.test_start_date} is too early. \"\n",
        "                           f\"No training data available.\")\n",
        "        if self.test_snapshots <= 0:\n",
        "            raise ValueError(f\"Test start date {self.test_start_date} is too late. \"\n",
        "                           f\"No test data available.\")\n",
        "\n",
        "    def _categorize_components(self):\n",
        "        \"\"\"\n",
        "        Categorize generators and identify storage units for action space.\n",
        "        \"\"\"\n",
        "        # Get generators with time-varying p_max_pu (renewable generators)\n",
        "        renewable_gens = self.network.generators_t.p_max_pu.columns\n",
        "\n",
        "        slack_generators = self.network.generators[self.network.generators.control == \"Slack\"].index\n",
        "        # in the 10-node SA network there are 4 slack gens so this should return a list of indexes\n",
        "\n",
        "        # Dispatchable generators: not slack, not renewable\n",
        "        self.dispatchable_gens = self.network.generators[\n",
        "            (~self.network.generators.index.isin(slack_generators)) &\n",
        "            (~self.network.generators.index.isin(renewable_gens))\n",
        "        ].index\n",
        "\n",
        "        # Renewable generators: have time-varying p_max_pu, not slack\n",
        "        self.renewable_gens = self.network.generators[\n",
        "            (self.network.generators.index.isin(renewable_gens)) &\n",
        "            (~self.network.generators.index.isin(slack_generators))\n",
        "        ].index\n",
        "\n",
        "        # Storage units (if any exist in the network)\n",
        "        self.storage_units = self.network.storage_units.index\n",
        "\n",
        "        # Store names as lists for easier indexing\n",
        "        self.dispatchable_names = list(self.dispatchable_gens)\n",
        "        self.renewable_names = list(self.renewable_gens)\n",
        "        self.storage_names = list(self.storage_units)\n",
        "\n",
        "        # Store counts\n",
        "        self.n_dispatchable = len(self.dispatchable_names)\n",
        "        self.n_renewable = len(self.renewable_names)\n",
        "        self.n_storage = len(self.storage_names)\n",
        "\n",
        "        # Get static limits for dispatchable generators\n",
        "        if self.n_dispatchable > 0:\n",
        "            dispatchable_df = self.network.generators.loc[self.dispatchable_gens]\n",
        "            self.disp_p_min = (dispatchable_df.p_min_pu * dispatchable_df.p_nom).values#returns numpy arrays\n",
        "            self.disp_p_max = (dispatchable_df.p_max_pu * dispatchable_df.p_nom).values\n",
        "        else:\n",
        "            self.disp_p_min = np.array([])\n",
        "            self.disp_p_max = np.array([])\n",
        "\n",
        "        # Get nominal capacities and minimum limits for renewable generators\n",
        "        if self.n_renewable > 0:\n",
        "            renewable_df = self.network.generators.loc[self.renewable_gens]\n",
        "            self.renewable_p_nom = renewable_df.p_nom.values\n",
        "            self.renewable_p_min_pu = renewable_df.p_min_pu.values\n",
        "        else:\n",
        "            self.renewable_p_nom = np.array([])\n",
        "            self.renewable_p_min_pu = np.array([])\n",
        "\n",
        "        # Get storage unit capacities\n",
        "        if self.n_storage > 0:\n",
        "            storage_df = self.network.storage_units.loc[self.storage_units]\n",
        "            #this is a bit redundant since self.storage_units is the array of all indices of self.network.storage_units but leave it in so could replace which indices you want\n",
        "            self.storage_p_nom = storage_df.p_nom.values\n",
        "        else:\n",
        "            self.storage_p_nom = np.array([])\n",
        "\n",
        "\n",
        "    def _create_action_space(self):\n",
        "        \"\"\"\n",
        "        Create action space with four distinct parts:\n",
        "        1. Dispatchable generators: [0,1] scaled to [p_min, p_max]\n",
        "        2. Renewable generators: [0,1] scaled to [0, current_p_max_pu * p_nom]\n",
        "        3. Storage p_set: [0,1] scaled to [-p_nom, +p_nom] (negative=charging, positive=discharging)\n",
        "        4. Storage p_dispatch: [0,1] scaled to [0, p_nom] (discharging magnitude)\n",
        "        \"\"\"\n",
        "        total_actions = self.n_dispatchable + self.n_renewable + (2 * self.n_storage)  # 2 actions per storage unit\n",
        "        self.action_space = gym.spaces.Box(0, 1, shape=(total_actions,))\n",
        "\n",
        "        # Store action space structure for easy reference\n",
        "        self.action_structure = {\n",
        "            'dispatchable': {\n",
        "                'start': 0,\n",
        "                'end': self.n_dispatchable,\n",
        "                'count': self.n_dispatchable\n",
        "            },\n",
        "            'renewable': {\n",
        "                'start': self.n_dispatchable,\n",
        "                'end': self.n_dispatchable + self.n_renewable,\n",
        "                'count': self.n_renewable\n",
        "            },\n",
        "            'storage_p_set': {\n",
        "                'start': self.n_dispatchable + self.n_renewable,\n",
        "                'end': self.n_dispatchable + self.n_renewable + self.n_storage,\n",
        "                'count': self.n_storage\n",
        "            },\n",
        "            'storage_p_dispatch': {\n",
        "                'start': self.n_dispatchable + self.n_renewable + self.n_storage,\n",
        "                'end': self.n_dispatchable + self.n_renewable + (2 * self.n_storage),\n",
        "                'count': self.n_storage\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _get_storage_observation(self):\n",
        "        \"\"\"\n",
        "        Get current storage unit states for observation.\n",
        "        Returns previous SOC (normalized) and current inflow (normalized) for each storage unit.\n",
        "        \"\"\"\n",
        "        if self.n_storage == 0:\n",
        "            return np.array([])\n",
        "\n",
        "        current_snapshot = self.network.snapshots[self.snapshot_idx]\n",
        "        storage_obs = []\n",
        "\n",
        "        for storage_name in self.storage_names:\n",
        "            # 1. Previous State of Charge (using your exact logic)\n",
        "            if self.snapshot_idx == 0:\n",
        "                # For first snapshot, previous SOC is the initial value\n",
        "                soc_prev = self.network.storage_units.state_of_charge_initial.loc[storage_name]\n",
        "            else:\n",
        "                previous_snapshot = self.network.snapshots[self.snapshot_idx - 1]\n",
        "                soc_prev = self.network.storage_units_t.state_of_charge.loc[previous_snapshot, storage_name]\n",
        "\n",
        "            # Get SOC limit from PyPSA parameters\n",
        "            p_nom = self.network.storage_units.loc[storage_name, 'p_nom']\n",
        "            max_hours = self.network.storage_units.loc[storage_name, 'max_hours']\n",
        "            max_soc = p_nom * max_hours  # SOC limit (energy capacity)\n",
        "\n",
        "            # Normalize SOC by its maximum possible value\n",
        "            normalized_soc_prev = soc_prev / max_soc if max_soc > 0 else 0\n",
        "            storage_obs.append(normalized_soc_prev)\n",
        "\n",
        "            # 2. Current Inflow (normalized by p_nom)\n",
        "            if hasattr(self.network.storage_units_t, 'inflow') and storage_name in self.network.storage_units_t.inflow.columns:\n",
        "                current_inflow = self.network.storage_units_t.inflow.loc[current_snapshot, storage_name]\n",
        "                # Normalize by p_nom for consistent scaling\n",
        "                normalized_inflow = current_inflow / p_nom if p_nom > 0 else 0\n",
        "            else:\n",
        "                # If no inflow data exists, use zero\n",
        "                normalized_inflow = 0.0\n",
        "            storage_obs.append(normalized_inflow)\n",
        "\n",
        "        return np.array(storage_obs, dtype=np.float32)\n",
        "\n",
        "    def create_storage_observation_bounds(self):\n",
        "        \"\"\"\n",
        "        Create bounds for storage unit observations.\n",
        "        Since no spill: SOC bounds are always [0, 1] when normalized.\n",
        "        \"\"\"\n",
        "        if self.n_storage == 0:\n",
        "            return np.array([]), np.array([])\n",
        "\n",
        "        values_per_storage = 2  # SOC + inflow\n",
        "        total_storage_obs = self.n_storage * values_per_storage\n",
        "\n",
        "        low_bounds = np.zeros(total_storage_obs)\n",
        "        high_bounds = np.zeros(total_storage_obs)\n",
        "\n",
        "        for i, storage_name in enumerate(self.storage_names):\n",
        "            base_idx = i * values_per_storage\n",
        "\n",
        "            # SOC bounds: Always [0, 1] when normalized by (p_nom * max_hours)\n",
        "            low_bounds[base_idx] = 0.0      # Normalized SOC min\n",
        "            high_bounds[base_idx] = 1.0     # Normalized SOC max\n",
        "\n",
        "            # Inflow bounds: Get from historical data\n",
        "            if hasattr(self.network.storage_units_t, 'inflow'):\n",
        "                p_nom = self.network.storage_units.loc[storage_name, 'p_nom']\n",
        "                if storage_name in self.network.storage_units_t.inflow.columns:\n",
        "                    inflow_data = self.network.storage_units_t.inflow[storage_name]\n",
        "\n",
        "                    min_inflow_norm = inflow_data.min() / p_nom if p_nom > 0 else 0\n",
        "                    max_inflow_norm = inflow_data.max() / p_nom if p_nom > 0 else 0\n",
        "\n",
        "                    low_bounds[base_idx + 1] = min_inflow_norm\n",
        "                    high_bounds[base_idx + 1] = max_inflow_norm\n",
        "                else:\n",
        "                     # If inflow data exists but not for this specific storage unit\n",
        "                    low_bounds[base_idx + 1] = 0.0\n",
        "                    high_bounds[base_idx + 1] = 0.0\n",
        "            else:\n",
        "                # No inflow data\n",
        "                low_bounds[base_idx + 1] = 0.0\n",
        "                high_bounds[base_idx + 1] = 0.0\n",
        "\n",
        "        return low_bounds.astype(np.float32), high_bounds.astype(np.float32)\n",
        "\n",
        "    def create_observation_bounds(self):\n",
        "        \"\"\"\n",
        "        Create bounds for the observation space based on:\n",
        "        - Load p_set values\n",
        "        - Renewable generator p_max_pu values\n",
        "        - Storage unit previous SOC (normalized) and current inflow (normalized)\n",
        "        \"\"\"\n",
        "        # 1. Load bounds\n",
        "        load_p_set_all = self.network.loads_t.p_set  # DataFrame with all snapshots and loads\n",
        "        load_low_bounds = load_p_set_all.min(axis=0).values  # Min across all snapshots for each load\n",
        "        load_high_bounds = load_p_set_all.max(axis=0).values  # Max across all snapshots for each load\n",
        "\n",
        "        # 2. Renewable generator bounds\n",
        "        if self.n_renewable > 0:\n",
        "            renewable_p_max_pu_all = self.network.generators_t.p_max_pu[self.renewable_names]\n",
        "            renewable_low_bounds = renewable_p_max_pu_all.min(axis=0).values\n",
        "            renewable_high_bounds = renewable_p_max_pu_all.max(axis=0).values\n",
        "        else:\n",
        "            renewable_low_bounds = np.array([])\n",
        "            renewable_high_bounds = np.array([])\n",
        "\n",
        "        # 3. Storage bounds (previous SOC + current inflow)\n",
        "        storage_low_bounds, storage_high_bounds = self.create_storage_observation_bounds()\n",
        "\n",
        "        # 4. Combine all bounds\n",
        "        low_bounds = np.concatenate([load_low_bounds, renewable_low_bounds, storage_low_bounds])\n",
        "        high_bounds = np.concatenate([load_high_bounds, renewable_high_bounds, storage_high_bounds])\n",
        "\n",
        "        return low_bounds.astype(np.float32), high_bounds.astype(np.float32)\n",
        "\n",
        "    def _get_observation(self):\n",
        "        \"\"\"\n",
        "        Get current network state as observation.\n",
        "\n",
        "        Returns observation vector with structure:\n",
        "        [load_1_demand, load_2_demand, ..., load_n_demand,\n",
        "        renewable_1_p_max_pu, renewable_2_p_max_pu, ..., renewable_m_p_max_pu,\n",
        "        storage_1_prev_soc_norm, storage_1_current_inflow_norm,\n",
        "        storage_2_prev_soc_norm, storage_2_current_inflow_norm,\n",
        "        ...,\n",
        "        storage_k_prev_soc_norm, storage_k_current_inflow_norm]\n",
        "        \"\"\"\n",
        "        # 1. Load demands (dynamic values at current snapshot)\n",
        "        load_demands = self.network.loads_t.p_set.iloc[self.snapshot_idx].values\n",
        "\n",
        "        # 2. Renewable generator p_max_pu values (time-varying availability at current snapshot)\n",
        "        if self.n_renewable > 0:\n",
        "            renewable_p_max_pu = self.network.generators_t.p_max_pu.iloc[self.snapshot_idx][self.renewable_names].values\n",
        "        else:\n",
        "            renewable_p_max_pu = np.array([])\n",
        "\n",
        "        # 3. Storage states (previous SOC normalized + current inflow normalized)\n",
        "        storage_states = self._get_storage_observation()\n",
        "\n",
        "        # 4. Combine all observations\n",
        "        observation = np.concatenate([load_demands, renewable_p_max_pu, storage_states])\n",
        "\n",
        "        return observation.astype(np.float32)\n",
        "\n",
        "    def reset_network(self):\n",
        "        \"\"\"Reset and ensure essential DataFrames exist.\"\"\"\n",
        "        #Note that we do not just create a new network here, as this consumes more memory and previously led to a segmentation fault\n",
        "        # we reset these ttributes for all snapshots, but they all start empty when the network is created so i think that's fine\n",
        "        # Initialize/reset generators_t.p_set\n",
        "        if not hasattr(self.network.generators_t, 'p_set') or self.network.generators_t.p_set.empty:\n",
        "            self.network.generators_t.p_set = pd.DataFrame(\n",
        "                0.0,\n",
        "                index=self.network.snapshots,\n",
        "                columns=self.network.generators.index\n",
        "            )\n",
        "        else:\n",
        "            self.network.generators_t.p_set.iloc[:, :] = 0.0\n",
        "\n",
        "        # Initialize/reset storage_units_t attributes\n",
        "        if not hasattr(self.network.storage_units_t, 'p_set') or self.network.storage_units_t.p_set.empty:\n",
        "            self.network.storage_units_t.p_set = pd.DataFrame(\n",
        "                0.0,\n",
        "                index=self.network.snapshots,\n",
        "                columns=self.network.storage_units.index\n",
        "            )\n",
        "        else:\n",
        "            self.network.storage_units_t.p_set.iloc[:, :] = 0.0\n",
        "\n",
        "\n",
        "        if not hasattr(self.network.storage_units_t, 'p_dispatch') or self.network.storage_units_t.p_dispatch.empty:\n",
        "            self.network.storage_units_t.p_dispatch = pd.DataFrame(\n",
        "                0.0,\n",
        "                index=self.network.snapshots,\n",
        "                columns=self.network.storage_units.index\n",
        "            )\n",
        "        else:\n",
        "            self.network.storage_units_t.p_dispatch.iloc[:, :] = 0.0\n",
        "\n",
        "        if not hasattr(self.network.storage_units_t, 'p_store') or self.network.storage_units_t.p_store.empty:\n",
        "            self.network.storage_units_t.p_store = pd.DataFrame(\n",
        "                0.0,\n",
        "                index=self.network.snapshots,\n",
        "                columns=self.network.storage_units.index\n",
        "            )\n",
        "        else:\n",
        "            self.network.storage_units_t.p_store.iloc[:, :] = 0.0\n",
        "\n",
        "        if not hasattr(self.network.storage_units_t, 'state_of_charge') or self.network.storage_units_t.state_of_charge.empty:\n",
        "            self.network.storage_units_t.state_of_charge = pd.DataFrame(\n",
        "                0.0,\n",
        "                index=self.network.snapshots,\n",
        "                columns=self.network.storage_units.index\n",
        "            )\n",
        "        else:\n",
        "            self.network.storage_units_t.state_of_charge.iloc[:, :] = 0.0\n",
        "\n",
        "        if not hasattr(self.network.storage_units_t, 'spill') or self.network.storage_units_t.spill.empty:\n",
        "            self.network.storage_units_t.spill = pd.DataFrame(\n",
        "                0.0,\n",
        "                index=self.network.snapshots,\n",
        "                columns=self.network.storage_units.index\n",
        "            )\n",
        "        else:\n",
        "            self.network.storage_units_t.spill.iloc[:, :] = 0.0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset to training data with proper episode length handling.\n",
        "        \"\"\"\n",
        "        # Set the seed if provided\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        # Reset counters\n",
        "        self.current_step = 0\n",
        "\n",
        "        if self.fixed_episode_length is not None:\n",
        "            # Fixed episode length: ensure episode can complete within training data\n",
        "            max_start_idx = max(0, self.train_snapshots - self.episode_length)\n",
        "            self.snapshot_idx = np.random.randint(0, max_start_idx + 1)\n",
        "        else:\n",
        "            # Variable episode length: can start anywhere in training data\n",
        "            self.snapshot_idx = np.random.randint(0, self.train_snapshots)\n",
        "\n",
        "        self.reset_network()\n",
        "\n",
        "        obs = self._get_observation()\n",
        "        info = {\n",
        "            'current_step': self.current_step,\n",
        "            'snapshot_idx': self.snapshot_idx,\n",
        "            'is_training': True,\n",
        "            'fixed_episodes': self.fixed_episode_length is not None,\n",
        "            'variable_episodes': self.variable_episodes\n",
        "        }\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    def reset_for_testing(self):\n",
        "        \"\"\"\n",
        "        Reset environment to start of test data with proper SOC initialization.\n",
        "        \"\"\"\n",
        "        # Reset to start of test period\n",
        "        self.snapshot_idx = self.train_snapshots\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Reset network state\n",
        "        self.reset_network()\n",
        "\n",
        "        # Set initial state of charge for storage units at test start\n",
        "        if self.n_storage > 0:\n",
        "            test_start_snapshot = self.network.snapshots[self.train_snapshots]\n",
        "            for storage_name in self.storage_names:\n",
        "                initial_soc = self.network.storage_units.loc[storage_name, 'state_of_charge_initial']\n",
        "                self.network.storage_units_t.state_of_charge.loc[test_start_snapshot, storage_name] = initial_soc\n",
        "\n",
        "        obs = self._get_observation()\n",
        "        info = {\n",
        "            'current_step': self.current_step,\n",
        "            'snapshot_idx': self.snapshot_idx,\n",
        "            'is_training': False\n",
        "        }\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    def get_test_snapshots(self):\n",
        "        \"\"\"\n",
        "        Return the test snapshots for external optimization.\n",
        "        \"\"\"\n",
        "        return self.network.snapshots[self.train_snapshots:]\n",
        "\n",
        "    def compute_storage_power_bounds(self):\n",
        "        \"\"\"\n",
        "        Compute the feasible bounds for p_set and p_dispatch to respect SOC limits.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict: Contains bounds for each storage unit\n",
        "        \"\"\"\n",
        "        bounds = {}\n",
        "\n",
        "        for i, storage_name in enumerate(self.storage_names):\n",
        "            # Get current SOC\n",
        "            if self.snapshot_idx == 0:\n",
        "                soc_prev = self.network.storage_units.state_of_charge_initial.loc[storage_name]\n",
        "            else:\n",
        "                previous_snapshot = self.network.snapshots[self.snapshot_idx - 1]\n",
        "                soc_prev = self.network.storage_units_t.state_of_charge.loc[previous_snapshot, storage_name]\n",
        "\n",
        "            # Get storage parameters\n",
        "            storage_unit = self.network.storage_units.loc[storage_name]\n",
        "            soc_max = storage_unit.p_nom * storage_unit.max_hours\n",
        "            eff_store = storage_unit.efficiency_store\n",
        "            eff_dispatch = storage_unit.efficiency_dispatch\n",
        "            standing_loss = storage_unit.standing_loss\n",
        "\n",
        "            # Get time step\n",
        "            if hasattr(self.network.snapshot_weightings, 'stores'):\n",
        "                delta_t = self.network.snapshot_weightings.stores.iloc[self.snapshot_idx]\n",
        "            else:\n",
        "                delta_t = self.network.snapshot_weightings.iloc[self.snapshot_idx]\n",
        "\n",
        "            eff_standing = (1 - standing_loss) ** delta_t\n",
        "\n",
        "            # Get inflow\n",
        "            if storage_name in self.network.storage_units_t.inflow.columns:\n",
        "                inflow = self.network.storage_units_t.inflow.loc[self.network.snapshots[self.snapshot_idx], storage_name]\n",
        "            else:\n",
        "                inflow = 0\n",
        "\n",
        "            # Base SOC change from standing losses and inflow\n",
        "            base_soc_change = soc_prev * eff_standing + inflow * delta_t\n",
        "\n",
        "            # Energy bounds for net storage change\n",
        "            min_net_energy = 0 - base_soc_change  # Don't go below 0 SOC\n",
        "            max_net_energy = soc_max - base_soc_change  # Don't exceed max SOC\n",
        "\n",
        "            # Convert to power bounds considering efficiencies\n",
        "            # Net energy change = (p_store * eff_store - p_dispatch/eff_dispatch) * delta_t\n",
        "            # So: min_net_energy ≤ (p_store * eff_store - p_dispatch/eff_dispatch) * delta_t ≤ max_net_energy\n",
        "\n",
        "            # Physical power limits\n",
        "            p_nom = storage_unit.p_nom\n",
        "\n",
        "            bounds[storage_name] = {\n",
        "                'soc_prev': soc_prev,\n",
        "                'soc_max': soc_max,\n",
        "                'min_net_energy_per_dt': min_net_energy / delta_t,\n",
        "                'max_net_energy_per_dt': max_net_energy / delta_t,\n",
        "                'eff_store': eff_store,\n",
        "                'eff_dispatch': eff_dispatch,\n",
        "                'p_nom': p_nom,\n",
        "                'delta_t': delta_t\n",
        "            }\n",
        "\n",
        "        return bounds\n",
        "\n",
        "    def scale_action(self, action):\n",
        "        \"\"\"\n",
        "        Scale action from [0,1] range to appropriate ranges for each component type,\n",
        "        with SOC bounds enforcement for storage units.\n",
        "        \"\"\"\n",
        "        scaled_actions = {}\n",
        "\n",
        "        # Scale dispatchable generator actions (unchanged)\n",
        "        if self.n_dispatchable > 0:\n",
        "            disp_actions = action[self.action_structure['dispatchable']['start']:\n",
        "                                self.action_structure['dispatchable']['end']]\n",
        "            scaled_actions['dispatchable'] = self.disp_p_min + disp_actions * (self.disp_p_max - self.disp_p_min)\n",
        "        else:\n",
        "            scaled_actions['dispatchable'] = np.array([])\n",
        "\n",
        "        # Scale renewable generator actions (unchanged)\n",
        "        if self.n_renewable > 0:\n",
        "            renewable_actions = action[self.action_structure['renewable']['start']:\n",
        "                                    self.action_structure['renewable']['end']]\n",
        "            current_p_max_pu = self.network.generators_t.p_max_pu.iloc[self.snapshot_idx][self.renewable_names].values\n",
        "            current_p_max = current_p_max_pu * self.renewable_p_nom\n",
        "            current_p_min = self.renewable_p_min_pu * self.renewable_p_nom\n",
        "            scaled_actions['renewable'] = current_p_min + renewable_actions * (current_p_max - current_p_min)\n",
        "        else:\n",
        "            scaled_actions['renewable'] = np.array([])\n",
        "\n",
        "        # Scale storage actions with SOC bounds enforcement\n",
        "        if self.n_storage > 0:\n",
        "            # Get SOC-based bounds\n",
        "            storage_bounds = self.compute_storage_power_bounds()\n",
        "            storage_p_set_actions = action[self.action_structure['storage_p_set']['start']:\n",
        "                                        self.action_structure['storage_p_set']['end']]\n",
        "            storage_p_dispatch_actions = action[self.action_structure['storage_p_dispatch']['start']:\n",
        "                                              self.action_structure['storage_p_dispatch']['end']]\n",
        "\n",
        "            scaled_p_set = np.zeros(self.n_storage)\n",
        "            scaled_p_dispatch = np.zeros(self.n_storage)\n",
        "\n",
        "            for i, storage_name in enumerate(self.storage_names):\n",
        "                bounds = storage_bounds[storage_name]\n",
        "                p_nom = bounds['p_nom']\n",
        "\n",
        "                # First, scale p_dispatch from [0,1] to [0, p_nom]\n",
        "                p_dispatch_raw = storage_p_dispatch_actions[i] * p_nom\n",
        "\n",
        "                # For given p_dispatch, find valid range for p_set\n",
        "                # Constraint: min_net_energy_per_dt ≤ (p_store * eff_store - p_dispatch/eff_dispatch) ≤ max_net_energy_per_dt\n",
        "                # Where p_store = p_dispatch - p_set\n",
        "                # So: min_net_energy_per_dt ≤ ((p_dispatch - p_set) * eff_store - p_dispatch/eff_dispatch) ≤ max_net_energy_per_dt\n",
        "\n",
        "                dispatch_energy_term = p_dispatch_raw / bounds['eff_dispatch']\n",
        "                store_coeff = bounds['eff_store']\n",
        "\n",
        "                # Rearranging: min_net_energy_per_dt ≤ p_dispatch_raw * eff_store - p_set * eff_store - dispatch_energy_term ≤ max_net_energy_per_dt\n",
        "                # So: p_dispatch_raw * eff_store - dispatch_energy_term - max_net_energy_per_dt ≤ p_set * eff_store ≤ p_dispatch_raw * eff_store - dispatch_energy_term - min_net_energy_per_dt\n",
        "\n",
        "                base_term = p_dispatch_raw * store_coeff - dispatch_energy_term\n",
        "                p_set_min_from_soc = (base_term - bounds['max_net_energy_per_dt']) / store_coeff\n",
        "                p_set_max_from_soc = (base_term - bounds['min_net_energy_per_dt']) / store_coeff\n",
        "\n",
        "                # Also enforce physical limits: -p_nom ≤ p_set ≤ p_nom\n",
        "                p_set_min = max(-p_nom, p_set_min_from_soc)\n",
        "                p_set_max = min(p_nom, p_set_max_from_soc)\n",
        "\n",
        "                ipdb.set_trace()\n",
        "\n",
        "                # Ensure feasible range exists\n",
        "                if p_set_min > p_set_max:\n",
        "                    # If no feasible range, clip p_dispatch and recalculate\n",
        "                    # This is a fallback - ideally shouldn't happen with proper bounds\n",
        "                    p_dispatch_raw = min(p_dispatch_raw, p_nom * 0.5)  # Conservative fallback\n",
        "                    ipdb.set_trace()\n",
        "                    base_term = p_dispatch_raw * store_coeff - p_dispatch_raw / bounds['eff_dispatch']\n",
        "                    ipdb.set_trace()\n",
        "                    p_set_min_from_soc = (base_term - bounds['max_net_energy_per_dt']) / store_coeff\n",
        "                    ipdb.set_trace()\n",
        "                    p_set_max_from_soc = (base_term - bounds['min_net_energy_per_dt']) / store_coeff\n",
        "                    ipdb.set_trace()\n",
        "                    p_set_min = max(-p_nom, p_set_min_from_soc)\n",
        "                    p_set_max = min(p_nom, p_set_max_from_soc)\n",
        "                ipdb.set_trace()\n",
        "                # Scale p_set action from [0,1] to [p_set_min, p_set_max]\n",
        "                if p_set_max > p_set_min:\n",
        "                    scaled_p_set[i] = p_set_min + storage_p_set_actions[i] * (p_set_max - p_set_min)\n",
        "                else:\n",
        "                    scaled_p_set[i] = p_set_min  # Fallback to minimum if range is degenerate\n",
        "\n",
        "                scaled_p_dispatch[i] = p_dispatch_raw\n",
        "\n",
        "            scaled_actions['storage_p_set'] = scaled_p_set\n",
        "            scaled_actions['storage_p_dispatch'] = scaled_p_dispatch\n",
        "        else:\n",
        "            scaled_actions['storage_p_set'] = np.array([])\n",
        "            scaled_actions['storage_p_dispatch'] = np.array([])\n",
        "        ipdb.set_trace()\n",
        "        return scaled_actions\n",
        "\n",
        "    def _update_storage_soc_single_snapshot(self, storage_name):\n",
        "        if self.snapshot_idx == 0:\n",
        "            # For first snapshot, previous SOC is the initial value\n",
        "            soc_prev = self.network.storage_units.state_of_charge_initial.loc[storage_name]\n",
        "        else:\n",
        "            previous_snapshot = self.network.snapshots[self.snapshot_idx - 1]\n",
        "            soc_prev = self.network.storage_units_t.state_of_charge.loc[previous_snapshot, storage_name]\n",
        "\n",
        "        current_snapshot = self.network.snapshots[self.snapshot_idx]\n",
        "\n",
        "        #Get storage parameters\n",
        "        storage_unit = self.network.storage_units.loc[storage_name]\n",
        "        soc_max = storage_unit.p_nom * storage_unit.max_hours\n",
        "        eff_store = storage_unit.efficiency_store\n",
        "        eff_dispatch = storage_unit.efficiency_dispatch\n",
        "        standing_loss = storage_unit.standing_loss\n",
        "\n",
        "        # Get time step\n",
        "        if hasattr(self.network.snapshot_weightings, 'stores'):\n",
        "            delta_t = self.network.snapshot_weightings.stores.iloc[self.snapshot_idx]\n",
        "        else:\n",
        "            delta_t = self.network.snapshot_weightings.iloc[self.snapshot_idx]\n",
        "\n",
        "        eff_standing = (1 - standing_loss) ** delta_t\n",
        "\n",
        "        # Get current operations (these determine the SOC change)\n",
        "        p_store = self.network.storage_units_t.p_store.loc[current_snapshot, storage_name]\n",
        "        p_dispatch = self.network.storage_units_t.p_dispatch.loc[current_snapshot, storage_name]\n",
        "        if storage_name in self.network.storage_units_t.inflow.columns:\n",
        "          inflow = self.network.storage_units_t.inflow.loc[current_snapshot, storage_name]\n",
        "        else:\n",
        "          inflow=0\n",
        "\n",
        "        # Calculate SOC without spill (could be non-zero even if soc_prev=0)\n",
        "        soc_without_spill = (soc_prev * eff_standing +\n",
        "                            (p_store * eff_store - p_dispatch/eff_dispatch + inflow) * delta_t)\n",
        "\n",
        "        # Calculate required spill\n",
        "        required_spill = max(0, (soc_without_spill - soc_max) / delta_t)\n",
        "\n",
        "        # Final SOC after spill\n",
        "        soc_actual = min(soc_without_spill, soc_max)\n",
        "\n",
        "        # Update the network\n",
        "        self.network.storage_units_t.state_of_charge.loc[current_snapshot, storage_name] = soc_actual\n",
        "        if hasattr(self.network, 'storage_units_t') and 'spill' in self.network.storage_units_t:\n",
        "            self.network.storage_units_t.spill.loc[current_snapshot, storage_name] = required_spill\n",
        "\n",
        "    def evaluate_objective_direct(self, current_snapshot):\n",
        "        \"\"\"\n",
        "        Direct evaluation of PyPSA operational objective function terms.\n",
        "\n",
        "        This function evaluates only the operational terms (marginal costs) that PyPSA\n",
        "        optimizes for generators and storage units, excluding capital costs and other\n",
        "        investment-related terms.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Total operational cost for the current snapshot including snapshot weighting\n",
        "        \"\"\"\n",
        "        total_cost = 0.0\n",
        "\n",
        "        # Get snapshot weighting for proper cost calculation\n",
        "        snapshot_weighting = self.network.snapshot_weightings.objective.loc[current_snapshot]\n",
        "\n",
        "        # Generator operational costs\n",
        "        if len(self.network.generators) > 0:\n",
        "            # Get marginal costs and power output\n",
        "            gen_marginal_costs = self.network.generators['marginal_cost']\n",
        "\n",
        "            gen_power = self.network.generators_t.p.loc[current_snapshot]\n",
        "\n",
        "            # Calculate generator operational cost\n",
        "            gen_cost = (gen_marginal_costs * gen_power).sum()\n",
        "            total_cost += gen_cost\n",
        "\n",
        "        # Storage unit operational costs\n",
        "        if len(self.network.storage_units) > 0:\n",
        "            # Marginal cost for storage dispatch (discharge)\n",
        "            storage_marginal_costs = self.network.storage_units['marginal_cost']\n",
        "            storage_p_dispatch = self.network.storage_units_t.p_dispatch.loc[current_snapshot]\n",
        "            storage_cost = (storage_marginal_costs * storage_p_dispatch).sum()\n",
        "            #multiply correpsonding entries of the pandas columns and then sum them\n",
        "            total_cost += storage_cost\n",
        "\n",
        "            # Marginal cost for storage charging\n",
        "            storage_marginal_costs_storage = self.network.storage_units['marginal_cost_storage']\n",
        "            storage_store_power = self.network.storage_units_t.p_store.loc[current_snapshot]\n",
        "            storage_store_cost = (storage_marginal_costs_storage * storage_store_power).sum()\n",
        "            total_cost += storage_store_cost\n",
        "\n",
        "            spill_costs = self.network.storage_units['spill_cost']\n",
        "            spill_amounts = self.network.storage_units_t.spill.loc[current_snapshot]\n",
        "            spill_cost = (spill_costs * spill_amounts).sum()\n",
        "            total_cost += spill_cost\n",
        "\n",
        "        # Apply snapshot weighting (this is crucial for proper cost calculation)\n",
        "        total_cost *= snapshot_weighting\n",
        "\n",
        "        return total_cost\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Calculate reward using stored objective components.\"\"\"\n",
        "        # Get the current snapshot name\n",
        "\n",
        "        current_snapshot = self.network.snapshots[self.snapshot_idx]\n",
        "        return -1 * self.evaluate_objective_direct(current_snapshot)\n",
        "\n",
        "    def calculate_constrained_reward(self):\n",
        "        \"\"\"\n",
        "        Calculate reward using summation method with dynamic constraint checking.\n",
        "\n",
        "        Summation method:\n",
        "        - Reward = -J(s) - P(s)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get base reward from objective function (negative for minimization)\n",
        "            base_reward = self._calculate_reward()\n",
        "\n",
        "            current_snapshot = self.network.snapshots[self.snapshot_idx]\n",
        "\n",
        "            # Initialize constraint tracking\n",
        "            constraint_results = {\n",
        "                'all_satisfied': True,\n",
        "                'violations': {},\n",
        "                'total_violation': 0.0,\n",
        "                'violations_by_group': {}\n",
        "            }\n",
        "\n",
        "            # 1. Check slack generator constraints\n",
        "            slack_generators = self.network.generators[self.network.generators.control == \"Slack\"].index\n",
        "            if not slack_generators.empty:\n",
        "                for gen_name in slack_generators:\n",
        "                    # Get actual power output after power flow\n",
        "                    p_actual = self.network.generators_t.p.loc[current_snapshot, gen_name]\n",
        "\n",
        "                    # Get limits\n",
        "                    p_min = self.network.generators.loc[gen_name, 'p_min_pu'] * self.network.generators.loc[gen_name, 'p_nom']\n",
        "                    p_max = self.network.generators.loc[gen_name, 'p_max_pu'] * self.network.generators.loc[gen_name, 'p_nom']\n",
        "\n",
        "                    # Check lower bound\n",
        "                    if p_actual < p_min:\n",
        "                        violation = float(p_min - p_actual)\n",
        "                        constraint_name = f\"Generator-slack-p-lower[snapshot={current_snapshot},Generator={gen_name}]\"\n",
        "                        constraint_results['violations'][constraint_name] = violation\n",
        "                        constraint_results['total_violation'] += violation\n",
        "                        constraint_results['all_satisfied'] = False\n",
        "\n",
        "                    # Check upper bound\n",
        "                    if p_actual > p_max:\n",
        "                        violation = float(p_actual - p_max)\n",
        "                        constraint_name = f\"Generator-slack-p-upper[snapshot={current_snapshot},Generator={gen_name}]\"\n",
        "                        constraint_results['violations'][constraint_name] = violation\n",
        "                        constraint_results['total_violation'] += violation\n",
        "                        constraint_results['all_satisfied'] = False\n",
        "\n",
        "            # 2. Check line flow constraints (CORRECTED)\n",
        "            for line_name in self.network.lines.index:\n",
        "                # Get line parameters\n",
        "                s_nom = self.network.lines.loc[line_name, 's_nom']\n",
        "                s_max_pu = 1.0  # Default, or get from lines_t.s_max_pu if it exists\n",
        "\n",
        "                # Calculate active power limit (this is what PyPSA's linear constraints check)\n",
        "                s_max = s_max_pu * s_nom\n",
        "\n",
        "                # Get active power flow from the linear power flow\n",
        "                # In PyPSA's linear formulation, this is the 's' variable value\n",
        "                p0 = abs(self.network.lines_t.p0.loc[current_snapshot, line_name])\n",
        "\n",
        "                # Check if active power flow exceeds limit\n",
        "                if p0 > s_max:\n",
        "                    violation = float(p0 - s_max)\n",
        "                    constraint_name = f\"Line-fix-s-upper[snapshot={current_snapshot},Line={line_name}]\"\n",
        "                    constraint_results['violations'][constraint_name] = violation\n",
        "                    constraint_results['total_violation'] += violation\n",
        "                    constraint_results['all_satisfied'] = False\n",
        "\n",
        "\n",
        "            # Calculate penalty\n",
        "            total_violation = float(constraint_results['total_violation'])\n",
        "            penalty = self.penalty_factor * total_violation\n",
        "\n",
        "            # Calculate final reward using summation method\n",
        "            constrained_reward = base_reward - penalty\n",
        "\n",
        "            # Ensure reward is a scalar\n",
        "            if hasattr(constrained_reward, '__len__'):\n",
        "                constrained_reward = float(constrained_reward)\n",
        "\n",
        "            return constrained_reward, constraint_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating summation reward: {e}\")\n",
        "            # Fall back to base reward on error\n",
        "            return self._calculate_reward(), {\n",
        "                'all_satisfied': True,\n",
        "                'violations': {},\n",
        "                'total_violation': 0.0\n",
        "            }\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one time step within the environment.\n",
        "\n",
        "        Args:\n",
        "            action: Array of setpoints for all controllable components [disp_gen1, disp_gen2, ...,\n",
        "                   renewable_gen1, renewable_gen2, ..., storage1, storage2, ...]\n",
        "\n",
        "        Returns:\n",
        "            observation: Network state after action\n",
        "            reward: Reward for this action\n",
        "            terminated: Whether episode is finished due to task completion\n",
        "            truncated: Whether episode is finished due to time limit\n",
        "            info: Additional information\n",
        "        \"\"\"\n",
        "        scaled_actions = self.scale_action(action)\n",
        "        # Apply dispatchable generator setpoints\n",
        "        if self.n_dispatchable > 0:\n",
        "            for i, gen_name in enumerate(self.dispatchable_names):\n",
        "                self.network.generators_t.p_set.iloc[self.snapshot_idx, self.network.generators_t.p_set.columns.get_loc(gen_name)] = scaled_actions['dispatchable'][i]\n",
        "\n",
        "        # Apply renewable generator setpoints\n",
        "        if self.n_renewable > 0:\n",
        "            for i, gen_name in enumerate(self.renewable_names):\n",
        "                self.network.generators_t.p_set.iloc[self.snapshot_idx,\n",
        "                    self.network.generators_t.p_set.columns.get_loc(gen_name)] = scaled_actions['renewable'][i]\n",
        "\n",
        "        # Apply storage unit setpoints\n",
        "        if self.n_storage > 0:\n",
        "            for i, storage_name in enumerate(self.storage_names):\n",
        "                self.network.storage_units_t.p_set.iloc[self.snapshot_idx,\n",
        "                    self.network.storage_units_t.p_set.columns.get_loc(storage_name)] = scaled_actions['storage_p_set'][i]\n",
        "                self.network.storage_units_t.p_dispatch.iloc[self.snapshot_idx,\n",
        "                    self.network.storage_units_t.p_dispatch.columns.get_loc(storage_name)] = scaled_actions['storage_p_dispatch'][i]\n",
        "                self.network.storage_units_t.p_store.iloc[self.snapshot_idx,\n",
        "                    self.network.storage_units_t.p_store.columns.get_loc(storage_name)] = scaled_actions['storage_p_dispatch'][i] - scaled_actions['storage_p_set'][i]\n",
        "\n",
        "            # Update state of charge using PyPSA's energy balance equation\n",
        "                if self.snapshot_idx > 0:\n",
        "                    self._update_storage_soc_single_snapshot(storage_name)\n",
        "        # Run power flow to get new network state\n",
        "        try:\n",
        "            self.network.lpf(self.network.snapshots[self.snapshot_idx])\n",
        "            power_flow_converged = True\n",
        "        except Exception as e:\n",
        "            print(f\"Power flow failed: {e}\")\n",
        "            power_flow_converged = False\n",
        "\n",
        "        # Calculate reward using constrained reward function\n",
        "        reward, constraint_results = self.calculate_constrained_reward()\n",
        "\n",
        "        # Increment step counters\n",
        "        self.current_step += 1\n",
        "        self.snapshot_idx += 1\n",
        "\n",
        "        # Handle cycling through snapshots\n",
        "        if self.snapshot_idx >= self.total_snapshots:\n",
        "            self.snapshot_idx = 0\n",
        "            self.reset_network()\n",
        "\n",
        "        # Get new observation\n",
        "        observation = self._get_observation()\n",
        "\n",
        "        # Check if episode is done\n",
        "        episode_done = self._check_done()\n",
        "        terminated = False\n",
        "        truncated = episode_done\n",
        "\n",
        "        # Additional info\n",
        "        info = {\n",
        "            'dispatchable_setpoints': scaled_actions['dispatchable'],\n",
        "            'renewable_setpoints': scaled_actions['renewable'],\n",
        "            'storage_p_set': scaled_actions['storage_p_set'],\n",
        "            'storage_p_dispatch': scaled_actions['storage_p_dispatch'],\n",
        "            'power_flow_converged': power_flow_converged,\n",
        "            'dispatchable_names': self.dispatchable_names,\n",
        "            'renewable_names': self.renewable_names,\n",
        "            'storage_names': self.storage_names,\n",
        "            'current_step': self.current_step,\n",
        "            'snapshot_idx': self.snapshot_idx,\n",
        "            'constraints_satisfied': constraint_results['all_satisfied'],\n",
        "            'constraint_violations': constraint_results['violations'],\n",
        "            'total_violation': constraint_results['total_violation']\n",
        "        }\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def _check_done(self):\n",
        "        \"\"\"\n",
        "        Modified to handle both fixed and variable episode lengths.\n",
        "        \"\"\"\n",
        "        # For fixed episodes, check step count\n",
        "        if self.episode_length is not None and self.current_step >= self.episode_length:\n",
        "            return True\n",
        "\n",
        "        # For all episodes, stop if we've reached the test data boundary\n",
        "        if self.snapshot_idx >= self.train_snapshots:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        \"\"\"\n",
        "        Set the random seed for reproducible experiments.\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def render(self, mode='human', info=None):\n",
        "        \"\"\"\n",
        "        Render the environment state.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        mode : str\n",
        "            Rendering mode (only 'human' supported)\n",
        "        info : dict, optional\n",
        "            Information dictionary from step() method containing constraint data\n",
        "        \"\"\"\n",
        "        print(\"=== Current Network State ===\")\n",
        "        print(f\"Episode step: {self.current_step}/{self.episode_length}\")\n",
        "        print(f\"Snapshot index: {self.snapshot_idx}/{self.total_snapshots}\")\n",
        "        print(f\"Current snapshot: {self.network.snapshots[self.snapshot_idx]}\")\n",
        "        print(f\"Generator setpoints: {self.network.generators_t.p_set.iloc[self.snapshot_idx].values}\")\n",
        "        print(f\"Load values: {self.network.loads_t.p_set.iloc[self.snapshot_idx].values}\")\n",
        "\n",
        "        all_satisfied = info['constraints_satisfied']\n",
        "        total_violation = info['total_violation']\n",
        "        violations = info['constraint_violations']\n",
        "\n",
        "\n",
        "        print(f\"All constraints satisfied: {all_satisfied}\")\n",
        "        print(f\"Total constraint violation: {total_violation:.4f}\")\n",
        "\n",
        "        # Show violated constraints if any\n",
        "        if not all_satisfied and violations:\n",
        "            print(\"\\n=== Constraint Violations ===\")\n",
        "            for constraint_name, violation in violations.items():\n",
        "                print(f\"  {constraint_name}: {violation:.4f}\")\n",
        "\n",
        "# network_file_path= \"/Users/antoniagrindrod/Documents/pypsa-earth_project/pypsa-earth-RL/networks/elec_s_10_ec_lc1.0_1h.nc\"\n",
        "# input_dir=\"/Users/antoniagrindrod/Documents/pypsa-earth_project/pypsa-earth-RL/RL/var_constraint_map\"\n",
        "# replacement_reward_offset=calculate_offset_k_initialization(network_file=network_file_path, input_dir=input_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5Q6N_2f1ZkZw",
      "metadata": {
        "id": "5Q6N_2f1ZkZw"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent_on_test_data(env, agent, optimal_objective_value):\n",
        "    \"\"\"\n",
        "    Evaluate trained agent on test data and compute MAPE and constraint violations.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    env : EnvDispatchConstr\n",
        "        The environment with train/test split\n",
        "    agent : trained RL agent\n",
        "        Agent with a method to get actions from observations\n",
        "    optimal_objective_value : float\n",
        "        The optimal objective value from pypsa.optimize() on test snapshots\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Evaluation metrics including MAPE and constraint violation percentage\n",
        "    \"\"\"\n",
        "\n",
        "    # Reset environment to test data start\n",
        "    obs, info = env.reset_for_testing()\n",
        "\n",
        "    # Track metrics\n",
        "    rl_total_reward = 0.0\n",
        "    total_violations = 0\n",
        "    total_constraint_checks = 0\n",
        "    violation_snapshots = 0\n",
        "    total_test_snapshots = env.test_snapshots\n",
        "\n",
        "    # Run agent on all test snapshots\n",
        "    for step in range(total_test_snapshots):\n",
        "        # Get action from agent\n",
        "        if hasattr(agent, 'get_action'):\n",
        "            action = agent.get_action(obs)\n",
        "        elif hasattr(agent, 'predict'):\n",
        "            action = agent.predict(obs)\n",
        "        else:\n",
        "            # Assume agent is callable\n",
        "            action = agent(obs)\n",
        "\n",
        "        # Take step in environment\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # Calculate base reward (without offset k for replacement method)\n",
        "        base_reward = env._calculate_reward()\n",
        "        rl_total_reward += base_reward\n",
        "\n",
        "        # Track constraint violations for this snapshot\n",
        "        snapshot_violations = info['constraint_violations']\n",
        "        snapshot_violation_count = len(snapshot_violations)\n",
        "\n",
        "        if snapshot_violation_count > 0:\n",
        "            violation_snapshots += 1\n",
        "\n",
        "        # The constraint violations in info already correspond to the processed snapshot\n",
        "        # Count total constraint checks and violations based on network structure\n",
        "        slack_generators = env.network.generators[env.network.generators.control == \"Slack\"].index\n",
        "        slack_constraint_count = len(slack_generators) * 2  # upper and lower bounds\n",
        "        line_constraint_count = len(env.network.lines)\n",
        "\n",
        "        snapshot_total_constraints = slack_constraint_count + line_constraint_count\n",
        "        total_constraint_checks += snapshot_total_constraints\n",
        "        total_violations += len(snapshot_violations)\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    # Calculate MAPE\n",
        "    # MAPE = |RL_objective - Optimal_objective| / |Optimal_objective| * 100%\n",
        "    mape = abs(rl_total_reward - optimal_objective_value) / abs(optimal_objective_value) * 100.0\n",
        "\n",
        "    # Calculate constraint violation percentage\n",
        "    # This is the percentage of constraint checks that resulted in violations\n",
        "    constraint_violation_percentage = (total_violations / total_constraint_checks) * 100.0 if total_constraint_checks > 0 else 0.0\n",
        "\n",
        "    # Also calculate percentage of snapshots with violations\n",
        "    snapshot_violation_percentage = (violation_snapshots / total_test_snapshots) * 100.0\n",
        "\n",
        "    results = {\n",
        "        'mape': mape,\n",
        "        'rl_total_objective': rl_total_reward,\n",
        "        'optimal_total_objective': optimal_objective_value,\n",
        "        'constraint_violation_percentage': constraint_violation_percentage,\n",
        "        'snapshot_violation_percentage': snapshot_violation_percentage,\n",
        "        'total_violations': total_violations,\n",
        "        'total_constraint_checks': total_constraint_checks,\n",
        "        'violation_snapshots': violation_snapshots,\n",
        "        'total_test_snapshots': total_test_snapshots\n",
        "    }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f9c06b56",
      "metadata": {
        "id": "f9c06b56"
      },
      "outputs": [],
      "source": [
        "class BackboneNetwork(nn.Module):\n",
        "    def __init__(self, input_features, hidden_dimensions, out_features, dropout):\n",
        "        super(BackboneNetwork, self).__init__()\n",
        "\n",
        "        # SIMPLIFIED: Single hidden layer network for debugging\n",
        "        self.neuralnet = nn.Sequential(\n",
        "            nn.Linear(input_features, hidden_dimensions),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dimensions, hidden_dimensions),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dimensions, out_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.neuralnet(x)\n",
        "        return output\n",
        "\n",
        "#Define the actor-critic network\n",
        "class actorCritic(nn.Module):\n",
        "    def __init__(self, actor, critic):\n",
        "        super().__init__()\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "    def forward(self, state):\n",
        "        action_pred = self.actor(state)\n",
        "        value_pred = self.critic(state)\n",
        "        return action_pred, value_pred\n",
        "        #Returns both the action predictions and the value predictions.\n",
        "\n",
        "#We'll use the networks defined above to create an actor and a critic. Then, we will create an agent, including the actor and the critic.\n",
        "#finish this step later\n",
        "# def create_agent(hidden_dimensions, dropout):\n",
        "#     INPUT_FEATURES =env_train.\n",
        "class PPO_agent:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 device,\n",
        "                 run,\n",
        "                 hidden_dimensions,\n",
        "                 dropout, discount_factor,\n",
        "                 max_episodes,\n",
        "                 print_interval,\n",
        "                 PPO_steps,\n",
        "                 n_trials,\n",
        "                 epsilon,\n",
        "                 entropy_coefficient,\n",
        "                 learning_rate,\n",
        "                 batch_size,\n",
        "                 optimizer_name,\n",
        "                 seed):\n",
        "\n",
        "        self.seed = seed\n",
        "        if seed is not None:\n",
        "            # Set PyTorch seed for this class\n",
        "            torch.manual_seed(seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed(seed)\n",
        "\n",
        "        self.env = env  # Store the environment as an attribute\n",
        "\n",
        "        self.device = device\n",
        "        self.run = run\n",
        "\n",
        "        # Get observation and action space dimensions for gymnasium environment\n",
        "        obs, _ = self.env.reset()\n",
        "\n",
        "        self.INPUT_FEATURES = obs.shape[0]  # Flattened observation size\n",
        "        self.ACTOR_OUTPUT_FEATURES = self.env.action_space.shape[0]* 2  # 2 parameters (alpha, beta) per action dimension\n",
        "\n",
        "        self.HIDDEN_DIMENSIONS = hidden_dimensions\n",
        "\n",
        "        self.CRITIC_OUTPUT_FEATURES = 1\n",
        "        self.DROPOUT = dropout\n",
        "\n",
        "        self.discount_factor = discount_factor\n",
        "        self.max_episodes = max_episodes\n",
        "        self.print_interval = print_interval\n",
        "        self.PPO_steps=PPO_steps\n",
        "        self.n_trials=n_trials\n",
        "        self.epsilon=epsilon\n",
        "        self.entropy_coefficient=entropy_coefficient\n",
        "        self.learning_rate=learning_rate\n",
        "\n",
        "        self.batch_size=batch_size\n",
        "\n",
        "        # Initialize actor network\n",
        "        self.actor = BackboneNetwork(\n",
        "            self.INPUT_FEATURES, self.HIDDEN_DIMENSIONS, self.ACTOR_OUTPUT_FEATURES, self.DROPOUT\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Initialize the final layer bias for Beta distribution\n",
        "        for name, param in self.actor.named_parameters():\n",
        "            if 'neuralnet.4.bias' in name:  # Adjust index based on your network structure\n",
        "                # Initialize to produce alpha=beta=2 (uniform-like distribution centered at 0.5)\n",
        "                param.data.fill_(0.0)  # softplus(0) + 1 = 2\n",
        "                print(f\"Initialized Beta parameters to produce uniform-like distribution\")\n",
        "\n",
        "        # Initialize critic network\n",
        "        self.critic = BackboneNetwork(\n",
        "            self.INPUT_FEATURES, self.HIDDEN_DIMENSIONS, self.CRITIC_OUTPUT_FEATURES, self.DROPOUT\n",
        "        ).to(self.device)\n",
        "\n",
        "        #Better move the .to(self.device) call separately for both self.actor and self.critic. This ensures the individual parts of the model are moved to the correct device before combined into the actorCritic class\n",
        "        # Combine into a single actor-critic model\n",
        "        self.model = actorCritic(self.actor, self.critic)\n",
        "\n",
        "        try:\n",
        "            # Try to get the optimizer from torch.optim based on the provided name\n",
        "            self.optimizer = getattr(torch.optim, optimizer_name)(self.model.parameters(), lr=self.learning_rate)\n",
        "        except AttributeError:\n",
        "            # Raise an error if the optimizer_name is not valid\n",
        "            raise ValueError(f\"Optimizer '{optimizer_name}' is not available in torch.optim.\")\n",
        "\n",
        "    def calculate_returns(self, rewards):\n",
        "        returns = []\n",
        "        cumulative_reward = 0\n",
        "        for r in reversed(rewards):\n",
        "            cumulative_reward = r +cumulative_reward*self.discount_factor\n",
        "            returns.insert(0, cumulative_reward)\n",
        "        returns = torch.tensor(returns).to(self.device)\n",
        "\n",
        "        # Only normalize if we have more than one element to avoid std() warning\n",
        "        if returns.numel() > 1:\n",
        "            epsilon = 1e-8  # Small constant to avoid division by zero\n",
        "            returns_std = returns.std()\n",
        "            if not torch.isnan(returns_std) and returns_std >= epsilon:\n",
        "                returns = (returns - returns.mean()) / (returns_std + epsilon)\n",
        "\n",
        "        #I had conceptual trouble with normalizing the reward by an average, because it seemed to me since we're adding more rewards for earlier timesteps, the cumulative reward for earlier times would be a lot larger. But need to consider dicount facotr.\n",
        "        # Future rewards contribute significantly to the cumulative return, so earlier timesteps will likely have larger returns.\n",
        "        #if gamma is close to 0, future rewards have little influence, and the return at each timestep will closely resemble the immediate reward, meaning the pattern might not be as clear.\n",
        "        return returns\n",
        "\n",
        "    #The advantage is calculated as the difference between the value predicted by the critic and the expected return from the actions chosen by the actor according to the policy.\n",
        "    def calculate_advantages(self, returns, values):\n",
        "        advantages = returns - values\n",
        "\n",
        "        # Only normalize if we have more than one element to avoid std() warning\n",
        "        if advantages.numel() > 1:\n",
        "            epsilon = 1e-8\n",
        "            advantages_std = advantages.std()\n",
        "            if not torch.isnan(advantages_std) and advantages_std >= epsilon:\n",
        "                advantages = (advantages - advantages.mean()) / (advantages_std + epsilon)\n",
        "\n",
        "        return advantages\n",
        "\n",
        "    #The standard policy gradient loss is calculated as the product of the policy action probabilities and the advantage function\n",
        "    #The standard policy gradietn loss cannot make corrections for abrupt policy changes. The surrogate loss modifies the standard loss to restrict the amount the policy can change in each iteration.\n",
        "    #The surrogate loss is the minimum of (policy ratio X advantage function) and (clipped value of policy ratio X advantage function) where the policy ratio is between the action probabilities according to the old versus new policies and clipping restricts the value to a region near 1.\n",
        "\n",
        "    def calculate_surrogate_loss(self, actions_log_probability_old, actions_log_probability_new, advantages):\n",
        "        advantages = advantages.detach()\n",
        "        # creates a new tensor that shares the same underlying data as the original tensor but breaks the computation graph. This means:\n",
        "        # The new tensor is treated as a constant with no gradients.\n",
        "        # Any operations involving this tensor do not affect the gradients of earlier computations in the graph.\n",
        "\n",
        "        #If the advantages are not detached, the backpropagation of the loss computed using the surrogate_loss would affect both the actor and the critic networks\n",
        "        # The surrogate loss is meant to update only the policy (actor).\n",
        "        # Allowing gradients to flow back through the advantages would inadvertently update the critic, potentially disrupting its learning process.\n",
        "\n",
        "        policy_ratio  = (actions_log_probability_new - actions_log_probability_old).exp()\n",
        "        surrogate_loss_1 = policy_ratio*advantages\n",
        "        surrogate_loss_2 = torch.clamp(policy_ratio, min =1.0-self.epsilon, max = 1.0+self.epsilon)*advantages\n",
        "        surrogate_loss=torch.min(surrogate_loss_1, surrogate_loss_2)\n",
        "        return surrogate_loss\n",
        "\n",
        "    #TRAINING THE AGENT\n",
        "    #Policy loss is the sum of the surrogate loss and the entropy bonus. It is used to update the actor (policy network)\n",
        "    #Value loss is based on the difference between the value predicted by the critic and the returns (cumulative reward) generated by the policy. This loss is used to update the critic (value network) to make predictions more accurate.\n",
        "\n",
        "    def calculate_losses(self, surrogate_loss, entropy, returns, value_pred):\n",
        "        entropy_bonus = self.entropy_coefficient*entropy\n",
        "        policy_loss = -(surrogate_loss+entropy_bonus).sum()\n",
        "        value_loss = torch.nn.functional.smooth_l1_loss(returns, value_pred).sum() #helps to smoothen the loss function and makes it less sensitive to outliers.\n",
        "        return policy_loss, value_loss\n",
        "\n",
        "    def init_training(self):\n",
        "        #create a set of buffers as empty arrays. To be used during training to store information\n",
        "        states = []\n",
        "        actions = []\n",
        "        actions_log_probability = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        return states, actions, actions_log_probability, values, rewards, done, episode_reward\n",
        "\n",
        "    def forward_pass(self):#this is just the training function (might just want to rename it)\n",
        "        # # === DETAILED OBJECT ANALYSIS ===\n",
        "        # import psutil\n",
        "        # import gc\n",
        "\n",
        "        # if not hasattr(self, '_episode_counter'):\n",
        "        #     self._episode_counter = 0\n",
        "        # self._episode_counter += 1\n",
        "\n",
        "        # mem_mb = psutil.Process().memory_info().rss / 1024 / 1024\n",
        "\n",
        "        # # Get ALL objects with \"Network\" in their type name\n",
        "        # network_objects = [obj for obj in gc.get_objects() if 'network' in str(type(obj)).lower()]\n",
        "\n",
        "        # print(f\"\\n=== EPISODE {self._episode_counter} OBJECT ANALYSIS ===\")\n",
        "        # print(f\"Memory: {mem_mb:.1f}MB\")\n",
        "        # print(f\"Total objects with 'network' in type: {len(network_objects)}\")\n",
        "\n",
        "        # # Count by exact type\n",
        "        # type_counts = {}\n",
        "        # for obj in network_objects:\n",
        "        #     obj_type = str(type(obj))\n",
        "        #     type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "\n",
        "        # # Print breakdown\n",
        "        # for obj_type, count in type_counts.items():\n",
        "        #     print(f\"  {obj_type}: {count}\")\n",
        "\n",
        "        # # Show actual PyPSA Network objects specifically\n",
        "        # actual_networks = [obj for obj in gc.get_objects() if type(obj).__name__ == 'Network' and 'pypsa' in str(type(obj))]\n",
        "        # print(f\"Actual PyPSA Network objects: {len(actual_networks)}\")\n",
        "\n",
        "        # if len(actual_networks) <= 5:  # Only print if reasonable number\n",
        "        #     for i, net in enumerate(actual_networks):\n",
        "        #         print(f\"  Network {i+1}: {id(net)} - {type(net)}\")\n",
        "\n",
        "        # network_id = id(self.env.network) if hasattr(self.env, 'network') else None\n",
        "        # print(f\"Current env.network ID: {network_id}\")\n",
        "        # print(\"=\" * 50)\n",
        "        # # === END ANALYSIS ===\n",
        "\n",
        "        # Reset environment with seed\n",
        "        if self.seed is not None:\n",
        "            state, _ = self.env.reset(seed=self.seed)\n",
        "        else:\n",
        "            state, _ = self.env.reset()\n",
        "\n",
        "        states, actions, actions_log_probability, values, rewards, done, episode_reward = self.init_training()\n",
        "\n",
        "        # Add this line to track violations\n",
        "        total_violations = 0\n",
        "\n",
        "        # # Create fresh network for each episode to avoid memory corruption\n",
        "        # fresh_network = create_pypsa_network()\n",
        "        # self.env.network = fresh_network\n",
        "\n",
        "        state, _ = self.env.reset()  # Gymnasium format returns (obs, info)\n",
        "\n",
        "        self.model.train() # Set model to training mode\n",
        "\n",
        "        while True:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            states.append(state_tensor)\n",
        "\n",
        "            # Get action predictions and values\n",
        "            action_mean, value_pred = self.model(state_tensor)\n",
        "\n",
        "\n",
        "\n",
        "            # Split actor output into alpha and beta parameters\n",
        "            action_dim = self.env.action_space.shape[0]\n",
        "            alpha_raw, beta_raw = torch.split(action_mean, action_dim, dim=-1)\n",
        "\n",
        "            # Ensure alpha, beta > 1 for well-behaved Beta distribution\n",
        "            alpha = torch.nn.functional.softplus(alpha_raw) + 1.0\n",
        "            beta = torch.nn.functional.softplus(beta_raw) + 1.0\n",
        "\n",
        "            # Create Beta distribution for continuous actions in [0,1]\n",
        "            dist = torch.distributions.Beta(alpha, beta)\n",
        "            action = dist.sample()\n",
        "\n",
        "            # No clamping needed - Beta distribution naturally outputs [0,1]\n",
        "            action_clamped = action\n",
        "\n",
        "            log_prob_action = dist.log_prob(action).sum(dim=-1)  # Sum over action dimensions\n",
        "\n",
        "            # Step environment with numpy action\n",
        "            action_np = action_clamped.detach().cpu().numpy().flatten()\n",
        "            state, reward, terminated, truncated, info = self.env.step(action_np)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            #accumulate violations for the epsiode\n",
        "            total_violations += sum(info['constraint_violations'].values())\n",
        "\n",
        "            actions.append(action_clamped)\n",
        "            actions_log_probability.append(log_prob_action)\n",
        "            values.append(value_pred)\n",
        "            rewards.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        states=torch.cat(states).to(self.device)#converts the list of individual states into a sinlem tensor that is necessary for later processing\n",
        "        #Creates a single tensor with dimensions like (N, state_dim), where: N is the number of states collected in the episode; state_dim is the dimensionality of each state.\n",
        "        #torch.cat() expects a sequence (e.g. list or tuple) of PyTorch tensors as input.\n",
        "        actions=torch.cat(actions).to(self.device)\n",
        "        #Note that, in the loop, both state and action are PyTorch tensors so that states and actions are both lists of PyTorch tensors\n",
        "        actions_log_probability=torch.cat(actions_log_probability).to(self.device)\n",
        "        values=torch.cat(values).squeeze(-1).to(self.device)# .squeeze removes a dimension of size 1 only from tensor at the specified position, in this case, -1, the last dimesion in the tensor. Note that .squeeze() does not do anything if the size of the dimension at the specified potision is not 1.\n",
        "        # print(f\"rewards NaNs: {torch.isnan(torch.tensor(rewards, dtype=torch.float32)).any()}\")\n",
        "        # print(f\"values NaNs: {torch.isnan(torch.tensor(values, dtype=torch.float32)).any()}\")\n",
        "        returns = self.calculate_returns(rewards)\n",
        "        advantages = self.calculate_advantages(returns, values)\n",
        "\n",
        "        # print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
        "        # print(f\"advantages NaNs (after calculation): {torch.isnan(advantages).any()}\")\n",
        "\n",
        "        return episode_reward, states, actions, actions_log_probability, advantages, returns, total_violations\n",
        "\n",
        "\n",
        "    def update_policy(self,\n",
        "            states,\n",
        "            actions,\n",
        "            actions_log_probability_old,\n",
        "            advantages,\n",
        "            returns):\n",
        "        #print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
        "        total_policy_loss = 0\n",
        "        total_value_loss = 0\n",
        "        actions_log_probability_old = actions_log_probability_old.detach()\n",
        "        actions=actions.detach()\n",
        "\n",
        "        # print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
        "        # print(f\"advantages NaNs (after calculation): {torch.isnan(advantages).any()}\")\n",
        "\n",
        "\n",
        "        #detach() is used to remove the tensor from the computation graph, meaning no gradients will be calculated for that tensor when performing backpropagation.\n",
        "        #In this context, it's used to ensure that the old actions and log probabilities do not participate in the gradient computation during the optimization of the policy, as we want to update the model based on the current policy rather than the old one.\n",
        "        #print(type(states), type(actions),type(actions_log_probability_old), type(advantages), type(returns))\n",
        "        training_results_dataset= TensorDataset(\n",
        "                states,\n",
        "                actions,\n",
        "                actions_log_probability_old,\n",
        "                advantages,\n",
        "                returns) #TensorDataset class expects all the arguments passed to it to be tensors (or other compatible types like NumPy arrays, which will be automatically converted to tensor\n",
        "        batch_dataset = DataLoader(\n",
        "                training_results_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False)\n",
        "        #creates a DataLoader instance in PyTorch, which is used to load the training_results_dataset in batches during training.\n",
        "        #batch_size defines how many samples will be included in each batch. The dataset will be divided into batches of size BATCH_SIZE. The model will then process one batch at a time, rather than all of the data at once,\n",
        "        #shuffle argument controls whether or not the data will be shuffled before being split into batches.\n",
        "        #Because shuffle is false, dataloader will provide the batches in the order the data appears in training_results_dataset. In this case, the batches will be formed from consecutive entries in the dataset, and the observations will appear in the same sequence as they are stored in the dataset.\n",
        "        for _ in range(self.PPO_steps):\n",
        "            for batch_idx, (states,actions,actions_log_probability_old, advantages, returns) in enumerate(batch_dataset):\n",
        "                #get new log prob of actions for all input states\n",
        "                action_mean, value_pred = self.model(states)\n",
        "                value_pred = value_pred.squeeze(-1)\n",
        "\n",
        "                # For continuous actions with Beta distribution\n",
        "                action_dim = self.env.action_space.shape[0]\n",
        "                alpha_raw, beta_raw = torch.split(action_mean, action_dim, dim=-1)\n",
        "\n",
        "                # Ensure alpha, beta > 1 for well-behaved Beta distribution\n",
        "                alpha = torch.nn.functional.softplus(alpha_raw) + 1.0\n",
        "                beta = torch.nn.functional.softplus(beta_raw) + 1.0\n",
        "\n",
        "                probability_distribution_new = torch.distributions.Beta(alpha, beta)\n",
        "                entropy = probability_distribution_new.entropy().sum(dim=-1)\n",
        "\n",
        "                #estimate new log probabilities using old actions\n",
        "                actions_log_probability_new = probability_distribution_new.log_prob(actions).sum(dim=-1)\n",
        "                # # Check for NaN or Inf in log probabilities\n",
        "                # if torch.isnan(actions_log_probability_old).any() or torch.isinf(actions_log_probability_old).any():\n",
        "                #     print(\"NaN or Inf detected in actions_log_probability_old!\")\n",
        "                #     return  # You can return or handle this case as needed\n",
        "\n",
        "                # if torch.isnan(actions_log_probability_new).any() or torch.isinf(actions_log_probability_new).any():\n",
        "                #     print(\"NaN or Inf detected in actions_log_probability_new!\")\n",
        "                #     return  # You can return or handle this case as needed\n",
        "\n",
        "                # print(f\"actions_log_probability_old NaNs: {torch.isnan(actions_log_probability_old).any()}\")\n",
        "                # print(f\"actions_log_probability_new NaNs: {torch.isnan(actions_log_probability_new).any()}\")\n",
        "                # print(f\"advantages NaNs: {torch.isnan(advantages).any()}\")\n",
        "\n",
        "                surrogate_loss = self.calculate_surrogate_loss(\n",
        "                    actions_log_probability_old,\n",
        "                    actions_log_probability_new,\n",
        "                    advantages\n",
        "                )\n",
        "\n",
        "                # print(f\"Surrogate Loss NaNs: {torch.isnan(surrogate_loss).any()}\")\n",
        "                # print(f\"Entropy NaNs: {torch.isnan(entropy).any()}\")\n",
        "                # print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
        "                # print(f\"Value Predictions NaNs: {torch.isnan(value_pred).any()}\")\n",
        "\n",
        "                policy_loss, value_loss = self.calculate_losses(\n",
        "                    surrogate_loss,\n",
        "                    entropy,\n",
        "                    returns,\n",
        "                    value_pred\n",
        "                )\n",
        "                self.optimizer.zero_grad() #clear existing gradietns in the optimizer (so that these don't propagate accross multiple .backward(). Ensures each optimization step uses only the gradients computed during the current batch.\n",
        "\n",
        "                # Skip backward pass if loss is NaN\n",
        "                if torch.isnan(policy_loss).any():\n",
        "                    print(\"NaN detected in policy_loss - skipping backward pass!\")\n",
        "                    continue\n",
        "                if torch.isnan(value_loss).any():\n",
        "                    print(\"NaN detected in value_loss - skipping backward pass!\")\n",
        "                    continue\n",
        "\n",
        "                policy_loss.backward() #computes gradients for policy_loss with respect to the agent's parameters\n",
        "                # #Check for NaN gradients after policy_loss backward\n",
        "                # for param in self.model.parameters():\n",
        "                #     if param.grad is not None:  # Check if gradients exist for this parameter\n",
        "                #         if torch.isnan(param.grad).any():\n",
        "                #             print(\"NaN gradient detected in policy_loss!\")\n",
        "                # #             return\n",
        "                value_loss.backward()\n",
        "                # Check for NaN gradients after value_loss backwardor param in self.model.parameters():\n",
        "                # for param in self.model.parameters():\n",
        "                #     if param.grad is not None:  # Check if gradients exist for this parameter\n",
        "                #         if torch.isnan(param.grad).any():\n",
        "                #             print(\"NaN gradient detected in value_loss!\")\n",
        "                #             return\n",
        "\n",
        "                self.optimizer.step()\n",
        "                #The update step is based on the learning rate and other hyperparameters of the optimizer\n",
        "                # The parameters of the agent are adjusted to reduce the policy and value losses.\n",
        "                total_policy_loss += policy_loss.item() #accumulate the scalar value of the policy loss for logging/ analysis\n",
        "                #policy_loss.item() extracts the numerical value of the loss tensor (detaching it from the computational graph).\n",
        "                #This value is added to total_policy_loss to compute the cumulative loss over all batches in the current PPO step.\n",
        "                #Result: tracks the total policy loss for the current training epoch\n",
        "                # The loss over the whole dataset is the sum of the losses over all batches.\n",
        "                #The training dataset is split into batches during the training process. Each batch represents a subset of the collected training data from one episode.\n",
        "                # Loss calculation is performed for each batch (policy loss and value loss)\n",
        "                # for each batch, gradients are calculated with respect to the total loss for that batch and the optimizer then updates the network parameters using these gradients.\n",
        "                # this is because the surrogate loss is only calculated over a single batch of data\n",
        "                #look at the formula for surrogate loss.\n",
        "                # It is written in terms of an expectation ˆ Et[. . .] that indicates the empirical average over a finite batch of samples.\n",
        "                # This means you have collected a set of data (time steps) from the environment, and you're averaging over these data points. The hat symbol implies you're approximating the true expectation with a finite sample of data from the environment. This empirical average can be computed as the mean of values from the sampled transitions\n",
        "                # the expectation is taken over all the data you've collected\n",
        "                #If you're training with multiple batches (i.e., collecting data in chunks), then you can think of the expectation as being computed over each batch.\n",
        "                #The overall expectation can indeed be seen as the sum of expectations computed for each batch, but The expectation of the sum is generally not exactly equal to the sum of the expectations unless the samples are independent, but in practical reinforcement learning algorithms, it's typically a good enough approximation\n",
        "                #For samples to be independent, the outcome of one sample must not provide any information about the outcome of another. Specifically, in the context of reinforcement learning, this means that the states, actions, rewards, and subsequent states observed in different time steps or different episodes should be independent of each other.\n",
        "                total_value_loss += value_loss.item()\n",
        "                #Notice that we are calculating an empirical average, which is already an approximation on the true value (the true expectation would be the average over an infinite amount of data, and the empirical average is the average over the finite amount of data that we have collected).\n",
        "                #But furthermore, we are approximating even the empirical average istelf. The empirical average is the average over all our collected datal, but here we actually batch our data, calculate average over each batch and then sum these averages, which is not exaclty equal to the average of the sums (but is a decent approximation).\n",
        "        return total_policy_loss / self.PPO_steps, total_value_loss / self.PPO_steps\n",
        "\n",
        "    def train(self):\n",
        "        train_rewards = []\n",
        "        # test_rewards = []\n",
        "        # policy_losses = []\n",
        "        # value_losses = []\n",
        "        #lens = []\n",
        "\n",
        "        for episode in range(1, self.max_episodes + 1):\n",
        "            # Perform a forward pass and collect experience\n",
        "            train_reward, states, actions, actions_log_probability, advantages, returns, violations = self.forward_pass()\n",
        "\n",
        "            # Update the policy using the experience collected\n",
        "            policy_loss, value_loss = self.update_policy(\n",
        "                states,\n",
        "                actions,\n",
        "                actions_log_probability,\n",
        "                advantages,\n",
        "                returns)\n",
        "            # test_reward = self.evaluate()\n",
        "\n",
        "            # # Visualize the environment if it supports rendering (currently this is done once each episode - might want to change to once every multiple of episodes)\n",
        "            # if hasattr(self.env, \"render\") and callable(getattr(self.env, \"render\", None)):\n",
        "            #   self.env.render()\n",
        "\n",
        "            # Log the results\n",
        "            # policy_losses.append(policy_loss)\n",
        "            # value_losses.append(value_loss)\n",
        "            train_rewards.append(train_reward)\n",
        "            # # run these when back online\n",
        "            # self.run[\"policy_loss\"].log(policy_loss)\n",
        "            # self.run[\"value_loss\"].log(value_loss)\n",
        "            self.run[\"train_reward\"].log(train_reward)\n",
        "            self.run[\"total_violation\"].log(violations)\n",
        "\n",
        "            # Calculate the mean of recent rewards and losses for display\n",
        "            mean_train_rewards = np.mean(train_rewards[-self.n_trials:])\n",
        "            #mean_test_rewards = np.mean(test_rewards[-self.n_trials:])\n",
        "            # mean_abs_policy_loss = np.mean(np.abs(policy_losses[-self.n_trials:]))\n",
        "            # mean_abs_value_loss = np.mean(np.abs(value_losses[-self.n_trials:]))\n",
        "\n",
        "            # Print results at specified intervals\n",
        "            if episode % self.print_interval == 0:\n",
        "                print(f'Episode: {episode:3} | \\\n",
        "                    Train Rewards: {train_reward:3.1f} \\\n",
        "                    Violations: {violations}\\\n",
        "                    Mean Train Rewards: {mean_train_rewards:3.1f}' )\n",
        "                    # \\\n",
        "                    # | Mean Abs Policy Loss: {mean_abs_policy_loss:2.2f} \\\n",
        "                    # | Mean Abs Value Loss: {mean_abs_value_loss:2.2f} ')\n",
        "\n",
        "\n",
        "\n",
        "                                    # | Mean Test Rewards: {mean_test_rewards:3.1f} \\\n",
        "                                    #| \"Episode Len: {np.mean(lens[-self.n_trials:])}\n",
        "\n",
        "\n",
        "\n",
        "            # # Check if reward threshold is reached\n",
        "            # if mean_test_rewards >= self.reward_threshold:\n",
        "            #     print(f'Reached reward threshold in {episode} episodes')\n",
        "            #     break\n",
        "        # Check if the environment has a close method before calling it\n",
        "        # if hasattr(self.env, \"close\") and callable(getattr(self.env, \"close\", None)):\n",
        "        #   self.env.close() #Close environment visualisation after training is done.\n",
        "        return train_rewards\n",
        "\n",
        "def plot_train_rewards(train_rewards, reward_threshold):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(train_rewards, label='Training Reward')\n",
        "    plt.xlabel('Episode', fontsize=20)\n",
        "    plt.ylabel('Training Reward', fontsize=20)\n",
        "    plt.hlines(reward_threshold, 0, len(train_rewards), color='y')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_test_rewards(test_rewards, reward_threshold):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(test_rewards, label='Testing Reward')\n",
        "    plt.xlabel('Episode', fontsize=20)\n",
        "    plt.ylabel('Testing Reward', fontsize=20)\n",
        "    plt.hlines(reward_threshold, 0, len(test_rewards), color='y')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_losses(policy_losses, value_losses):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(value_losses, label='Value Losses')\n",
        "    plt.plot(policy_losses, label='Policy Losses')\n",
        "    plt.xlabel('Episode', fontsize=20)\n",
        "    plt.ylabel('Loss', fontsize=20)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "acf82e05",
      "metadata": {
        "id": "acf82e05"
      },
      "outputs": [],
      "source": [
        "import ipdb\n",
        "class EnvDispatchReplacement(EnvDispatchConstr):\n",
        "    \"\"\"\n",
        "    Environment using the Replacement reward method instead of Summation.\n",
        "\n",
        "    Inherits from Env2Gen1LoadConstr but modifies the reward calculation\n",
        "    to implement the replacement method from the RL-OPF paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,network_file, constraint_penalty_factor=100, offset_k=2500, test_start_date='2013-12-01 00:00:00',\n",
        "                 fixed_episode_length=None):\n",
        "        \"\"\"\n",
        "        Initialize the replacement reward environment.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        network_file : str\n",
        "            Path to the PyPSA network file\n",
        "        episode_length : int, optional\n",
        "            Length of episodes (defaults to total snapshots)\n",
        "        constraint_penalty_factor : float\n",
        "            Penalty factor for constraint violations\n",
        "        offset_k : float\n",
        "            Offset value for replacement reward method\n",
        "        test_start_date : str\n",
        "            Start date for test period (everything from this date onwards is test data)\n",
        "        fixed_episode_length : int, optional\n",
        "            Fixed episode length if specified, otherwise episodes are variable\n",
        "        \"\"\"\n",
        "        # Call parent constructor - this will initialize all base attributes\n",
        "        super().__init__(\n",
        "            network_file=network_file,\n",
        "            constraint_penalty_factor=constraint_penalty_factor,\n",
        "            test_start_date=test_start_date,\n",
        "            fixed_episode_length=fixed_episode_length\n",
        "        )\n",
        "\n",
        "        # Add replacement-specific attributes\n",
        "        self.offset_k = offset_k\n",
        "        self.reward_method = \"replacement\"\n",
        "\n",
        "        # Store initialization parameters for offset calculation\n",
        "        self.k_method = \"mean\"  # Default method for k calculation\n",
        "        self.k_samples = 1000  # Default number of samples for k calculation\n",
        "\n",
        "    def calculate_constrained_reward(self):\n",
        "        \"\"\"\n",
        "        Calculate reward using replacement method with dynamic constraint checking.\n",
        "\n",
        "        Replacement method:\n",
        "        - If all constraints satisfied: return -J(s) + k\n",
        "        - If constraints violated: return -P(s)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get base reward from objective function (negative for minimization)\n",
        "            base_reward = self._calculate_reward()\n",
        "            # Get constraint violations using the same logic as the parent class\n",
        "            # But we need to implement it here directly instead of calling super()\n",
        "            current_snapshot = self.network.snapshots[self.snapshot_idx]\n",
        "\n",
        "            # Initialize constraint tracking\n",
        "            constraint_results = {\n",
        "                'all_satisfied': True,\n",
        "                'violations': {},\n",
        "                'total_violation': 0.0,\n",
        "                'violations_by_group': {}\n",
        "            }\n",
        "\n",
        "            # 1. Check slack generator constraints\n",
        "            slack_generators = self.network.generators[self.network.generators.control == \"Slack\"].index\n",
        "            if not slack_generators.empty:\n",
        "                for gen_name in slack_generators:\n",
        "                    # Get actual power output after power flow\n",
        "                    p_actual = self.network.generators_t.p.loc[current_snapshot, gen_name]\n",
        "\n",
        "                    # Get limits\n",
        "                    p_min = self.network.generators.loc[gen_name, 'p_min_pu'] * self.network.generators.loc[gen_name, 'p_nom']\n",
        "                    p_max = self.network.generators.loc[gen_name, 'p_max_pu'] * self.network.generators.loc[gen_name, 'p_nom']\n",
        "\n",
        "                    # Check lower bound\n",
        "                    if p_actual < p_min:\n",
        "                        violation = float(p_min - p_actual)\n",
        "                        constraint_name = f\"Generator-slack-p-lower[snapshot={current_snapshot},Generator={gen_name}]\"\n",
        "                        constraint_results['violations'][constraint_name] = violation\n",
        "                        constraint_results['total_violation'] += violation\n",
        "                        constraint_results['all_satisfied'] = False\n",
        "\n",
        "                    # Check upper bound\n",
        "                    if p_actual > p_max:\n",
        "                        violation = float(p_actual - p_max)\n",
        "                        constraint_name = f\"Generator-slack-p-upper[snapshot={current_snapshot},Generator={gen_name}]\"\n",
        "                        constraint_results['violations'][constraint_name] = violation\n",
        "                        constraint_results['total_violation'] += violation\n",
        "                        constraint_results['all_satisfied'] = False\n",
        "\n",
        "            # 2. Check line flow constraints (CORRECTED)\n",
        "            for line_name in self.network.lines.index:\n",
        "                # Get line parameters\n",
        "                s_nom = self.network.lines.loc[line_name, 's_nom']\n",
        "                s_max_pu = 1.0  # Default, or get from lines_t.s_max_pu if it exists\n",
        "\n",
        "                # Calculate active power limit (this is what PyPSA's linear constraints check)\n",
        "                s_max = s_max_pu * s_nom\n",
        "\n",
        "                # Get active power flow from the linear power flow\n",
        "                # In PyPSA's linear formulation, this is the 's' variable value\n",
        "                p0 = abs(self.network.lines_t.p0.loc[current_snapshot, line_name])\n",
        "\n",
        "                # Check if active power flow exceeds limit\n",
        "                if p0 > s_max:\n",
        "                    violation = float(p0 - s_max)\n",
        "                    constraint_name = f\"Line-fix-s-upper[snapshot={current_snapshot},Line={line_name}]\"\n",
        "                    constraint_results['violations'][constraint_name] = violation\n",
        "                    constraint_results['total_violation'] += violation\n",
        "                    constraint_results['all_satisfied'] = False\n",
        "\n",
        "            # Apply replacement method\n",
        "            if constraint_results['all_satisfied']:\n",
        "                # All constraints satisfied: return optimization reward + offset k\n",
        "                constrained_reward = base_reward + self.offset_k\n",
        "            else:\n",
        "                # Constraints violated: return only penalty (negative)\n",
        "                total_violation = float(constraint_results['total_violation'])\n",
        "                constrained_reward = -self.penalty_factor * total_violation\n",
        "\n",
        "            # Ensure reward is a scalar\n",
        "            if hasattr(constrained_reward, '__len__'):\n",
        "                constrained_reward = float(constrained_reward)\n",
        "\n",
        "            return constrained_reward, constraint_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating replacement reward: {e}\")\n",
        "            # Fall back to base reward on error\n",
        "            return self._calculate_reward(), {\n",
        "                'all_satisfied': True,\n",
        "                'violations': {},\n",
        "                'total_violation': 0.0\n",
        "            }\n",
        "\n",
        "    def get_reward_method_info(self):\n",
        "        \"\"\"\n",
        "        Get information about the reward method being used.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict: Information about the reward method\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'method': 'replacement',\n",
        "            'offset_k': self.offset_k,\n",
        "            'k_method': self.k_method,\n",
        "            'k_samples': self.k_samples,\n",
        "            'penalty_factor': self.penalty_factor,\n",
        "            'train_snapshots': self.train_snapshots,\n",
        "            'test_snapshots': self.test_snapshots,\n",
        "            'test_start_date': str(self.test_start_date),\n",
        "            'fixed_episode_length': self.fixed_episode_length\n",
        "        }\n",
        "\n",
        "# network_file_path= \"/Users/antoniagrindrod/Documents/pypsa-earth_project/pypsa-earth-RL/networks/elec_s_10_ec_lc1.0_1h.nc\"\n",
        "# input_dir=\"/Users/antoniagrindrod/Documents/pypsa-earth_project/pypsa-earth-RL/RL/var_constraint_map\"\n",
        "# replacement_reward_offset=calculate_offset_k_initialization(network_file=network_file_path, input_dir=input_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Tl544b00EmYT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl544b00EmYT",
        "outputId": "fc0faf2c-f651-43ba-e809-1f087a53f587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "O-8KVHl6vJkb",
      "metadata": {
        "id": "O-8KVHl6vJkb"
      },
      "outputs": [],
      "source": [
        "# Simple Sweep Tracker - Only tracks successful completions\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "class SimpleSweepTracker:\n",
        "    def __init__(self, base_dir='/content/drive/MyDrive/Colab_Notebooks/sweep_results'):\n",
        "        self.base_dir = base_dir\n",
        "        self.config_file = os.path.join(base_dir, 'all_configs.json')\n",
        "        self.completed_file = os.path.join(base_dir, 'completed_runs.csv')\n",
        "\n",
        "        os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    def setup_configs(self):\n",
        "        \"\"\"Generate and save all configurations - run once\"\"\"\n",
        "        configs = self._generate_all_configs()\n",
        "\n",
        "        with open(self.config_file, 'w') as f:\n",
        "            json.dump(configs, f, indent=2)\n",
        "\n",
        "        print(f\"✓ Created {len(configs)} configurations\")\n",
        "        print(f\"✓ Saved to: {self.config_file}\")\n",
        "        return len(configs)\n",
        "\n",
        "    def _generate_all_configs(self):\n",
        "        \"\"\"Generate configs using sweep_params like your original code\"\"\"\n",
        "        base_params = {\n",
        "            \"optimizer_name\": \"Adam\",\n",
        "            \"MAX_EPISODES\": 10000,\n",
        "            \"PRINT_INTERVAL\": 100,\n",
        "            \"N_TRIALS\": 8,\n",
        "            \"DROPOUT\": 0,\n",
        "            \"network_file\": \"elec_s_10_ec_lc1.0_1h.nc\",\n",
        "            \"optimization_result_file\": \"elec_s_10_ec_lc1.0_1h_Test_Objective.txt\"\n",
        "        }\n",
        "\n",
        "        # Parameters to sweep (from your original code)\n",
        "        sweep_params = {\n",
        "            \"LEARNING_RATE\": [1e-4, 3e-4, 1e-3, 3e-3],\n",
        "            \"EPSILON\": [0.1, 0.2, 0.3],\n",
        "            \"ENTROPY_COEFFICIENT\": [0.01, 0.05, 0.1],\n",
        "            \"HIDDEN_DIMENSIONS\": [32, 64, 128],\n",
        "            \"PPO_STEPS\": [8, 16],\n",
        "            \"BATCH_SIZE\": [128, 256],\n",
        "            \"DISCOUNT_FACTOR\": [0.95, 0.99],\n",
        "            \"constraint_penalty_factor\": [0, 25, 50, 100],\n",
        "            \"env_class\": [\"EnvDispatchConstr\", \"EnvDispatchReplacement\"]\n",
        "        }\n",
        "\n",
        "        # Your priority configurations (these are good combinations to try first)\n",
        "        priority_configs = [\n",
        "            {\"LEARNING_RATE\": 1e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFICIENT\": 0.1,\n",
        "             \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "             \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\": 0, \"episode_length\": 4,\n",
        "             \"env_class\": \"EnvDispatchConstr\"},\n",
        "            {\"LEARNING_RATE\": 3e-4, \"EPSILON\": 0.2, \"ENTROPY_COEFFICIENT\": 0.05,\n",
        "             \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "             \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\": 0, \"episode_length\": 4,\n",
        "             \"env_class\": \"EnvDispatchConstr\"},\n",
        "            {\"LEARNING_RATE\": 1e-4, \"EPSILON\": 0.1, \"ENTROPY_COEFFICIENT\": 0.01,\n",
        "             \"HIDDEN_DIMENSIONS\": 128, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "             \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\": 0, \"episode_length\": 4,\n",
        "             \"env_class\": \"EnvDispatchConstr\"},\n",
        "            {\"LEARNING_RATE\": 3e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFICIENT\": 0.1,\n",
        "             \"HIDDEN_DIMENSIONS\": 32, \"PPO_STEPS\": 8, \"BATCH_SIZE\": 128,\n",
        "             \"DISCOUNT_FACTOR\": 0.95, \"constraint_penalty_factor\": 0, \"episode_length\": 4,\n",
        "             \"env_class\": \"EnvDispatchConstr\"},\n",
        "            {\"LEARNING_RATE\": 1e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFICIENT\": 0.1,\n",
        "             \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "             \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\": 100, \"episode_length\": 4,\n",
        "             \"env_class\": \"EnvDispatchConstr\"},\n",
        "            {\"LEARNING_RATE\": 3e-4, \"EPSILON\": 0.2, \"ENTROPY_COEFFICIENT\": 0.05,\n",
        "             \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "             \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\": 100, \"episode_length\": 4,\n",
        "             \"env_class\": \"EnvDispatchConstr\"},\n",
        "            {\"LEARNING_RATE\": 1e-4, \"EPSILON\": 0.1, \"ENTROPY_COEFFICIENT\": 0.01,\n",
        "             \"HIDDEN_DIMENSIONS\": 128, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "             \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\": 100, \"episode_length\": 4,\n",
        "             \"env_class\": \"EnvDispatchConstr\"},\n",
        "            {\"LEARNING_RATE\": 3e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFICIENT\": 0.1,\n",
        "             \"HIDDEN_DIMENSIONS\": 32, \"PPO_STEPS\": 8, \"BATCH_SIZE\": 128,\n",
        "             \"DISCOUNT_FACTOR\": 0.95, \"constraint_penalty_factor\": 100, \"episode_length\": 4,\n",
        "             \"env_class\": \"EnvDispatchConstr\"},\n",
        "        ]\n",
        "\n",
        "        # Generate random configurations from sweep_params if you want more\n",
        "        import random\n",
        "        random.seed(42)  # For reproducible random configs\n",
        "\n",
        "        num_random_configs = 5  # Add some random combinations\n",
        "        random_configs = []\n",
        "\n",
        "        for _ in range(num_random_configs):\n",
        "            config = {param: random.choice(values) for param, values in sweep_params.items()}\n",
        "            config[\"episode_length\"] = 4  # Add any fixed params for random configs\n",
        "            random_configs.append(config)\n",
        "\n",
        "        # Combine priority and random configs\n",
        "        all_param_configs = priority_configs + random_configs\n",
        "\n",
        "        # Generate configs with seeds - SEED FIRST, then configs\n",
        "        seeds = [42, 123, 7]\n",
        "        all_configs = []\n",
        "        run_id = 0\n",
        "\n",
        "        # For each seed, go through all parameter configs\n",
        "        for seed in seeds:\n",
        "            for config_idx, param_config in enumerate(all_param_configs):\n",
        "                run_config = {\n",
        "                    **base_params,\n",
        "                    **param_config,\n",
        "                    \"run_id\": run_id,\n",
        "                    \"config_name\": f\"Config_{config_idx}_Seed_{seed}\",\n",
        "                    \"config_idx\": config_idx,\n",
        "                    \"seed\": seed\n",
        "                }\n",
        "                all_configs.append(run_config)\n",
        "                run_id += 1\n",
        "\n",
        "        return all_configs\n",
        "\n",
        "    def get_completed_runs(self):\n",
        "        \"\"\"Get list of successfully completed run IDs\"\"\"\n",
        "        if not os.path.exists(self.completed_file):\n",
        "            return []\n",
        "        df = pd.read_csv(self.completed_file)\n",
        "        return df['run_id'].tolist()\n",
        "\n",
        "    def mark_completed(self, run_id, results):\n",
        "        \"\"\"Mark a run as completed and save results\"\"\"\n",
        "        # Prepare the data\n",
        "        completion_data = {\n",
        "            'run_id': run_id,\n",
        "            'completed_at': datetime.now().isoformat(),\n",
        "            **results  # Add all the results\n",
        "        }\n",
        "\n",
        "        # Append to completed runs file\n",
        "        new_row = pd.DataFrame([completion_data])\n",
        "\n",
        "        if os.path.exists(self.completed_file):\n",
        "            existing_df = pd.read_csv(self.completed_file)\n",
        "            combined_df = pd.concat([existing_df, new_row], ignore_index=True)\n",
        "        else:\n",
        "            combined_df = new_row\n",
        "\n",
        "        combined_df.to_csv(self.completed_file, index=False)\n",
        "        print(f\"✓ Marked run {run_id} as completed\")\n",
        "\n",
        "    def get_next_run(self):\n",
        "        \"\"\"Get the next run that hasn't been completed\"\"\"\n",
        "        # Load all configs\n",
        "        with open(self.config_file, 'r') as f:\n",
        "            all_configs = json.load(f)\n",
        "\n",
        "        # Get completed run IDs\n",
        "        completed_ids = self.get_completed_runs()\n",
        "\n",
        "        # Find first uncompleted run\n",
        "        for config in all_configs:\n",
        "            if config['run_id'] not in completed_ids:\n",
        "                return config\n",
        "\n",
        "        return None  # All done!\n",
        "\n",
        "    def status(self):\n",
        "        \"\"\"Show simple status\"\"\"\n",
        "        # Load configs\n",
        "        with open(self.config_file, 'r') as f:\n",
        "            total_configs = len(json.load(f))\n",
        "\n",
        "        completed_ids = self.get_completed_runs()\n",
        "        completed_count = len(completed_ids)\n",
        "        remaining = total_configs - completed_count\n",
        "\n",
        "        print(f\"📊 Sweep Status:\")\n",
        "        print(f\"   Total runs: {total_configs}\")\n",
        "        print(f\"   Completed: {completed_count} ({completed_count/total_configs*100:.1f}%)\")\n",
        "        print(f\"   Remaining: {remaining}\")\n",
        "\n",
        "        if remaining > 0:\n",
        "            next_run = self.get_next_run()\n",
        "            if next_run:\n",
        "                print(f\"   Next run: {next_run['run_id']} ({next_run['config_name']})\")\n",
        "        else:\n",
        "            print(\"   🎉 All runs completed!\")\n",
        "\n",
        "        return {'total': total_configs, 'completed': completed_count, 'remaining': remaining}\n",
        "\n",
        "    def show_recent_completions(self, n=5):\n",
        "        \"\"\"Show recently completed runs\"\"\"\n",
        "        if not os.path.exists(tracker.completed_file):\n",
        "            print(\"No completed runs yet\")\n",
        "            return\n",
        "\n",
        "        df = pd.read_csv(tracker.completed_file)\n",
        "        if len(df) == 0:\n",
        "            print(\"No completed runs yet\")\n",
        "            return\n",
        "\n",
        "        df['completed_at'] = pd.to_datetime(df['completed_at'])\n",
        "        recent = df.sort_values('completed_at', ascending=False).head(n)\n",
        "\n",
        "        print(f\"🕒 Last {min(n, len(df))} completed runs:\")\n",
        "        for _, row in recent.iterrows():\n",
        "            time_str = row['completed_at'].strftime('%m/%d %H:%M')\n",
        "            print(f\"   Run {row['run_id']}: {time_str}\")\n",
        "\n",
        "def run_single_experiment(run_id, tracker):\n",
        "    \"\"\"Run a single experiment\"\"\"\n",
        "    # Get the config\n",
        "    with open(tracker.config_file, 'r') as f:\n",
        "        all_configs = json.load(f)\n",
        "\n",
        "    config = next(c for c in all_configs if c['run_id'] == run_id)\n",
        "    print(f\"🚀 Starting run {run_id}: {config['config_name']}\")\n",
        "\n",
        "    try:\n",
        "        # Your training code here (same as before)\n",
        "        results = execute_training_simple(config)\n",
        "\n",
        "        # Mark as completed\n",
        "        tracker.mark_completed(run_id, results)\n",
        "        print(f\"✅ Run {run_id} completed successfully\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Run {run_id} failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def execute_training_simple(config):\n",
        "    \"\"\"Your training code - simplified version\"\"\"\n",
        "    # Set up paths\n",
        "    gdrive_base = '/content/drive/MyDrive/Colab_Notebooks/'\n",
        "    network_file_path = os.path.join(gdrive_base, \"networks_1_year_connected\", config[\"network_file\"])\n",
        "    optimization_result_path = os.path.join(gdrive_base, \"optimized_network\", config[\"optimization_result_file\"])\n",
        "\n",
        "    # Load optimization result\n",
        "    with open(optimization_result_path, 'r') as f:\n",
        "        objective = float(f.read().strip())\n",
        "\n",
        "    objective=-1*objective# multiply by negative one since comparing to reward found by RL agent\n",
        "\n",
        "    # Set seeds\n",
        "    seed = config[\"seed\"]\n",
        "    import random, numpy as np, torch\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Calculate offset (only needed for replacement env)\n",
        "    replacement_reward_offset = None\n",
        "    if config[\"env_class\"] == \"EnvDispatchReplacement\":\n",
        "        replacement_reward_offset = calculate_offset_k_initialization(\n",
        "            network_file=network_file_path\n",
        "        )\n",
        "\n",
        "    # Initialize Neptune\n",
        "    API_TOKEN = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1ODQwZjA5OS05MDFmLTQ2MWYtYWJiMi0yMDkzYmEwNzgzMzEifQ==\"\n",
        "    PROJECT_NAME = \"EnergyGridRL/elec-s-10-ec-lc10-1h-Dispatch\"\n",
        "    run = neptune.init_run(\n",
        "    project=PROJECT_NAME,\n",
        "    api_token=API_TOKEN,\n",
        "    name=config['config_name'],\n",
        "    tags=[\"hyperparameter_sweep\"])\n",
        "\n",
        "    # Log parameters\n",
        "    for key, value in config.items():\n",
        "        if key not in ['run_id', 'config_name', 'config_idx']:\n",
        "            run[f\"parameters/{key}\"] = value\n",
        "\n",
        "    if replacement_reward_offset is not None:\n",
        "        run[\"replacement_reward_offset\"] = replacement_reward_offset\n",
        "\n",
        "    # Create environment based on env_class\n",
        "    if config[\"env_class\"] == \"EnvDispatchConstr\":\n",
        "        env = EnvDispatchConstr(\n",
        "            network_file=network_file_path,\n",
        "            constraint_penalty_factor=config[\"constraint_penalty_factor\"]\n",
        "        )\n",
        "    elif config[\"env_class\"] == \"EnvDispatchReplacement\":\n",
        "        env = EnvDispatchReplacement(\n",
        "            network_file=network_file_path,\n",
        "            constraint_penalty_factor=config[\"constraint_penalty_factor\"],\n",
        "            offset_k=replacement_reward_offset\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown env_class: {config['env_class']}\")\n",
        "\n",
        "    env.seed(seed)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    agent = PPO_agent(\n",
        "        env=env, run=run, device=device,\n",
        "        hidden_dimensions=config[\"HIDDEN_DIMENSIONS\"],\n",
        "        dropout=config[\"DROPOUT\"],\n",
        "        discount_factor=config[\"DISCOUNT_FACTOR\"],\n",
        "        optimizer_name=config[\"optimizer_name\"],\n",
        "        max_episodes=config[\"MAX_EPISODES\"],\n",
        "        print_interval=config[\"PRINT_INTERVAL\"],\n",
        "        PPO_steps=config[\"PPO_STEPS\"],\n",
        "        n_trials=config[\"N_TRIALS\"],\n",
        "        epsilon=config[\"EPSILON\"],\n",
        "        entropy_coefficient=config[\"ENTROPY_COEFFICIENT\"],\n",
        "        learning_rate=config[\"LEARNING_RATE\"],\n",
        "        batch_size=config[\"BATCH_SIZE\"],\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    train_rewards = agent.train()\n",
        "\n",
        "    # Evaluate\n",
        "    test_results = evaluate_agent_on_test_data(agent, env, objective)\n",
        "\n",
        "    # Log to Neptune\n",
        "    run[\"training_results/final_reward\"] = train_rewards[-1]\n",
        "    run[\"training_results/mean_last_100_reward\"] = np.mean(train_rewards[-100:])\n",
        "    run[\"training_results/best_reward\"] = np.max(train_rewards)\n",
        "\n",
        "    for key, value in test_results.items():\n",
        "        run[f\"test_results/{key}\"] = value\n",
        "\n",
        "    # Save trained model to Neptune\n",
        "    save_model_to_neptune(agent, run, config)\n",
        "\n",
        "    run.stop()\n",
        "\n",
        "    # Return key results for local tracking\n",
        "    results = {\n",
        "        'env_class': config[\"env_class\"],\n",
        "        'mape': test_results.get('mape'),\n",
        "        'rl_total_objective': test_results.get('rl_total_objective'),\n",
        "        'optimal_total_objective': test_results.get('optimal_total_objective'),\n",
        "        'constraint_violation_percentage': test_results.get('constraint_violation_percentage'),\n",
        "        'final_reward': train_rewards[-1],\n",
        "        'mean_last_100_reward': np.mean(train_rewards[-100:]),\n",
        "        'best_reward': np.max(train_rewards),\n",
        "        'neptune_run_id': run.get_run_url().split('/')[-1],  # Get Neptune run ID for model retrieval\n",
        "    }\n",
        "\n",
        "    if replacement_reward_offset is not None:\n",
        "        results['replacement_reward_offset'] = replacement_reward_offset\n",
        "\n",
        "    return results\n",
        "\n",
        "# === SIMPLE USAGE ===\n",
        "\n",
        "def setup_sweep():\n",
        "    \"\"\"Run this once to set up all configs\"\"\"\n",
        "    tracker = SimpleSweepTracker()\n",
        "    tracker.setup_configs()\n",
        "    tracker.status()\n",
        "    return tracker\n",
        "\n",
        "def run_next():\n",
        "    \"\"\"Run the next pending experiment\"\"\"\n",
        "    tracker = SimpleSweepTracker()\n",
        "    next_config = tracker.get_next_run()\n",
        "\n",
        "    if next_config is None:\n",
        "        print(\"🎉 All experiments completed!\")\n",
        "        return False\n",
        "\n",
        "    success = run_single_experiment(next_config['run_id'], tracker)\n",
        "    tracker.status()  # Show updated status\n",
        "    return success\n",
        "\n",
        "def check_status():\n",
        "    \"\"\"Check current status\"\"\"\n",
        "    tracker = SimpleSweepTracker()\n",
        "    tracker.status()\n",
        "    tracker.show_recent_completions()\n",
        "\n",
        "def run_specific(run_id):\n",
        "    \"\"\"Run a specific experiment by ID\"\"\"\n",
        "    tracker = SimpleSweepTracker()\n",
        "    return run_single_experiment(run_id, tracker)\n",
        "\n",
        "def resume_sweep(max_runs=5):\n",
        "    \"\"\"Resume running experiments\"\"\"\n",
        "    tracker = SimpleSweepTracker()\n",
        "    completed = 0\n",
        "\n",
        "    while completed < max_runs:\n",
        "        next_config = tracker.get_next_run()\n",
        "        if next_config is None:\n",
        "            print(\"🎉 All experiments completed!\")\n",
        "            break\n",
        "\n",
        "        success = run_single_experiment(next_config['run_id'], tracker)\n",
        "        if success:\n",
        "            completed += 1\n",
        "\n",
        "        tracker.status()\n",
        "\n",
        "    print(f\"Completed {completed} runs in this session\")\n",
        "    return completed\n",
        "\n",
        "def save_model_to_neptune(agent, neptune_run, config):\n",
        "    \"\"\"Save the trained model to Neptune\"\"\"\n",
        "    import tempfile\n",
        "    import torch\n",
        "    import json\n",
        "\n",
        "    # Create temporary files for the model components\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        # Save actor-critic network state dict\n",
        "        # Assuming agent.actor_critic exists and is the combined model\n",
        "        if hasattr(agent, 'actor_critic') and agent.actor_critic is not None:\n",
        "             actor_critic_path = os.path.join(temp_dir, 'actor_critic.pt')\n",
        "             torch.save(agent.actor_critic.state_dict(), actor_critic_path)\n",
        "        elif hasattr(agent, 'model') and agent.model is not None:\n",
        "            # If using agent.model as the combined model\n",
        "            actor_critic_path = os.path.join(temp_dir, 'actor_critic.pt')\n",
        "            torch.save(agent.model.state_dict(), actor_critic_path)\n",
        "        else:\n",
        "            print(\"Warning: Agent does not have a combined actor_critic or model attribute to save.\")\n",
        "            return\n",
        "\n",
        "\n",
        "        # Save complete model info (for reconstruction)\n",
        "        model_info = {\n",
        "            'config': config,\n",
        "            'state_space_dim': agent.env.observation_space.shape[0],\n",
        "            'action_space_dim': agent.env.action_space.shape[0],\n",
        "            'hidden_dimensions': config['HIDDEN_DIMENSIONS'],\n",
        "            'dropout': config['DROPOUT'],\n",
        "            'model_architecture': str(agent.model) if hasattr(agent, 'model') and agent.model is not None else \"N/A\" # Architecture as string\n",
        "        }\n",
        "        model_info_path = os.path.join(temp_dir, 'model_info.json')\n",
        "        with open(model_info_path, 'w') as f:\n",
        "            json.dump(model_info, f, indent=2)\n",
        "\n",
        "        # Upload to Neptune\n",
        "        neptune_run[\"model/actor_critic\"].upload(actor_critic_path)\n",
        "        neptune_run[\"model/model_info\"].upload(model_info_path)\n",
        "        neptune_run[\"model/saved_at\"] = datetime.now().isoformat()\n",
        "\n",
        "        print(f\"✓ Model saved to Neptune for run {config['run_id']}\")\n",
        "\n",
        "def load_model_from_neptune(neptune_run_id, my_api=None):\n",
        "    \"\"\"Load a trained model from Neptune\"\"\"\n",
        "    import neptune\n",
        "    import tempfile\n",
        "    import torch\n",
        "\n",
        "    my_api = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1ODQwZjA5OS05MDFmLTQ2MWYtYWJiMi0yMDkzYmEwNzgzMzEifQ==\"\n",
        "    PROJECT_NAME = \"EnergyGridRL/elec-s-10-ec-lc10-1h-Dispatch\"\n",
        "\n",
        "    run = neptune.init_run(\n",
        "    project=PROJECT_NAME,\n",
        "    api_token=my_api,\n",
        "    run=neptune_run_id,\n",
        "    mode=\"read-only\")\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        # Download model files\n",
        "        model_info_path = os.path.join(temp_dir, 'model_info.json')\n",
        "        actor_critic_path = os.path.join(temp_dir, 'actor_critic.pt')\n",
        "\n",
        "        run[\"model/model_info\"].download(model_info_path)\n",
        "        run[\"model/actor_critic\"].download(actor_critic_path)\n",
        "\n",
        "        # Load model info\n",
        "        with open(model_info_path, 'r') as f:\n",
        "            model_info = json.load(f)\n",
        "\n",
        "        # Load state dict\n",
        "        state_dict = torch.load(actor_critic_path, map_location='cpu')\n",
        "\n",
        "        run.stop()\n",
        "\n",
        "        return {\n",
        "            'state_dict': state_dict,\n",
        "            'model_info': model_info,\n",
        "            'config': model_info['config']\n",
        "        }\n",
        "def reconstruct_agent_from_saved_model(model_data, network_file_path=None):\n",
        "    \"\"\"Reconstruct a PPO agent from saved model data\"\"\"\n",
        "    config = model_data['config']\n",
        "\n",
        "    # Reconstruct environment\n",
        "    if network_file_path is None:\n",
        "        gdrive_base = '/content/drive/MyDrive/Colab_Notebooks/'\n",
        "        network_file_path = os.path.join(gdrive_base, \"networks_1_year_connected\", config[\"network_file\"])\n",
        "\n",
        "    if config[\"env_class\"] == \"EnvDispatchConstr\":\n",
        "        env = EnvDispatchConstr(\n",
        "    network_file=network_file_path,  # Full path: \"/content/drive/MyDrive/Colab_Notebooks/networks_1_year_connected/elec_s_10_ec_lc1.0_1h.nc\"\n",
        "    constraint_penalty_factor=config[\"constraint_penalty_factor\"]\n",
        ")\n",
        "    elif config[\"env_class\"] == \"EnvDispatchReplacement\":\n",
        "        # You'd need to recalculate or save the offset_k\n",
        "        replacement_reward_offset = calculate_offset_k_initialization(\n",
        "            network_file=network_file_path\n",
        "        )#CHANGE THIS SO LOAD CALCULATED VALUE\n",
        "        env = EnvDispatchReplacement(\n",
        "            network_file=network_file_path,\n",
        "            constraint_penalty_factor=config[\"constraint_penalty_factor\"],\n",
        "            offset_k=replacement_reward_offset\n",
        "        )\n",
        "\n",
        "    # Create agent (without training)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    agent = PPO_agent(\n",
        "        env=env,\n",
        "        run=None,  # No Neptune logging needed\n",
        "        device=device,\n",
        "        hidden_dimensions=config[\"HIDDEN_DIMENSIONS\"],\n",
        "        dropout=config[\"DROPOUT\"],\n",
        "        discount_factor=config[\"DISCOUNT_FACTOR\"],\n",
        "        optimizer_name=config[\"optimizer_name\"],\n",
        "        max_episodes=1,  # Not training\n",
        "        print_interval=1000,\n",
        "        PPO_steps=config[\"PPO_STEPS\"],\n",
        "        n_trials=config[\"N_TRIALS\"],\n",
        "        epsilon=config[\"EPSILON\"],\n",
        "        entropy_coefficient=config[\"ENTROPY_COEFFICIENT\"],\n",
        "        learning_rate=config[\"LEARNING_RATE\"],\n",
        "        batch_size=config[\"BATCH_SIZE\"],\n",
        "        seed=config[\"seed\"]\n",
        "    )\n",
        "\n",
        "    # Load the trained weights\n",
        "    if 'state_dict' in model_data:\n",
        "        agent.model.load_state_dict(model_data['state_dict']) # Assuming agent.model is used\n",
        "    elif 'actor_critic_state_dict' in model_data:\n",
        "        agent.model.load_state_dict(model_data['actor_critic_state_dict']) # Fallback if named differently\n",
        "    else:\n",
        "        print(\"Warning: Could not find state dictionary in model data.\")\n",
        "\n",
        "    # Set to evaluation mode\n",
        "    agent.model.eval() # Assuming agent.model is the combined model\n",
        "\n",
        "    print(f\"✓ Agent reconstructed from saved model (Run {config['run_id']})\")\n",
        "    return agent, env\n",
        "\n",
        "def get_best_models(top_k=5, metric='mape'):\n",
        "    \"\"\"Get the top K models based on a performance metric\"\"\"\n",
        "    tracker = SimpleSweepTracker()\n",
        "\n",
        "    if not os.path.exists(tracker.completed_file):\n",
        "        print(\"No completed runs found\")\n",
        "        return []\n",
        "\n",
        "    df = pd.read_csv(tracker.completed_file)\n",
        "\n",
        "    if metric not in df.columns:\n",
        "        print(f\"Metric '{metric}' not found. Available metrics: {df.columns.tolist()}\")\n",
        "        return []\n",
        "\n",
        "    # Sort by metric (assuming lower is better for mape, higher for rewards)\n",
        "    if metric in ['mape', 'constraint_violation_percentage']:\n",
        "        best_runs = df.nsmallest(top_k, metric)\n",
        "    else:\n",
        "        best_runs = df.nlargest(top_k, metric)\n",
        "\n",
        "    print(f\"🏆 Top {top_k} models by {metric}:\")\n",
        "    for i, (_, row) in enumerate(best_runs.iterrows(), 1):\n",
        "        print(f\"{i}. Run {row['run_id']}: {metric}={row[metric]:.4f} (Neptune: {row.get('neptune_run_id', 'N/A')})\")\n",
        "\n",
        "    return best_runs.to_dict('records')\n",
        "\n",
        "def load_best_model(metric='mape', from_neptune=True):\n",
        "    \"\"\"Load the single best model\"\"\"\n",
        "    best_models = get_best_models(top_k=1, metric=metric)\n",
        "    if not best_models:\n",
        "        return None\n",
        "\n",
        "    best_model = best_models[0]\n",
        "\n",
        "    if from_neptune and 'neptune_run_id' in best_model:\n",
        "        print(f\"Loading best model from Neptune (Run {best_model['run_id']})...\")\n",
        "        model_data = load_model_from_neptune(best_model['neptune_run_id'])\n",
        "        return reconstruct_agent_from_saved_model(model_data)\n",
        "    else:\n",
        "        print(\"No model file information found\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7QHVWSy7-7A6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QHVWSy7-7A6",
        "outputId": "4b9a454c-71c8-4e7d-f693-c825c51f1e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Created 39 configurations\n",
            "✓ Saved to: /content/drive/MyDrive/Colab_Notebooks/sweep_results/all_configs.json\n",
            "📊 Sweep Status:\n",
            "   Total runs: 39\n",
            "   Completed: 0 (0.0%)\n",
            "   Remaining: 39\n",
            "   Next run: 0 (Config_0_Seed_42)\n"
          ]
        }
      ],
      "source": [
        "# Set up all configurations - this creates the files\n",
        "tracker = setup_sweep()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "uHlCMPhh_FjT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHlCMPhh_FjT",
        "outputId": "e9847955-1030-4c25-9077-277dc86942f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First run configuration:\n",
            "  optimizer_name: Adam\n",
            "  MAX_EPISODES: 10000\n",
            "  PRINT_INTERVAL: 100\n",
            "  N_TRIALS: 8\n",
            "  DROPOUT: 0\n",
            "  network_file: elec_s_10_ec_lc1.0_1h.nc\n",
            "  optimization_result_file: elec_s_10_ec_lc1.0_1h_Test_Objective.txt\n",
            "  LEARNING_RATE: 0.001\n",
            "  EPSILON: 0.3\n",
            "  ENTROPY_COEFFICIENT: 0.1\n",
            "  HIDDEN_DIMENSIONS: 64\n",
            "  PPO_STEPS: 16\n",
            "  BATCH_SIZE: 256\n",
            "  DISCOUNT_FACTOR: 0.99\n",
            "  constraint_penalty_factor: 0\n",
            "  episode_length: 4\n",
            "  env_class: EnvDispatchConstr\n",
            "  run_id: 0\n",
            "  config_name: Config_0_Seed_42\n",
            "  config_idx: 0\n",
            "  seed: 42\n"
          ]
        }
      ],
      "source": [
        "# See what the first experiment looks like\n",
        "tracker = SimpleSweepTracker()\n",
        "first_config = tracker.get_next_run()\n",
        "print(\"First run configuration:\")\n",
        "for key, value in first_config.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5CeE_UqB_IHU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CeE_UqB_IHU",
        "outputId": "a9ca73f5-6f8e-4265-e427-578ff927b286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting run 0: Config_0_Seed_42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/EnergyGridRL/elec-s-10-ec-lc10-1h-Dispatch/e/EL1-40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypsa.network.io:Importing network from PyPSA version v0.0.0 while current version is v0.35.2. Read the release notes at https://pypsa.readthedocs.io/en/latest/release_notes.html to prepare your network for import.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed ZA0 0 PHS: set max_hours to 8.0\n",
            "Fixed ZA0 5 PHS: set max_hours to 8.0\n",
            "Fixed ZA0 6 hydro: corrected max_hours from 3831.6270020496813 to 6.0\n",
            "=== FIXING ARTIFICIAL LINES WITH REASONABLE CAPACITY ===\n",
            "Found 3 artificial lines to fix:\n",
            "\n",
            "🔧 Fixing: lines new ZA0 4 <-> ZA2 0 AC\n",
            "    Connected buses: ZA0 4 ↔ ZA2 0\n",
            "    Bus demands: ZA0 4: 15945.8 MW, ZA2 0: 452.6 MW\n",
            "    s_nom: 0.0 → 47837.3 MW\n",
            "    s_nom_extendable: → False\n",
            "\n",
            "🔧 Fixing: lines new ZA0 0 <-> ZA1 0 AC\n",
            "    Connected buses: ZA0 0 ↔ ZA1 0\n",
            "    Bus demands: ZA0 0: 3513.0 MW, ZA1 0: 1386.9 MW\n",
            "    s_nom: 0.0 → 10538.9 MW\n",
            "    s_nom_extendable: → False\n",
            "\n",
            "🔧 Fixing: lines new ZA0 0 <-> ZA3 0 AC\n",
            "    Connected buses: ZA0 0 ↔ ZA3 0\n",
            "    Bus demands: ZA0 0: 3513.0 MW, ZA3 0: 721.1 MW\n",
            "    s_nom: 0.0 → 10538.9 MW\n",
            "    s_nom_extendable: → False\n",
            "Exact test start date not found. Using nearest: 2013-12-01 00:00:00\n",
            "Using variable episode length, max: None\n",
            "Initialized Beta parameters to produce uniform-like distribution\n",
            "> \u001b[0;32m/tmp/ipython-input-676271826.py\u001b[0m(612)\u001b[0;36mscale_action\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    611 \u001b[0;31m                \u001b[0;31m# Ensure feasible range exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 612 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mp_set_min\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mp_set_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    613 \u001b[0;31m                    \u001b[0;31m# If no feasible range, clip p_dispatch and recalculate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n",
            "> \u001b[0;32m/tmp/ipython-input-676271826.py\u001b[0m(627)\u001b[0;36mscale_action\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    626 \u001b[0;31m                \u001b[0;31m# Scale p_set action from [0,1] to [p_set_min, p_set_max]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 627 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mp_set_max\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mp_set_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    628 \u001b[0;31m                    \u001b[0mscaled_p_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_set_min\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstorage_p_set_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_set_max\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp_set_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n",
            "> \u001b[0;32m/tmp/ipython-input-676271826.py\u001b[0m(612)\u001b[0;36mscale_action\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    611 \u001b[0;31m                \u001b[0;31m# Ensure feasible range exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 612 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mp_set_min\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mp_set_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    613 \u001b[0;31m                    \u001b[0;31m# If no feasible range, clip p_dispatch and recalculate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n",
            "> \u001b[0;32m/tmp/ipython-input-676271826.py\u001b[0m(627)\u001b[0;36mscale_action\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    626 \u001b[0;31m                \u001b[0;31m# Scale p_set action from [0,1] to [p_set_min, p_set_max]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 627 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mp_set_max\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mp_set_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    628 \u001b[0;31m                    \u001b[0mscaled_p_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_set_min\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstorage_p_set_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_set_max\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp_set_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-676271826.py:602: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in scalar divide\n",
            "\n",
            "/tmp/ipython-input-676271826.py:603: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in scalar divide\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/tmp/ipython-input-676271826.py\u001b[0m(612)\u001b[0;36mscale_action\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    611 \u001b[0;31m                \u001b[0;31m# Ensure feasible range exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 612 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mp_set_min\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mp_set_max\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    613 \u001b[0;31m                    \u001b[0;31m# If no feasible range, clip p_dispatch and recalculate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> p_set_min > p_set_max\n",
            "np.True_\n",
            "ipdb> bounds['eff_dispatch']\n",
            "np.float64(0.9)\n",
            "ipdb> min(p_dispatch_raw, p_nom * 0.5)\n",
            "np.float64(300.0)\n",
            "ipdb> p_dispatch_raw * store_coeff - p_dispatch_raw / bounds['eff_dispatch']\n",
            "np.float64(-664.9222373962402)\n",
            "ipdb> p_set_min_from_soc = (base_term - bounds['max_net_energy_per_dt']) / store_coeff                     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<stdin>:1: RuntimeWarning:\n",
            "\n",
            "divide by zero encountered in scalar divide\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ipdb> store_coeff\n",
            "np.float64(0.0)\n",
            "ipdb> print(self.network.storage_units.efficiency_store)\n",
            "StorageUnit\n",
            "ZA0 0 PHS      0.866025\n",
            "ZA0 5 PHS      0.866025\n",
            "ZA0 6 hydro    0.000000\n",
            "Name: efficiency_store, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Run just the first experiment\n",
        "success = run_next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vp-YtCMt_NnF",
      "metadata": {
        "id": "Vp-YtCMt_NnF"
      },
      "outputs": [],
      "source": [
        "# See what happened\n",
        "check_status()\n",
        "\n",
        "# Look at the completed run details\n",
        "tracker = SimpleSweepTracker()\n",
        "print(\"\\nCompleted run details:\")\n",
        "\n",
        "if os.path.exists(tracker.completed_file):\n",
        "    df = pd.read_csv(tracker.completed_file)\n",
        "    if not df.empty:\n",
        "        print(df.iloc[0])  # Show first completed run\n",
        "    else:\n",
        "        print(\"Completed runs file is empty.\")\n",
        "else:\n",
        "    print(\"Completed runs file not found.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pypsa-earth-rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
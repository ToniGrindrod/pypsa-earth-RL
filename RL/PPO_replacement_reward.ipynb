{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "PJfT8e2MPYnj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PJfT8e2MPYnj",
        "outputId": "e28505b2-608f-4fef-d059-fcc08ee97c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypsa in /usr/local/lib/python3.11/dist-packages (0.35.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pypsa) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pypsa) (1.16.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from pypsa) (2.2.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from pypsa) (2025.7.1)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.11/dist-packages (from pypsa) (1.7.2)\n",
            "Requirement already satisfied: linopy>=0.4 in /usr/local/lib/python3.11/dist-packages (from pypsa) (0.5.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pypsa) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from pypsa) (5.24.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from pypsa) (0.13.2)\n",
            "Requirement already satisfied: geopandas>=0.9 in /usr/local/lib/python3.11/dist-packages (from pypsa) (1.1.1)\n",
            "Requirement already satisfied: shapely<2.1 in /usr/local/lib/python3.11/dist-packages (from pypsa) (2.0.7)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.11/dist-packages (from pypsa) (3.5)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.11/dist-packages (from pypsa) (2.1.0)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.11/dist-packages (from pypsa) (0.35.0)\n",
            "Requirement already satisfied: highspy in /usr/local/lib/python3.11/dist-packages (from pypsa) (1.11.0)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas>=0.9->pypsa) (0.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas>=0.9->pypsa) (25.0)\n",
            "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from geopandas>=0.9->pypsa) (3.7.1)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.11/dist-packages (from linopy>=0.4->pypsa) (1.4.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from linopy>=0.4->pypsa) (0.12.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from linopy>=0.4->pypsa) (2.11.0)\n",
            "Requirement already satisfied: dask>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from linopy>=0.4->pypsa) (2025.5.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from linopy>=0.4->pypsa) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from linopy>=0.4->pypsa) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->pypsa) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->pypsa) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->pypsa) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypsa) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypsa) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypsa) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypsa) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypsa) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pypsa) (3.2.3)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.11/dist-packages (from netcdf4->pypsa) (1.6.4.post1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netcdf4->pypsa) (2025.8.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->pypsa) (8.5.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (2025.3.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (6.0.2)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (8.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24->pypsa) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask>=0.18.0->linopy>=0.4->pypsa) (3.23.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask>=0.18.0->linopy>=0.4->pypsa) (1.0.0)\n",
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.11/dist-packages (1.14.0)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (3.1.45)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (11.3.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.10.1)\n",
            "Requirement already satisfied: boto3>=1.28.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (1.40.6)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (11.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (8.2.1)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (1.0.0)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (3.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from neptune-client) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from neptune-client) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (1.17.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (4.14.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (2.5.0)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.11/dist-packages (from neptune-client) (1.8.0)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.6 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.28.0->neptune-client) (1.40.6)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.28.0->neptune-client) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.28.0->neptune-client) (0.13.1)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.1.1)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.6)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.0.2)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.11/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.20.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->neptune-client) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->neptune-client) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->neptune-client) (2025.8.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.25.0)\n",
            "Requirement already satisfied: importlib-resources>=1.3 in /usr/local/lib/python3.11/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->neptune-client) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->neptune-client) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neptune-client) (2025.2)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.11/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.26.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (0.1.1)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (24.11.1)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.2.2)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (2.9.0.20250809)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypsa\n",
        "!pip install neptune-client\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dbo-BPT6uEJC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbo-BPT6uEJC",
        "outputId": "c9b7cf9c-a674-477e-8e21-91c00be6f798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.11/dist-packages (0.13.13)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.19.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "#Install debugger\n",
        "!pip install ipdb\n",
        "import ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ce4a5972",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce4a5972",
        "outputId": "936f835c-de12-45d7-b2b0-d1c2a1d3b5de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[neptune] [warning] NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs-legacy.neptune.ai/setup/upgrading/\n"
          ]
        }
      ],
      "source": [
        "import pypsa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "import gc\n",
        "import psutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import neptune\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5LqhyczzAmD-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LqhyczzAmD-",
        "outputId": "420906ec-ba26-40af-a772-b1d149fce2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab_Notebooks/networks\n"
          ]
        }
      ],
      "source": [
        "# Upload files to Google Drive first, then:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Access your files\n",
        "%cd /content/drive/MyDrive/Colab_Notebooks/networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780fd1d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def load_mappings(network_file, input_dir=\"var_constraint_map\"):\n",
        "    \"\"\"\n",
        "    Load previously saved variable ID to name mapping and constraints from files in Google Drive.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    network_file : str\n",
        "        Path to the network file used to create the mappings\n",
        "    input_dir : str, optional\n",
        "        Directory relative to Google Drive MyDrive where the mapping files are stored\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (var_id_to_name, constraints) - The loaded mappings\n",
        "    \"\"\"\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Create the full path to Google Drive directory\n",
        "    gdrive_base = '/content/drive/MyDrive/Colab_Notebooks'\n",
        "    full_input_dir = os.path.join(gdrive_base, input_dir)\n",
        "    \n",
        "    # Get the network filename without path or extension\n",
        "    network_name = os.path.basename(network_file)\n",
        "    network_name = os.path.splitext(network_name)[0]\n",
        "    \n",
        "    # Create input filenames\n",
        "    var_map_file = os.path.join(full_input_dir, f\"{network_name}_var_id_to_name.pkl\")\n",
        "    constraints_file = os.path.join(full_input_dir, f\"{network_name}_constraints.pkl\")\n",
        "    \n",
        "    # Check if files exist\n",
        "    if not os.path.exists(var_map_file) or not os.path.exists(constraints_file):\n",
        "        raise FileNotFoundError(f\"Mapping files for {network_name} not found in {full_input_dir}\")\n",
        "    \n",
        "    # Load mappings from files\n",
        "    with open(var_map_file, 'rb') as f:\n",
        "        var_id_to_name = pickle.load(f)\n",
        "        \n",
        "    with open(constraints_file, 'rb') as f:\n",
        "        constraints = pickle.load(f)\n",
        "    \n",
        "    print(f\"Loaded variable mapping from: {var_map_file}\")\n",
        "    print(f\"Loaded constraints from: {constraints_file}\")\n",
        "    \n",
        "    return var_id_to_name, constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9db9ca87",
      "metadata": {
        "id": "9db9ca87"
      },
      "outputs": [],
      "source": [
        "def create_pypsa_network(network_file):\n",
        "    \"\"\"Create a PyPSA network from the .nc file.\"\"\"\n",
        "    # Initialize network\n",
        "    n = pypsa.Network(network_file)\n",
        "\n",
        "    return n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "hbJnmzIwvPrO",
      "metadata": {
        "id": "hbJnmzIwvPrO"
      },
      "outputs": [],
      "source": [
        "def calculate_offset_k_initialization(envClass,k_method='mean', k_samples=1000):\n",
        "   \"\"\"\n",
        "   Calculate the offset k for replacement reward method.\n",
        "   This ensures valid states always have higher rewards than invalid states.\n",
        "\n",
        "   Parameters:\n",
        "   -----------\n",
        "   k_samples : int\n",
        "       Number of random samples to use for estimation\n",
        "   k_method : str\n",
        "       Method to calculate k: 'mean' or 'worst_case'\n",
        "\n",
        "   Returns:\n",
        "   --------\n",
        "   float: Offset value k\n",
        "   \"\"\"\n",
        "   print(f\"Sampling {k_samples} random states to calculate offset k...\")\n",
        "\n",
        "   temp_env = envClass()\n",
        "   #this initializes episode_length to number of snapshots and constraint_penalty_factor to None.\n",
        "   #I'm just making this env to access certain attributes/ methods; which should be fine since none of these attributes/methods reference these two parameters.\n",
        "\n",
        "   objective_values = []\n",
        "   successful_samples = 0\n",
        "\n",
        "   try:\n",
        "       for i in range(k_samples):\n",
        "           try:\n",
        "               # STEP 1: Sample ONE random snapshot (random state)\n",
        "               random_snapshot_idx = np.random.randint(0, temp_env.total_snapshots)\n",
        "\n",
        "               # STEP 2: Sample ONE random action for that state\n",
        "               random_action = np.random.random(temp_env.n_non_slack)\n",
        "               scaled_action = temp_env.scale_action(random_action)\n",
        "\n",
        "               # STEP 3: Apply the action to the sampled state\n",
        "               for j, gen_name in enumerate(temp_env.non_slack_names):\n",
        "                   temp_env.network.generators_t.p_set.iloc[random_snapshot_idx,\n",
        "                       temp_env.network.generators_t.p_set.columns.get_loc(gen_name)] = scaled_action[j]\n",
        "\n",
        "               # STEP 4: Evaluate objective for this ONE state-action combination\n",
        "               temp_env.snapshot_idx = random_snapshot_idx  # Set snapshot for evaluation\n",
        "               temp_env.network.lpf()\n",
        "               obj_value = temp_env._evaluate_stored_objective()\n",
        "               objective_values.append(obj_value)\n",
        "               successful_samples += 1\n",
        "\n",
        "               # Progress indicator every 200 samples\n",
        "               if (i + 1) % 200 == 0:\n",
        "                   print(f\"  Completed {i + 1}/{k_samples} samples...\")\n",
        "\n",
        "           except Exception as e:\n",
        "               # Skip failed samples but continue\n",
        "               if i < 5:  # Only print first few errors to avoid spam\n",
        "                   print(f\"  Sample {i} failed: {e}\")\n",
        "               continue\n",
        "\n",
        "       # Calculate offset based on method\n",
        "       if objective_values:\n",
        "           if k_method == 'worst_case':\n",
        "               k = abs(max(objective_values))\n",
        "               print(f\"  Using worst-case method: k = |{max(objective_values):.2f}| = {k:.2f}\")\n",
        "           else:  # method == 'mean'\n",
        "               mean_val = np.mean(objective_values)\n",
        "               k = abs(mean_val)\n",
        "               print(f\"  Using mean method: k = |{mean_val:.2f}| = {k:.2f}\")\n",
        "\n",
        "           print(f\"  Successfully sampled {successful_samples}/{k_samples} states\")\n",
        "           print(f\"  Objective value range: [{min(objective_values):.2f}, {max(objective_values):.2f}]\")\n",
        "       else:\n",
        "           print(\"  Warning: No successful samples, using default k value\")\n",
        "           k = 2500  # Default fallback value\n",
        "\n",
        "   finally:\n",
        "       # Clean up temporary environment (optional but good practice)\n",
        "       del temp_env\n",
        "\n",
        "   return k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cae12ed9",
      "metadata": {
        "id": "cae12ed9"
      },
      "outputs": [],
      "source": [
        "def get_variable_value(network,var_name):\n",
        "        \"\"\"\n",
        "        Get the current value of a single optimization variable from the network.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        network : pypsa.Network\n",
        "            The PyPSA network object\n",
        "        var_name : str\n",
        "            Variable name like 'Generator-p[snapshot=now,Generator=coal_gen_1]'\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float : current value of the variable\n",
        "        \"\"\"\n",
        "        # Parse the variable name\n",
        "        # Format: ComponentName-attribute[dimension1=value1,dimension2=value2,...]\n",
        "\n",
        "        # Split on first '[' to separate base name from coordinates\n",
        "        if '[' in var_name:\n",
        "            base_name, coords_str = var_name.split('[', 1)\n",
        "            coords_str = coords_str.rstrip(']')  # Remove trailing ']'\n",
        "        else:\n",
        "            base_name = var_name\n",
        "            coords_str = \"\"\n",
        "\n",
        "        # Parse base name: ComponentName-attribute\n",
        "        component_name, attribute = base_name.split('-', 1)\n",
        "\n",
        "        # Parse coordinates if they exist\n",
        "        coords = {}\n",
        "        if coords_str:\n",
        "            # Split by comma and parse key=value pairs\n",
        "            for coord_pair in coords_str.split(','):\n",
        "                key, value = coord_pair.split('=', 1)\n",
        "                coords[key.strip()] = value.strip()\n",
        "\n",
        "        # Determine if this has time dimension (snapshot)\n",
        "        has_snapshot = 'snapshot' in coords\n",
        "\n",
        "        if has_snapshot:\n",
        "            # Access dynamic dataframe using n.dynamic()\n",
        "            snapshot_value = coords['snapshot']\n",
        "            component_instance = coords[component_name]\n",
        "\n",
        "            # Special handling for branch flow variables\n",
        "            if component_name in network.passive_branch_components and attribute == 's':\n",
        "                # For branch components, 's' is stored as 'p0' in the network\n",
        "                # We can use p0 directly as the value of 's'\n",
        "                try:\n",
        "                    return network.dynamic(component_name)['p0'].loc[snapshot_value, component_instance]\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not get flow value for {var_name}: {e}\")\n",
        "                    return 0.0\n",
        "\n",
        "            # Get dynamic data for normal case\n",
        "            dynamic_data = network.dynamic(component_name)\n",
        "\n",
        "            # Access the specific attribute DataFrame\n",
        "            if attribute in dynamic_data:\n",
        "                return dynamic_data[attribute].loc[snapshot_value, component_instance]\n",
        "            else:\n",
        "                raise KeyError(f\"Attribute {attribute} not found in dynamic data for {component_name}\")\n",
        "\n",
        "        else:\n",
        "            # Access static dataframe using n.static()\n",
        "            component_instance = coords[component_name]\n",
        "\n",
        "            # Get static data\n",
        "            static_data = network.static(component_name)\n",
        "\n",
        "            # Access the value\n",
        "            return static_data.loc[component_instance, attribute]\n",
        "\n",
        "def create_variable_values_mapping(network, variable_names):\n",
        "        \"\"\"\n",
        "        Create a mapping from optimization variable names to their current values in the network.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        network : pypsa.Network\n",
        "            The PyPSA network object\n",
        "        variable_names : list\n",
        "            List of variable names like ['Generator-p[snapshot=now,Generator=coal_gen_1]', ...]\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : mapping from variable name to current value\n",
        "        \"\"\"\n",
        "        var_values = {}\n",
        "\n",
        "        for var_name in variable_names:\n",
        "            try:\n",
        "                value = get_variable_value(network,var_name)\n",
        "                var_values[var_name] = value\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not get value for {var_name}: {e}\")\n",
        "                var_values[var_name] = 0.0  # Default fallback\n",
        "\n",
        "        return var_values\n",
        "\n",
        "def evaluate_objective(var_id_to_name, network, snapshot_idx, only_generator_terms= True):\n",
        "        \"\"\"\n",
        "        Direct evaluation without mock objects.\n",
        "        Only includes terms from the current snapshot in the objective function.\n",
        "        \"\"\"\n",
        "        temp_model = network.optimize.create_model()\n",
        "\n",
        "        # Extract objective components\n",
        "        obj_expr = temp_model.objective\n",
        "        objective_vars = obj_expr.vars.copy()\n",
        "        objective_coeffs = obj_expr.coeffs.copy()\n",
        "        objective_const = obj_expr.const.copy() if hasattr(obj_expr, 'const') else 0\n",
        "\n",
        "        # Get variable name mapping for current network\n",
        "        id_to_name = var_id_to_name\n",
        "\n",
        "        # Get the current snapshot name\n",
        "        current_snapshot = snapshot_idx\n",
        "\n",
        "        # Get current variable values\n",
        "        variable_names = []\n",
        "        var_indices = []\n",
        "        vars_flat = objective_vars.values.flatten()\n",
        "        coeffs_flat = objective_coeffs.values.flatten()\n",
        "\n",
        "        # Filter variables to only include those from the current snapshot\n",
        "        for i, var_id in enumerate(vars_flat):\n",
        "            if var_id != -1 and var_id in id_to_name:\n",
        "                var_name = id_to_name[var_id]\n",
        "\n",
        "                # Filter for generator-related terms if requested\n",
        "                if only_generator_terms:\n",
        "                    # Check if this is a generator-related variable\n",
        "                    # Adjust these patterns based on your specific variable naming\n",
        "                    is_generator_var = ('generator' in var_name.lower())\n",
        "\n",
        "                    if not is_generator_var:\n",
        "                        continue  # Skip non-generator variables\n",
        "\n",
        "                # Check if this variable belongs to the current snapshot\n",
        "                if 'snapshot=' in var_name:\n",
        "                    # Extract the snapshot value from the variable name\n",
        "                    snapshot_part = var_name.split('snapshot=')[1].split(',')[0].split(']')[0]\n",
        "                    if snapshot_part == str(current_snapshot):\n",
        "                        variable_names.append(var_name)\n",
        "                        var_indices.append(i)\n",
        "                else:\n",
        "                    # Include variables without snapshot dimension (like investment variables)\n",
        "                    variable_names.append(var_name)\n",
        "                    var_indices.append(i)\n",
        "\n",
        "        var_values = create_variable_values_mapping(network,variable_names)\n",
        "\n",
        "        # Direct mathematical evaluation using only variables from current snapshot\n",
        "        if var_indices:\n",
        "            result = np.sum(coeffs_flat[var_indices] *\n",
        "                        [var_values.get(name, 0) for name in variable_names]) + \\\n",
        "                    objective_const\n",
        "        else:\n",
        "            # If no variables for this snapshot, just return the constant\n",
        "            result = objective_const\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe8e8c8",
      "metadata": {
        "id": "6fe8e8c8"
      },
      "outputs": [],
      "source": [
        "def evaluate_baseline_reward(network_file, env, var_id_to_name):\n",
        "    network_baseline=create_pypsa_network(network_file)#network to run pypsa optimize on\n",
        "    network_baseline.optimize()\n",
        "    snapshots=network_baseline.snapshots\n",
        "    objective_sum=0\n",
        "    for snapshot_idx in range(len(snapshots)):\n",
        "        objective_sum+=evaluate_objective(var_id_to_name, network_baseline, snapshots[snapshot_idx])\n",
        "    baseline_reward_value=-objective_sum*(env.episode_length/ len(snapshots))#assuming episode length is a multiple of the number of snapshots\n",
        "    #reward is -1 times the objective value\n",
        "    return baseline_reward_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8f1d307",
      "metadata": {
        "id": "b8f1d307"
      },
      "outputs": [],
      "source": [
        "class Env2Gen1LoadConstr(gym.Env):\n",
        "    \"\"\"\n",
        "    OpenAI Gym environment for Optimal Power Flow using PyPSA.\n",
        "    Simple example with 2 generators and 1 load.\n",
        "    Implemented without constraints (omitted bus voltage limits, line limits, etc.).\n",
        "\n",
        "    Action Space: Continuous setpoints for generators within their capacity limits\n",
        "    Observation Space: Network state, represented by:\n",
        "    - the active and reactive power of all loads,\n",
        "    - the reactive power prices of all generators,\n",
        "    - the active power setpoints of all generators\n",
        "    (This follows http://arxiv.org/abs/2403.17831. Additional variables can be added optionally)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,network_file, episode_length=None, constraint_penalty_factor=100, input_dir=\"var_constraint_map\"):\n",
        "        super(Env2Gen1LoadConstr, self).__init__()\n",
        "\n",
        "        # Use provided network or create new one\n",
        "        self.network =create_pypsa_network(network_file)\n",
        "        self.network.optimize() #only do this if you want RL agent to control dispatch of generators\n",
        "        #It will call pypsa's optimization to set all other controllable component attributes to optimal value.\n",
        "        #Later we reset the generator dynamic attributes so that the agent can choose these\n",
        "\n",
        "        #this is if you want RL agent only to control dispatch of generators\n",
        "        self._initialize_optimization_components()\n",
        "        self.penalty_factor=constraint_penalty_factor\n",
        "\n",
        "        # Episode management\n",
        "        self.total_snapshots = len(self.network.snapshots)\n",
        "        self.episode_length = episode_length if episode_length is not None else self.total_snapshots\n",
        "        self.current_step = 0  # Steps within current episode\n",
        "        self.snapshot_idx = 0  # Current snapshot index (cycles through all snapshots)\n",
        "\n",
        "        # Find the slack generator\n",
        "        # The agent will only be able to control the active power setpoints of generators that are not the slack generator\n",
        "        self.slack_generator_idx = self.network.generators[self.network.generators.control == \"Slack\"].index\n",
        "        #Note that there should only ever be one slack bus.\n",
        "        #Need to implement handling for multiple generators control set to \"Slack\",\n",
        "        #In that case, will need to designate one for slack and change control of others\n",
        "        #TO DO: Implement handling for multiple slack generators\n",
        "\n",
        "        #Get indices of all generators that are not the slack generator (after ensuring there is only one slack generator)\n",
        "        self.non_slack_gens = self.network.generators[self.network.generators.control != \"Slack\"].index #must explicitly search again, can't reference results of last query since this may have changed since choosing slack generator\n",
        "        self.non_slack_names = list(self.non_slack_gens)\n",
        "        self.n_non_slack = len(self.non_slack_names)\n",
        "        self.non_slack_df = self.network.generators.loc[self.non_slack_gens]  # Fixed: was non_slack_generators\n",
        "        # The agent will only be able to control the active power setpoints of generators that are not the slack generator\n",
        "\n",
        "        # Get generator limits (in MW)\n",
        "        self.a_min = (self.non_slack_df.p_min_pu * self.non_slack_df.p_nom).values\n",
        "        self.a_max = (self.non_slack_df.p_max_pu * self.non_slack_df.p_nom).values\n",
        "        #TO DO: When generalize, control of the generator will determine the action space of the agent\n",
        "        # Whichever quantity is controllable, according to the control, will be the quantity the agent chooses as its action (e.g. could be active or reactive power)\n",
        "        # Then we would need to change the bounds of the the relevant bounds will be the actions space to be the min and max of whatever the quantity is\n",
        "\n",
        "        # Define action space: continuous setpoints for each generator within limits\n",
        "        # Action space is a Box with shape (n_non_slack,) where each element has value between 0 and 1.\n",
        "        self.action_space = gym.spaces.Box(0, 1, shape=(self.n_non_slack,))\n",
        "\n",
        "        # Define observation space - network state\n",
        "        # This will include: generator outputs, loads, voltages, line flows, etc.\n",
        "        # For simplicity, let's include key network variables\n",
        "        # obs_dim = (self.n_non_slack+1)+len(self.network.loads)\n",
        "        # First term: self.n_non_slack: Active power outputs of all non-slackgenerators! Actually I removed this because that actually corresponds to the action choice.\n",
        "        # TO DO: Later we might need to include a history of the network as part of the observation - to obey ramp constraints etc.\n",
        "        #Second term: Cost function components (start with one each - marginal cost) of all generators, including slack\n",
        "        # Third term: Active power of load demands\n",
        "        # TO DO: include power of storage units and noncontrollable generators (small generators and storage units; not including slack generator) in observation space\n",
        "\n",
        "\n",
        "        #TO DO: If use .pf instead of .lpf, add another of each term for active power AND reactive power\n",
        "        #include the power of the slack generator in the observation, since agent's action (choice of active power for non-slack generators influences power of slack generator, through power flow calculations)\n",
        "\n",
        "        # Initialize the network state\n",
        "        self.reset()\n",
        "\n",
        "        # Observation space is bounded - you may want to adjust these bounds based on your system\n",
        "\n",
        "        # Create observation space\n",
        "        low_bounds, high_bounds = self.create_observation_bounds()\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=low_bounds,\n",
        "            high=high_bounds,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        #Bounds are set according to specific component of the observation\n",
        "        #Could set the bounds of active power of each generator to correspond to min and max p\n",
        "\n",
        "    def _initialize_optimization_components(self):\n",
        "        \"\"\"\n",
        "        Initialize all optimization components in one pass to avoid creating multiple models.\n",
        "        This method:\n",
        "        1. Creates the optimization model once\n",
        "        2. Extracts objective components (vars, coeffs, const)\n",
        "        3. Creates the variable ID to name mapping\n",
        "        4. Extracts constraints\n",
        "        5. Cleans up the model\n",
        "        \"\"\"\n",
        "        # Create model once - this is an expensive operation\n",
        "        temp_model = self.network.optimize.create_model()\n",
        "\n",
        "        # Extract objective components\n",
        "        obj_expr = temp_model.objective\n",
        "        self.objective_vars = obj_expr.vars.copy()\n",
        "        self.objective_coeffs = obj_expr.coeffs.copy()\n",
        "        self.objective_const = obj_expr.const.copy() if hasattr(obj_expr, 'const') else 0\n",
        "\n",
        "        self.var_id_to_name, self.constraints = load_mappings(self.network_file,self.input_dir=\"var_constraint_map\")\n",
        "\n",
        "        # Clean up to free memory\n",
        "        del temp_model, obj_expr\n",
        "        gc.collect()\n",
        "\n",
        "    def create_observation_bounds(self):\n",
        "        \"\"\"\n",
        "        Create bounds for the observation space based on generator costs and load power\n",
        "        \"\"\"\n",
        "        # --- Generator Cost Bounds ---\n",
        "        gen_cost_lower = []\n",
        "        gen_cost_upper = []\n",
        "\n",
        "        for gen in self.network.generators.index:\n",
        "            # Get generator specs\n",
        "            p_nom = self.network.generators.loc[gen, \"p_nom\"]\n",
        "            p_min_pu = self.network.generators.loc[gen, \"p_min_pu\"]\n",
        "            p_max_pu = self.network.generators.loc[gen, \"p_max_pu\"]\n",
        "            marginal_cost = self.network.generators.loc[gen, \"marginal_cost\"]\n",
        "\n",
        "            # Min cost: generator at minimum output\n",
        "            min_cost = p_min_pu * p_nom * marginal_cost\n",
        "\n",
        "            # Max cost: generator at maximum output\n",
        "            max_cost = p_max_pu * p_nom * marginal_cost\n",
        "            #TO DO: add other cost function components if applicable\n",
        "            gen_cost_lower.append(min_cost)\n",
        "            gen_cost_upper.append(max_cost)\n",
        "\n",
        "        # --- Load Power Bounds ---\n",
        "        load_p_lower = []\n",
        "        load_p_upper = []\n",
        "\n",
        "        for load in self.network.loads.index:\n",
        "            # Get load specs\n",
        "            p_nom = self.network.loads.loc[load, \"p_set\"]#TO DO: not sure if this is correct. in general might not set p_set when specifying load component (might instead specifiy p_nom)\n",
        "\n",
        "            # If you have time series data for loads\n",
        "            if hasattr(self.network, \"loads_t\") and not self.network.loads_t.p.empty:\n",
        "                min_load = self.network.loads_t.p.iloc[self.snapshot_idx].min()\n",
        "                max_load = self.network.loads_t.p.iloc[self.snapshot_idx].max()\n",
        "\n",
        "            load_p_lower.append(min_load)\n",
        "            load_p_upper.append(max_load)\n",
        "\n",
        "        # Combine all bounds\n",
        "        low_bounds = np.concatenate([gen_cost_lower, load_p_lower]).astype(np.float32)\n",
        "        high_bounds = np.concatenate([gen_cost_upper, load_p_upper]).astype(np.float32)\n",
        "\n",
        "        return low_bounds, high_bounds\n",
        "\n",
        "    def reset_network(self):\n",
        "        \"\"\"Reset and ensure essential DataFrames exist.\"\"\"\n",
        "        #Note that we do not just create a new network here, as this consumes more memory and previously led to a segmentation fault\n",
        "        # TO DO: For a general network, we would need to reset all time-varying data (i.e. all components with time-varying data)\n",
        "        # Ensure generators_t DataFrames exist and are reset\n",
        "        if not hasattr(self.network, 'generators_t'):\n",
        "            # This shouldn't happen with PyPSA, but just in case\n",
        "            pass\n",
        "\n",
        "        # Initialize/reset generators_t.p_set\n",
        "        if not hasattr(self.network.generators_t, 'p_set') or self.network.generators_t.p_set.empty:\n",
        "            self.network.generators_t.p_set = pd.DataFrame(\n",
        "                0.0,\n",
        "                index=self.network.snapshots,\n",
        "                columns=self.network.generators.index\n",
        "            )\n",
        "        else:\n",
        "            self.network.generators_t.p_set.iloc[:, :] = 0.0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Reset the environment to initial state.\n",
        "        Returns initial observation and info (gymnasium format).\n",
        "        \"\"\"\n",
        "        # Set seed if provided\n",
        "        if seed is not None:\n",
        "            self.seed(seed)\n",
        "\n",
        "        # Reset episode counters\n",
        "        self.current_step = 0\n",
        "        self.snapshot_idx = 0\n",
        "\n",
        "        self.reset_network()\n",
        "        #self._test_hidden_references()\n",
        "        #can run that instead to check if there are hidden references preventing network cleanup\n",
        "\n",
        "        # Get initial observation\n",
        "        obs = self._get_observation()\n",
        "\n",
        "        # Info dict for gymnasium compatibility\n",
        "        info = {\n",
        "            'current_step': self.current_step,\n",
        "            'snapshot_idx': self.snapshot_idx\n",
        "        }\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    def _get_observation(self):\n",
        "        \"\"\"\n",
        "        Get current network state as observation.\n",
        "        \"\"\"\n",
        "        obs_components = []\n",
        "\n",
        "        # obs_dim = (self.n_non_slack+1)+len(self.network.loads)\n",
        "        # First term: self.n_non_slack: Active power outputs of all non-slackgenerators! Actually I removed this because that actually corresponds to the action choice.\n",
        "        # TO DO: Later we might need to include a history of the network as part of the observation - to obey ramp constraints etc.\n",
        "        # Second term: Cost function components (start with one each - marginal cost) of all generators, including slack\n",
        "        # Third term: Active power of load demands\n",
        "\n",
        "\n",
        "        # Generator power costs (for now just marginal) - TO DO: change this to include more components of cost function to make this more accurate\n",
        "        gen_costs = self.network.generators.marginal_cost.values #TO DO: not sure if the marginal_cost should be accessed from generators_t instead (would do generators.marginal_cost.iloc[snapshot_idx].values). But then not static\n",
        "        obs_components.append(gen_costs)\n",
        "\n",
        "        load_demands = np.array([])\n",
        "        # Load demands - handle case where we might have empty loads_t.p\n",
        "        if hasattr(self.network, 'loads_t') and not self.network.loads_t.p.empty:\n",
        "            load_demands = self.network.loads_t.p.iloc[self.snapshot_idx].values\n",
        "        obs_components.append(load_demands)\n",
        "\n",
        "        #The following line should be uncommented when want to enforce network constraints via action masking\n",
        "        #obs_components.append(self._get_mask())\n",
        "\n",
        "        # TO DO: Could later try to add bus voltages etc. and see if difference on agent learning. (Paper says these are optional components of the observation space)\n",
        "        # Concatenate all observation components\n",
        "        observation = np.concatenate(obs_components).astype(np.float32)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def scale_action(self, action):\n",
        "        \"\"\"\n",
        "        Scale action from [0,1] range to [self.a_min, self.a_max] range.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        action : numpy.ndarray\n",
        "            Action values in range [0,1]\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray\n",
        "            Scaled action in range [self.a_min, self.a_max]\n",
        "        \"\"\"\n",
        "        return self.a_min + action * (self.a_max - self.a_min)\n",
        "\n",
        "    def get_variable_value(self, var_name):\n",
        "        \"\"\"\n",
        "        Get the current value of a single optimization variable from the network.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        network : pypsa.Network\n",
        "            The PyPSA network object\n",
        "        var_name : str\n",
        "            Variable name like 'Generator-p[snapshot=now,Generator=coal_gen_1]'\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float : current value of the variable\n",
        "        \"\"\"\n",
        "        # Parse the variable name\n",
        "        # Format: ComponentName-attribute[dimension1=value1,dimension2=value2,...]\n",
        "\n",
        "        # Split on first '[' to separate base name from coordinates\n",
        "        if '[' in var_name:\n",
        "            base_name, coords_str = var_name.split('[', 1)\n",
        "            coords_str = coords_str.rstrip(']')  # Remove trailing ']'\n",
        "        else:\n",
        "            base_name = var_name\n",
        "            coords_str = \"\"\n",
        "\n",
        "        # Parse base name: ComponentName-attribute\n",
        "        component_name, attribute = base_name.split('-', 1)\n",
        "\n",
        "        # Parse coordinates if they exist\n",
        "        coords = {}\n",
        "        if coords_str:\n",
        "            # Split by comma and parse key=value pairs\n",
        "            for coord_pair in coords_str.split(','):\n",
        "                key, value = coord_pair.split('=', 1)\n",
        "                coords[key.strip()] = value.strip()\n",
        "\n",
        "        # Determine if this has time dimension (snapshot)\n",
        "        has_snapshot = 'snapshot' in coords\n",
        "\n",
        "        if has_snapshot:\n",
        "            # Access dynamic dataframe using n.dynamic()\n",
        "            snapshot_value = coords['snapshot']\n",
        "            component_instance = coords[component_name]\n",
        "\n",
        "            # Special handling for branch flow variables\n",
        "            if component_name in self.network.passive_branch_components and attribute == 's':\n",
        "                # For branch components, 's' is stored as 'p0' in the network\n",
        "                # We can use p0 directly as the value of 's'\n",
        "                try:\n",
        "                    return self.network.dynamic(component_name)['p0'].loc[snapshot_value, component_instance]\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not get flow value for {var_name}: {e}\")\n",
        "                    return 0.0\n",
        "\n",
        "            # Get dynamic data for normal case\n",
        "            dynamic_data = self.network.dynamic(component_name)\n",
        "\n",
        "            # Access the specific attribute DataFrame\n",
        "            if attribute in dynamic_data:\n",
        "                return dynamic_data[attribute].loc[snapshot_value, component_instance]\n",
        "            else:\n",
        "                raise KeyError(f\"Attribute {attribute} not found in dynamic data for {component_name}\")\n",
        "\n",
        "        else:\n",
        "            # Access static dataframe using n.static()\n",
        "            component_instance = coords[component_name]\n",
        "\n",
        "            # Get static data\n",
        "            static_data = self.network.static(component_name)\n",
        "\n",
        "            # Access the value\n",
        "            return static_data.loc[component_instance, attribute]\n",
        "\n",
        "\n",
        "    def create_variable_values_mapping(self,variable_names):\n",
        "        \"\"\"\n",
        "        Create a mapping from optimization variable names to their current values in the network.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        network : pypsa.Network\n",
        "            The PyPSA network object\n",
        "        variable_names : list\n",
        "            List of variable names like ['Generator-p[snapshot=now,Generator=coal_gen_1]', ...]\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : mapping from variable name to current value\n",
        "        \"\"\"\n",
        "        var_values = {}\n",
        "\n",
        "        for var_name in variable_names:\n",
        "            try:\n",
        "                value = self.get_variable_value(var_name)\n",
        "                var_values[var_name] = value\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not get value for {var_name}: {e}\")\n",
        "                var_values[var_name] = 0.0  # Default fallback\n",
        "\n",
        "        return var_values\n",
        "\n",
        "    def _evaluate_stored_objective(self,  only_generator_terms=True):\n",
        "        \"\"\"\n",
        "        Direct evaluation without mock objects.\n",
        "        Only includes terms from the current snapshot in the objective function.\n",
        "        \"\"\"\n",
        "        # Get variable name mapping for current network\n",
        "        id_to_name = self.var_id_to_name\n",
        "\n",
        "        # Get the current snapshot name\n",
        "        current_snapshot = self.network.snapshots[self.snapshot_idx]\n",
        "\n",
        "        # Get current variable values\n",
        "        variable_names = []\n",
        "        var_indices = []\n",
        "        vars_flat = self.objective_vars.values.flatten()\n",
        "        coeffs_flat = self.objective_coeffs.values.flatten()\n",
        "\n",
        "        # Filter variables to only include those from the current snapshot\n",
        "        for i, var_id in enumerate(vars_flat):\n",
        "            if var_id != -1 and var_id in id_to_name:\n",
        "                var_name = id_to_name[var_id]\n",
        "\n",
        "                # Filter for generator-related terms if requested\n",
        "                if only_generator_terms:\n",
        "                    # Check if this is a generator-related variable\n",
        "                    # Adjust these patterns based on your specific variable naming\n",
        "                    is_generator_var = ('generator' in var_name.lower())\n",
        "                    if not is_generator_var:\n",
        "                        continue  # Skip non-generator variables\n",
        "\n",
        "\n",
        "                # Check if this variable belongs to the current snapshot\n",
        "                if 'snapshot=' in var_name:\n",
        "                    # Extract the snapshot value from the variable name\n",
        "                    snapshot_part = var_name.split('snapshot=')[1].split(',')[0].split(']')[0]\n",
        "                    if snapshot_part == str(current_snapshot):\n",
        "                        variable_names.append(var_name)\n",
        "                        var_indices.append(i)\n",
        "                else:\n",
        "                    # Include variables without snapshot dimension (like investment variables)\n",
        "                    variable_names.append(var_name)\n",
        "                    var_indices.append(i)\n",
        "\n",
        "        var_values = self.create_variable_values_mapping(variable_names)\n",
        "\n",
        "        # Direct mathematical evaluation using only variables from current snapshot\n",
        "        if var_indices:\n",
        "            result = np.sum(coeffs_flat[var_indices] *\n",
        "                        [var_values.get(name, 0) for name in variable_names]) + \\\n",
        "                    self.objective_const\n",
        "        else:\n",
        "            # If no variables for this snapshot, just return the constant\n",
        "            result = self.objective_const\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _evaluate_constraint(self, constraint_key):\n",
        "        \"\"\"\n",
        "        Evaluate a single constraint using current network values.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        constraint_key : str\n",
        "            Key of the specific constraint to evaluate\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple: (bool, float)\n",
        "            Boolean indicating if constraint is satisfied, and violation amount (0 if satisfied)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if constraint_key not in self.constraints:\n",
        "                return True, 0\n",
        "\n",
        "            constraint = self.constraints[constraint_key]\n",
        "            lhs = constraint['lhs']\n",
        "            rhs = constraint['rhs']\n",
        "            sign = constraint['sign']\n",
        "        except Exception as e:\n",
        "            print(f\"Error accessing constraint {constraint_key}: {e}\")\n",
        "            return True, 0  # Default to no violation on error\n",
        "\n",
        "        # Get variable name mapping for current network\n",
        "        id_to_name = self.var_id_to_name\n",
        "\n",
        "        # Evaluate LHS if it's a linear expression\n",
        "        lhs_value = 0\n",
        "        if hasattr(lhs, 'vars') and hasattr(lhs, 'coeffs'):\n",
        "            # Get current variable values\n",
        "            variable_names = []\n",
        "            vars_flat = lhs.vars.flatten()  # Flatten to handle any dimensionality\n",
        "            coeffs_flat = lhs.coeffs.flatten()  # Flatten coefficients as well\n",
        "\n",
        "            # Create a mask for valid variable indices (not -1)\n",
        "            valid_indices = vars_flat != -1\n",
        "\n",
        "            # Filter out -1 indices\n",
        "            valid_vars = vars_flat[valid_indices]\n",
        "            valid_coeffs = coeffs_flat[valid_indices]\n",
        "\n",
        "            # Get variable names only for valid indices\n",
        "            for var_id in valid_vars:\n",
        "                if var_id in id_to_name:\n",
        "                    variable_names.append(id_to_name[var_id])\n",
        "\n",
        "            # Get variable values\n",
        "            var_values = self.create_variable_values_mapping(variable_names)\n",
        "\n",
        "            # Direct mathematical evaluation using only valid variables and coefficients\n",
        "            try:\n",
        "                # Convert variable values to numpy array\n",
        "                var_values_array = np.array([var_values.get(name, 0) for name in variable_names])\n",
        "\n",
        "                # Calculate LHS value using only valid variables and coefficients\n",
        "                lhs_value = np.sum(valid_coeffs * var_values_array)\n",
        "\n",
        "                # Add constant if it exists\n",
        "                if hasattr(lhs, 'const'):\n",
        "                    lhs_value += lhs.const.item() if hasattr(lhs.const, 'item') else lhs.const\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in constraint evaluation for {constraint_key}: {e}\")\n",
        "                return True, 0  # Skip this constraint on error\n",
        "        else:\n",
        "            # If it's a constant or simple value\n",
        "            lhs_value = lhs\n",
        "\n",
        "        # Get RHS value (will be a constant)\n",
        "        rhs_value = rhs\n",
        "\n",
        "        # Check constraint satisfaction based on sign\n",
        "        try:\n",
        "            # Handle both scalar and array values\n",
        "            if sign == '==':\n",
        "                if isinstance(lhs_value, (np.ndarray, list)) or isinstance(rhs_value, (np.ndarray, list)):\n",
        "                    # Convert to numpy arrays if needed\n",
        "                    lhs_array = np.asarray(lhs_value)\n",
        "                    rhs_array = np.asarray(rhs_value)\n",
        "\n",
        "                    # Handle shape mismatches\n",
        "                    if lhs_array.shape != rhs_array.shape:\n",
        "                        if np.isscalar(lhs_array) or len(lhs_array.shape) == 0:\n",
        "                            lhs_array = np.full_like(rhs_array, lhs_array)\n",
        "                        elif np.isscalar(rhs_array) or len(rhs_array.shape) == 0:\n",
        "                            rhs_array = np.full_like(lhs_array, rhs_array)\n",
        "                        else:\n",
        "                            # If shapes still don't match, return no violation\n",
        "                            print(f\"Shape mismatch in constraint {constraint_key}: {lhs_array.shape} vs {rhs_array.shape}\")\n",
        "                            return True, 0\n",
        "\n",
        "                    satisfied = np.all(np.isclose(lhs_array, rhs_array))\n",
        "                    violation = float(np.sum(np.abs(lhs_array - rhs_array)))\n",
        "                else:\n",
        "                    satisfied = np.isclose(lhs_value, rhs_value)\n",
        "                    violation = float(abs(lhs_value - rhs_value))\n",
        "            elif sign == '<=':\n",
        "                if isinstance(lhs_value, (np.ndarray, list)) or isinstance(rhs_value, (np.ndarray, list)):\n",
        "                    # Convert to numpy arrays if needed\n",
        "                    lhs_array = np.asarray(lhs_value)\n",
        "                    rhs_array = np.asarray(rhs_value)\n",
        "\n",
        "                    # Handle shape mismatches\n",
        "                    if lhs_array.shape != rhs_array.shape:\n",
        "                        if np.isscalar(lhs_array) or len(lhs_array.shape) == 0:\n",
        "                            lhs_array = np.full_like(rhs_array, lhs_array)\n",
        "                        elif np.isscalar(rhs_array) or len(rhs_array.shape) == 0:\n",
        "                            rhs_array = np.full_like(lhs_array, rhs_array)\n",
        "                        else:\n",
        "                            # If shapes still don't match, return no violation\n",
        "                            print(f\"Shape mismatch in constraint {constraint_key}: {lhs_array.shape} vs {rhs_array.shape}\")\n",
        "                            return True, 0\n",
        "\n",
        "                    satisfied = np.all(lhs_array <= rhs_array)\n",
        "                    violation = float(np.sum(np.maximum(0, lhs_array - rhs_array)))\n",
        "                else:\n",
        "                    satisfied = lhs_value <= rhs_value\n",
        "                    violation = float(max(0, lhs_value - rhs_value))\n",
        "            elif sign == '>=':\n",
        "                if isinstance(lhs_value, (np.ndarray, list)) or isinstance(rhs_value, (np.ndarray, list)):\n",
        "                    # Convert to numpy arrays if needed\n",
        "                    lhs_array = np.asarray(lhs_value)\n",
        "                    rhs_array = np.asarray(rhs_value)\n",
        "\n",
        "                    # Handle shape mismatches - don't think I need this\n",
        "                    if lhs_array.shape != rhs_array.shape:\n",
        "                        if np.isscalar(lhs_array) or len(lhs_array.shape) == 0:\n",
        "                            lhs_array = np.full_like(rhs_array, lhs_array)\n",
        "                        elif np.isscalar(rhs_array) or len(rhs_array.shape) == 0:\n",
        "                            rhs_array = np.full_like(lhs_array, rhs_array)\n",
        "                        else:\n",
        "                            # If shapes still don't match, return no violation\n",
        "                            print(f\"Shape mismatch in constraint {constraint_key}: {lhs_array.shape} vs {rhs_array.shape}\")\n",
        "                            return True, 0\n",
        "\n",
        "                    satisfied = np.all(lhs_array >= rhs_array)\n",
        "                    violation = float(np.sum(np.maximum(0, rhs_array - lhs_array)))\n",
        "                else:\n",
        "                    satisfied = lhs_value >= rhs_value\n",
        "                    violation = float(max(0, rhs_value - lhs_value))\n",
        "            else:\n",
        "                # Unknown sign\n",
        "                satisfied = True\n",
        "                violation = 0\n",
        "\n",
        "            # Ensure violation is a scalar\n",
        "            if hasattr(violation, '__len__'):\n",
        "                violation = float(np.sum(violation))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error comparing constraint values for {constraint_key}: {e}\")\n",
        "            print(f\"LHS type: {type(lhs_value)}, RHS type: {type(rhs_value)}\")\n",
        "            print(f\"LHS: {lhs_value}, RHS: {rhs_value}\")\n",
        "            satisfied = True\n",
        "            violation = 0\n",
        "\n",
        "        return satisfied, violation\n",
        "\n",
        "    def _evaluate_all_constraints(self):\n",
        "        \"\"\"\n",
        "        Evaluate constraints relevant to the current snapshot.\n",
        "        Only includes constraints that:\n",
        "        1. Are for the current snapshot only, or\n",
        "        2. Link the current snapshot with previous snapshots (but not future snapshots)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict: Information about constraint violations\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'all_satisfied': True,\n",
        "            'violations': {},\n",
        "            'total_violation': 0.0,\n",
        "            'violations_by_group': {}  # Track violations by constraint group\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Get the current snapshot name\n",
        "            current_snapshot = self.network.snapshots[self.snapshot_idx]\n",
        "\n",
        "            # Evaluate each individual constraint that's relevant to the current snapshot\n",
        "            for constraint_key in self.constraints:\n",
        "                try:\n",
        "                    # Check if this constraint is relevant to the current snapshot\n",
        "                    is_relevant = False\n",
        "\n",
        "                    # If constraint has no snapshot specification, include it\n",
        "                    if 'snapshot=' not in constraint_key:\n",
        "                        is_relevant = True\n",
        "                    else:\n",
        "                        # Extract all snapshots mentioned in this constraint\n",
        "                        constraint_snapshots = []\n",
        "                        parts = constraint_key.split('snapshot=')\n",
        "                        for i in range(1, len(parts)):\n",
        "                            snapshot_val = parts[i].split(',')[0].split(']')[0]\n",
        "                            constraint_snapshots.append(snapshot_val)\n",
        "\n",
        "                        # Include if current snapshot is mentioned\n",
        "                        if str(current_snapshot) in constraint_snapshots:\n",
        "                            # Check if any future snapshots are mentioned\n",
        "                            has_future_snapshots = False\n",
        "                            for snap in constraint_snapshots:\n",
        "                                try:\n",
        "                                    # Find the index of this snapshot in the network's snapshots\n",
        "                                    snap_idx = list(self.network.snapshots).index(snap)\n",
        "                                    if snap_idx > self.snapshot_idx:\n",
        "                                        has_future_snapshots = True\n",
        "                                        break\n",
        "                                except (ValueError, TypeError):\n",
        "                                    # If snapshot can't be found or compared, skip this check\n",
        "                                    pass\n",
        "\n",
        "                            # Include if no future snapshots are mentioned\n",
        "                            if not has_future_snapshots:\n",
        "                                is_relevant = True\n",
        "\n",
        "                    # Only evaluate if the constraint is relevant\n",
        "                    if is_relevant:\n",
        "                        satisfied, violation = self._evaluate_constraint(constraint_key)\n",
        "                        if not satisfied:\n",
        "                            results['all_satisfied'] = False\n",
        "                            results['violations'][constraint_key] = float(violation)  # Ensure it's a scalar\n",
        "                            results['total_violation'] += float(violation)\n",
        "\n",
        "                            # Also track violations by constraint group (original name without coordinates)\n",
        "                            if '[' in constraint_key:\n",
        "                                group_name = constraint_key.split('[')[0]\n",
        "                                if group_name not in results['violations_by_group']:\n",
        "                                    results['violations_by_group'][group_name] = 0.0\n",
        "                                results['violations_by_group'][group_name] += float(violation)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error evaluating constraint {constraint_key}: {e}\")\n",
        "                    # Continue with other constraints\n",
        "        except Exception as e:\n",
        "            print(f\"Error in constraint evaluation: {e}\")\n",
        "            # Return default results\n",
        "\n",
        "        return results\n",
        "\n",
        "    def calculate_constrained_reward(self):\n",
        "        \"\"\"\n",
        "        Calculate reward with constraint violation penalties.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        float: Reward value with constraint penalties\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get base reward from objective function\n",
        "            base_reward = self._calculate_reward()\n",
        "\n",
        "            # Evaluate constraints\n",
        "            constraint_results = self._evaluate_all_constraints()\n",
        "\n",
        "            # Apply penalty for constraint violations\n",
        "            # Using a high penalty factor to make violations very noticeable\n",
        "            # Increased from 100.0 to make violations more obvious\n",
        "\n",
        "            # Ensure total_violation is a scalar\n",
        "            total_violation = float(constraint_results['total_violation'])\n",
        "            penalty = self.penalty_factor * total_violation\n",
        "\n",
        "            # Final reward is base reward minus penalties\n",
        "            constrained_reward = base_reward - penalty\n",
        "\n",
        "            # Ensure reward is a scalar\n",
        "            if hasattr(constrained_reward, '__len__'):\n",
        "                constrained_reward = float(constrained_reward)\n",
        "\n",
        "            return constrained_reward, constraint_results\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating constrained reward: {e}\")\n",
        "            # Fall back to unconstrained reward\n",
        "            return self._calculate_reward()\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        \"\"\"Calculate reward using stored objective components.\"\"\"\n",
        "        # Create a minimal mock expression or use your evaluation directly\n",
        "        return -1 * self._evaluate_stored_objective()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Execute one time step within the environment.\n",
        "\n",
        "        Args:\n",
        "            action: Array of generator setpoints [gen1_setpoint, gen2_setpoint, ...]\n",
        "\n",
        "        Returns:\n",
        "            observation: Network state after action\n",
        "            reward: Reward for this action\n",
        "            terminated: Whether episode is finished due to task completion\n",
        "            truncated: Whether episode is finished due to time limit\n",
        "            info: Additional information\n",
        "        \"\"\"\n",
        "        scaled_action = self.scale_action(action)\n",
        "\n",
        "        # Update generator setpoints for non-slack generators only\n",
        "        for i, gen_name in enumerate(self.non_slack_names):\n",
        "            self.network.generators_t.p_set.iloc[self.snapshot_idx,\n",
        "                self.network.generators_t.p_set.columns.get_loc(gen_name)] = scaled_action[i]\n",
        "        # Run power flow to get new network state\n",
        "        try:\n",
        "            # You can choose linear or non-linear power flow\n",
        "            self.network.lpf()  # Linear power flow\n",
        "            # print(self.network.generators_t.p.loc['now'])  # Commented out to reduce output\n",
        "            # self.network.pf()  # Non-linear power flow (alternative)\n",
        "\n",
        "            power_flow_converged = True\n",
        "        except Exception as e:\n",
        "            print(f\"Power flow failed: {e}\")\n",
        "            power_flow_converged = False\n",
        "\n",
        "        # Calculate reward using constrained reward function\n",
        "        reward, constraint_results = self.calculate_constrained_reward()\n",
        "        #reward=self._calculate_reward()\n",
        "\n",
        "        # Increment step counters\n",
        "        self.current_step += 1\n",
        "        self.snapshot_idx += 1\n",
        "\n",
        "        # Handle cycling through snapshots\n",
        "        if self.snapshot_idx >= self.total_snapshots:\n",
        "            self.snapshot_idx = 0  # Reset to beginning\n",
        "            self.reset_network()\n",
        "            #self._test_hidden_references()\n",
        "            #can run that instead to check if there are hidden references preventing network cleanup\n",
        "\n",
        "\n",
        "        # Get new observation\n",
        "        observation = self._get_observation()\n",
        "\n",
        "        # Check if episode is done\n",
        "        episode_done = self._check_done()\n",
        "\n",
        "        # In gymnasium, we need to separate terminated vs truncated\n",
        "        terminated = False  # Task completion (not applicable here)\n",
        "        truncated = episode_done  # Time limit reached\n",
        "\n",
        "        # Additional info\n",
        "        info = {\n",
        "            'generator_setpoints': scaled_action,\n",
        "            'power_flow_converged': power_flow_converged,\n",
        "            'generator_names': self.non_slack_names,\n",
        "            'current_step': self.current_step,\n",
        "            'snapshot_idx': self.snapshot_idx,\n",
        "            'constraints_satisfied': constraint_results['all_satisfied'],\n",
        "            'constraint_violations': constraint_results['violations'],\n",
        "            'total_violation': constraint_results['total_violation']\n",
        "        }\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "\n",
        "    def _check_done(self):\n",
        "        \"\"\"\n",
        "        Check if episode should terminate.\n",
        "\n",
        "        Episode terminates when we've reached the specified episode length.\n",
        "        \"\"\"\n",
        "        if self.current_step >= self.episode_length:\n",
        "            return True\n",
        "\n",
        "        # TO DO: add other cases might want to terminate\n",
        "        # You might want to terminate on:\n",
        "        # - Power flow convergence failure\n",
        "        # - Voltage limit violations\n",
        "        # - Line overloads\n",
        "\n",
        "        return False\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        \"\"\"\n",
        "        Set the random seed for reproducible experiments.\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def get_constraint_info(self):\n",
        "        \"\"\"\n",
        "        Get detailed information about the constraints in the model.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict: Detailed information about constraints and their current status\n",
        "        \"\"\"\n",
        "        # Evaluate all constraints\n",
        "        all_results = self._evaluate_all_constraints()\n",
        "\n",
        "        # Organize constraint information by groups and individual constraints\n",
        "        constraint_info = {\n",
        "            'all_satisfied': all_results['all_satisfied'],\n",
        "            'total_violation': all_results['total_violation'],\n",
        "            'groups': {},\n",
        "            'individual': {}\n",
        "        }\n",
        "\n",
        "        # Group constraints by their base name\n",
        "        for constraint_key in self.constraints:\n",
        "            # Evaluate this specific constraint\n",
        "            satisfied, violation = self._evaluate_constraint(constraint_key)\n",
        "\n",
        "            # Store individual constraint info\n",
        "            constraint_info['individual'][constraint_key] = {\n",
        "                'satisfied': satisfied,\n",
        "                'violation': violation,\n",
        "                'sign': self.constraints[constraint_key]['sign']\n",
        "            }\n",
        "\n",
        "            # Also group by constraint type\n",
        "            if '[' in constraint_key:\n",
        "                group_name = constraint_key.split('[')[0]\n",
        "                if group_name not in constraint_info['groups']:\n",
        "                    constraint_info['groups'][group_name] = {\n",
        "                        'count': 0,\n",
        "                        'satisfied_count': 0,\n",
        "                        'violated_count': 0,\n",
        "                        'total_violation': 0.0,\n",
        "                        'max_violation': 0.0,\n",
        "                        'sign': self.constraints[constraint_key]['sign']\n",
        "                    }\n",
        "\n",
        "                group_info = constraint_info['groups'][group_name]\n",
        "                group_info['count'] += 1\n",
        "\n",
        "                if satisfied:\n",
        "                    group_info['satisfied_count'] += 1\n",
        "                else:\n",
        "                    group_info['violated_count'] += 1\n",
        "                    group_info['total_violation'] += violation\n",
        "                    group_info['max_violation'] = max(group_info['max_violation'], violation)\n",
        "            else:\n",
        "                # Handle standalone constraints\n",
        "                if 'standalone' not in constraint_info['groups']:\n",
        "                    constraint_info['groups']['standalone'] = {\n",
        "                        'count': 0,\n",
        "                        'satisfied_count': 0,\n",
        "                        'violated_count': 0,\n",
        "                        'total_violation': 0.0,\n",
        "                        'max_violation': 0.0\n",
        "                    }\n",
        "\n",
        "                group_info = constraint_info['groups']['standalone']\n",
        "                group_info['count'] += 1\n",
        "\n",
        "                if satisfied:\n",
        "                    group_info['satisfied_count'] += 1\n",
        "                else:\n",
        "                    group_info['violated_count'] += 1\n",
        "                    group_info['total_violation'] += violation\n",
        "                    group_info['max_violation'] = max(group_info['max_violation'], violation)\n",
        "\n",
        "        return constraint_info\n",
        "\n",
        "    def render(self, mode='human', info=None):\n",
        "        \"\"\"\n",
        "        Render the environment state.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        mode : str\n",
        "            Rendering mode (only 'human' supported)\n",
        "        info : dict, optional\n",
        "            Information dictionary from step() method containing constraint data\n",
        "        \"\"\"\n",
        "        print(\"=== Current Network State ===\")\n",
        "        print(f\"Episode step: {self.current_step}/{self.episode_length}\")\n",
        "        print(f\"Snapshot index: {self.snapshot_idx}/{self.total_snapshots}\")\n",
        "        print(f\"Current snapshot: {self.network.snapshots[self.snapshot_idx]}\")\n",
        "        print(f\"Generator setpoints: {self.network.generators_t.p_set.iloc[self.snapshot_idx].values}\")\n",
        "        print(f\"Load values: {self.network.loads_t.p_set.iloc[self.snapshot_idx].values}\")\n",
        "\n",
        "        all_satisfied = info['constraints_satisfied']\n",
        "        total_violation = info['total_violation']\n",
        "        violations = info['constraint_violations']\n",
        "\n",
        "\n",
        "        print(f\"All constraints satisfied: {all_satisfied}\")\n",
        "        print(f\"Total constraint violation: {total_violation:.4f}\")\n",
        "\n",
        "        # Show violated constraints if any\n",
        "        if not all_satisfied and violations:\n",
        "            print(\"\\n=== Constraint Violations ===\")\n",
        "            for constraint_name, violation in violations.items():\n",
        "                print(f\"  {constraint_name}: {violation:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "WgkA8eiuJVW4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "WgkA8eiuJVW4",
        "outputId": "6aa85e4e-8e4e-4516-9a96-d444d0c746f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pypsa.network.io:Importing network from PyPSA version v0.0.0 while current version is v0.35.1. Read the release notes at https://pypsa.readthedocs.io/en/latest/release_notes.html to prepare your network for import.\n",
            "WARNING:pypsa.consistency:The following lines have carriers which are not defined:\n",
            "Index(['0', '1', '2', '3', '4', '5', '6'], dtype='object', name='Line')\n",
            "WARNING:pypsa.consistency:The following buses have carriers which are not defined:\n",
            "Index(['ZA0 0', 'ZA0 1', 'ZA0 2', 'ZA0 3', 'ZA0 4', 'ZA1 0', 'ZA2 0', 'ZA3 0'], dtype='object', name='Bus')\n",
            "Writing constraints.: 100%|\u001b[38;2;128;191;255m██████████\u001b[0m| 13/13 [00:01<00:00, 10.30it/s]\n",
            "Writing continuous variables.: 100%|\u001b[38;2;128;191;255m██████████\u001b[0m| 6/6 [00:00<00:00, 20.72it/s]\n",
            "WARNING:linopy.constants:Optimization potentially failed: \n",
            "Status: warning\n",
            "Termination condition: infeasible\n",
            "Solution: 0 primals, 0 duals\n",
            "Objective: nan\n",
            "Solver model: available\n",
            "Solver message: Infeasible\n",
            "\n",
            "WARNING:pypsa.consistency:The following sub_networks have carriers which are not defined:\n",
            "Index(['0'], dtype='object', name='SubNetwork')\n",
            "WARNING:pypsa.consistency:The following lines have carriers which are not defined:\n",
            "Index(['0', '1', '2', '3', '4', '5', '6'], dtype='object', name='Line')\n",
            "WARNING:pypsa.consistency:The following buses have carriers which are not defined:\n",
            "Index(['ZA0 0', 'ZA0 1', 'ZA0 2', 'ZA0 3', 'ZA0 4', 'ZA1 0', 'ZA2 0', 'ZA3 0'], dtype='object', name='Bus')\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1561884744.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEnv2Gen1LoadConstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"elec_s_5_ec_lc1.0_3h.nc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loads_t\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-416271505.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, network_file, episode_length, constraint_penalty_factor)\u001b[0m\n\u001b[1;32m     29\u001b[0m                                    \"StorageUnit-fix-state_of_charge-upper\"]#this is if you want RL agent only to control dispatch of generators\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_optimization_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint_penalty_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-416271505.py\u001b[0m in \u001b[0;36m_initialize_optimization_components\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m                                             specific_lhs = type('LinearExpr', (), {\n\u001b[1;32m    231\u001b[0m                                                 \u001b[0;34m'vars'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlhs_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_vars\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlhs_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                                                 \u001b[0;34m'coeffs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlhs_coeffs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_coeffs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlhs_coeffs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m                                                 \u001b[0;34m'copy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                                             })\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36mvalues\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "env=Env2Gen1LoadConstr(network_file=\"elec_s_5_ec_lc1.0_3h.nc\")\n",
        "print(hasattr(env.network, \"loads_t\") and not env.network.loads_t.p.empty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f9c06b56",
      "metadata": {
        "id": "f9c06b56"
      },
      "outputs": [],
      "source": [
        "class BackboneNetwork(nn.Module):\n",
        "    def __init__(self, input_features, hidden_dimensions, out_features, dropout):\n",
        "        super(BackboneNetwork, self).__init__()\n",
        "\n",
        "        # SIMPLIFIED: Single hidden layer network for debugging\n",
        "        self.neuralnet = nn.Sequential(\n",
        "            nn.Linear(input_features, hidden_dimensions),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dimensions, hidden_dimensions),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dimensions, out_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.neuralnet(x)\n",
        "        return output\n",
        "\n",
        "#Define the actor-critic network\n",
        "class actorCritic(nn.Module):\n",
        "    def __init__(self, actor, critic):\n",
        "        super().__init__()\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "    def forward(self, state):\n",
        "        action_pred = self.actor(state)\n",
        "        value_pred = self.critic(state)\n",
        "        return action_pred, value_pred\n",
        "        #Returns both the action predictions and the value predictions.\n",
        "\n",
        "#We'll use the networks defined above to create an actor and a critic. Then, we will create an agent, including the actor and the critic.\n",
        "#finish this step later\n",
        "# def create_agent(hidden_dimensions, dropout):\n",
        "#     INPUT_FEATURES =env_train.\n",
        "class PPO_agent:\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 device,\n",
        "                 run,\n",
        "                 hidden_dimensions,\n",
        "                 dropout, discount_factor,\n",
        "                 max_episodes,\n",
        "                 print_interval,\n",
        "                 PPO_steps,\n",
        "                 n_trials,\n",
        "                 epsilon,\n",
        "                 entropy_coefficient,\n",
        "                 learning_rate,\n",
        "                 batch_size,\n",
        "                 optimizer_name,\n",
        "                 seed):\n",
        "\n",
        "        self.seed = seed\n",
        "        if seed is not None:\n",
        "            # Set PyTorch seed for this class\n",
        "            torch.manual_seed(seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed(seed)\n",
        "\n",
        "        self.env = env  # Store the environment as an attribute\n",
        "\n",
        "        self.device = device\n",
        "        self.run = run\n",
        "\n",
        "        # Get observation and action space dimensions for gymnasium environment\n",
        "        obs, _ = self.env.reset()\n",
        "\n",
        "        self.INPUT_FEATURES = obs.shape[0]  # Flattened observation size\n",
        "        self.ACTOR_OUTPUT_FEATURES = self.env.action_space.shape[0]* 2  # 2 parameters (alpha, beta) per action dimension\n",
        "\n",
        "        self.HIDDEN_DIMENSIONS = hidden_dimensions\n",
        "\n",
        "        self.CRITIC_OUTPUT_FEATURES = 1\n",
        "        self.DROPOUT = dropout\n",
        "\n",
        "        self.discount_factor = discount_factor\n",
        "        self.max_episodes = max_episodes\n",
        "        self.print_interval = print_interval\n",
        "        self.PPO_steps=PPO_steps\n",
        "        self.n_trials=n_trials\n",
        "        self.epsilon=epsilon\n",
        "        self.entropy_coefficient=entropy_coefficient\n",
        "        self.learning_rate=learning_rate\n",
        "\n",
        "        self.batch_size=batch_size\n",
        "\n",
        "        # Initialize actor network\n",
        "        self.actor = BackboneNetwork(\n",
        "            self.INPUT_FEATURES, self.HIDDEN_DIMENSIONS, self.ACTOR_OUTPUT_FEATURES, self.DROPOUT\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Initialize the final layer bias for Beta distribution\n",
        "        for name, param in self.actor.named_parameters():\n",
        "            if 'neuralnet.4.bias' in name:  # Adjust index based on your network structure\n",
        "                # Initialize to produce alpha=beta=2 (uniform-like distribution centered at 0.5)\n",
        "                param.data.fill_(0.0)  # softplus(0) + 1 = 2\n",
        "                print(f\"Initialized Beta parameters to produce uniform-like distribution\")\n",
        "\n",
        "        # Initialize critic network\n",
        "        self.critic = BackboneNetwork(\n",
        "            self.INPUT_FEATURES, self.HIDDEN_DIMENSIONS, self.CRITIC_OUTPUT_FEATURES, self.DROPOUT\n",
        "        ).to(self.device)\n",
        "\n",
        "        #Better move the .to(self.device) call separately for both self.actor and self.critic. This ensures the individual parts of the model are moved to the correct device before combined into the actorCritic class\n",
        "        # Combine into a single actor-critic model\n",
        "        self.model = actorCritic(self.actor, self.critic)\n",
        "\n",
        "        try:\n",
        "            # Try to get the optimizer from torch.optim based on the provided name\n",
        "            self.optimizer = getattr(torch.optim, optimizer_name)(self.model.parameters(), lr=self.learning_rate)\n",
        "        except AttributeError:\n",
        "            # Raise an error if the optimizer_name is not valid\n",
        "            raise ValueError(f\"Optimizer '{optimizer_name}' is not available in torch.optim.\")\n",
        "\n",
        "    def calculate_returns(self, rewards):\n",
        "        returns = []\n",
        "        cumulative_reward = 0\n",
        "        for r in reversed(rewards):\n",
        "            cumulative_reward = r +cumulative_reward*self.discount_factor\n",
        "            returns.insert(0, cumulative_reward)\n",
        "        returns = torch.tensor(returns).to(self.device)\n",
        "\n",
        "        # Only normalize if we have more than one element to avoid std() warning\n",
        "        if returns.numel() > 1:\n",
        "            epsilon = 1e-8  # Small constant to avoid division by zero\n",
        "            returns_std = returns.std()\n",
        "            if not torch.isnan(returns_std) and returns_std >= epsilon:\n",
        "                returns = (returns - returns.mean()) / (returns_std + epsilon)\n",
        "\n",
        "        #I had conceptual trouble with normalizing the reward by an average, because it seemed to me since we're adding more rewards for earlier timesteps, the cumulative reward for earlier times would be a lot larger. But need to consider dicount facotr.\n",
        "        # Future rewards contribute significantly to the cumulative return, so earlier timesteps will likely have larger returns.\n",
        "        #if gamma is close to 0, future rewards have little influence, and the return at each timestep will closely resemble the immediate reward, meaning the pattern might not be as clear.\n",
        "        return returns\n",
        "\n",
        "    #The advantage is calculated as the difference between the value predicted by the critic and the expected return from the actions chosen by the actor according to the policy.\n",
        "    def calculate_advantages(self, returns, values):\n",
        "        advantages = returns - values\n",
        "\n",
        "        # Only normalize if we have more than one element to avoid std() warning\n",
        "        if advantages.numel() > 1:\n",
        "            epsilon = 1e-8\n",
        "            advantages_std = advantages.std()\n",
        "            if not torch.isnan(advantages_std) and advantages_std >= epsilon:\n",
        "                advantages = (advantages - advantages.mean()) / (advantages_std + epsilon)\n",
        "\n",
        "        return advantages\n",
        "\n",
        "    #The standard policy gradient loss is calculated as the product of the policy action probabilities and the advantage function\n",
        "    #The standard policy gradietn loss cannot make corrections for abrupt policy changes. The surrogate loss modifies the standard loss to restrict the amount the policy can change in each iteration.\n",
        "    #The surrogate loss is the minimum of (policy ratio X advantage function) and (clipped value of policy ratio X advantage function) where the policy ratio is between the action probabilities according to the old versus new policies and clipping restricts the value to a region near 1.\n",
        "\n",
        "    def calculate_surrogate_loss(self, actions_log_probability_old, actions_log_probability_new, advantages):\n",
        "        advantages = advantages.detach()\n",
        "        # creates a new tensor that shares the same underlying data as the original tensor but breaks the computation graph. This means:\n",
        "        # The new tensor is treated as a constant with no gradients.\n",
        "        # Any operations involving this tensor do not affect the gradients of earlier computations in the graph.\n",
        "\n",
        "        #If the advantages are not detached, the backpropagation of the loss computed using the surrogate_loss would affect both the actor and the critic networks\n",
        "        # The surrogate loss is meant to update only the policy (actor).\n",
        "        # Allowing gradients to flow back through the advantages would inadvertently update the critic, potentially disrupting its learning process.\n",
        "\n",
        "        policy_ratio  = (actions_log_probability_new - actions_log_probability_old).exp()\n",
        "        surrogate_loss_1 = policy_ratio*advantages\n",
        "        surrogate_loss_2 = torch.clamp(policy_ratio, min =1.0-self.epsilon, max = 1.0+self.epsilon)*advantages\n",
        "        surrogate_loss=torch.min(surrogate_loss_1, surrogate_loss_2)\n",
        "        return surrogate_loss\n",
        "\n",
        "    #TRAINING THE AGENT\n",
        "    #Policy loss is the sum of the surrogate loss and the entropy bonus. It is used to update the actor (policy network)\n",
        "    #Value loss is based on the difference between the value predicted by the critic and the returns (cumulative reward) generated by the policy. This loss is used to update the critic (value network) to make predictions more accurate.\n",
        "\n",
        "    def calculate_losses(self, surrogate_loss, entropy, returns, value_pred):\n",
        "        entropy_bonus = self.entropy_coefficient*entropy\n",
        "        policy_loss = -(surrogate_loss+entropy_bonus).sum()\n",
        "        value_loss = torch.nn.functional.smooth_l1_loss(returns, value_pred).sum() #helps to smoothen the loss function and makes it less sensitive to outliers.\n",
        "        return policy_loss, value_loss\n",
        "\n",
        "    def init_training(self):\n",
        "        #create a set of buffers as empty arrays. To be used during training to store information\n",
        "        states = []\n",
        "        actions = []\n",
        "        actions_log_probability = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        return states, actions, actions_log_probability, values, rewards, done, episode_reward\n",
        "\n",
        "    def forward_pass(self):#this is just the training function (might just want to rename it)\n",
        "        # # === DETAILED OBJECT ANALYSIS ===\n",
        "        # import psutil\n",
        "        # import gc\n",
        "\n",
        "        # if not hasattr(self, '_episode_counter'):\n",
        "        #     self._episode_counter = 0\n",
        "        # self._episode_counter += 1\n",
        "\n",
        "        # mem_mb = psutil.Process().memory_info().rss / 1024 / 1024\n",
        "\n",
        "        # # Get ALL objects with \"Network\" in their type name\n",
        "        # network_objects = [obj for obj in gc.get_objects() if 'network' in str(type(obj)).lower()]\n",
        "\n",
        "        # print(f\"\\n=== EPISODE {self._episode_counter} OBJECT ANALYSIS ===\")\n",
        "        # print(f\"Memory: {mem_mb:.1f}MB\")\n",
        "        # print(f\"Total objects with 'network' in type: {len(network_objects)}\")\n",
        "\n",
        "        # # Count by exact type\n",
        "        # type_counts = {}\n",
        "        # for obj in network_objects:\n",
        "        #     obj_type = str(type(obj))\n",
        "        #     type_counts[obj_type] = type_counts.get(obj_type, 0) + 1\n",
        "\n",
        "        # # Print breakdown\n",
        "        # for obj_type, count in type_counts.items():\n",
        "        #     print(f\"  {obj_type}: {count}\")\n",
        "\n",
        "        # # Show actual PyPSA Network objects specifically\n",
        "        # actual_networks = [obj for obj in gc.get_objects() if type(obj).__name__ == 'Network' and 'pypsa' in str(type(obj))]\n",
        "        # print(f\"Actual PyPSA Network objects: {len(actual_networks)}\")\n",
        "\n",
        "        # if len(actual_networks) <= 5:  # Only print if reasonable number\n",
        "        #     for i, net in enumerate(actual_networks):\n",
        "        #         print(f\"  Network {i+1}: {id(net)} - {type(net)}\")\n",
        "\n",
        "        # network_id = id(self.env.network) if hasattr(self.env, 'network') else None\n",
        "        # print(f\"Current env.network ID: {network_id}\")\n",
        "        # print(\"=\" * 50)\n",
        "        # # === END ANALYSIS ===\n",
        "\n",
        "        # Reset environment with seed\n",
        "        if self.seed is not None:\n",
        "            state, _ = self.env.reset(seed=self.seed)\n",
        "        else:\n",
        "            state, _ = self.env.reset()\n",
        "\n",
        "        states, actions, actions_log_probability, values, rewards, done, episode_reward = self.init_training()\n",
        "\n",
        "        # Add this line to track violations\n",
        "        total_violations = 0\n",
        "\n",
        "        # # Create fresh network for each episode to avoid memory corruption\n",
        "        # fresh_network = create_pypsa_network()\n",
        "        # self.env.network = fresh_network\n",
        "\n",
        "        state, _ = self.env.reset()  # Gymnasium format returns (obs, info)\n",
        "\n",
        "        self.model.train() # Set model to training mode\n",
        "\n",
        "        while True:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            states.append(state_tensor)\n",
        "\n",
        "            # Get action predictions and values\n",
        "            action_mean, value_pred = self.model(state_tensor)\n",
        "\n",
        "\n",
        "\n",
        "            # Split actor output into alpha and beta parameters\n",
        "            action_dim = self.env.action_space.shape[0]\n",
        "            alpha_raw, beta_raw = torch.split(action_mean, action_dim, dim=-1)\n",
        "\n",
        "            # Ensure alpha, beta > 1 for well-behaved Beta distribution\n",
        "            alpha = torch.nn.functional.softplus(alpha_raw) + 1.0\n",
        "            beta = torch.nn.functional.softplus(beta_raw) + 1.0\n",
        "\n",
        "            # Create Beta distribution for continuous actions in [0,1]\n",
        "            dist = torch.distributions.Beta(alpha, beta)\n",
        "            action = dist.sample()\n",
        "\n",
        "            # No clamping needed - Beta distribution naturally outputs [0,1]\n",
        "            action_clamped = action\n",
        "\n",
        "            log_prob_action = dist.log_prob(action).sum(dim=-1)  # Sum over action dimensions\n",
        "\n",
        "            # Step environment with numpy action\n",
        "            action_np = action_clamped.detach().cpu().numpy().flatten()\n",
        "            state, reward, terminated, truncated, info = self.env.step(action_np)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            #accumulate violations for the epsiode\n",
        "            total_violations += sum(info['constraint_violations'].values())\n",
        "\n",
        "            actions.append(action_clamped)\n",
        "            actions_log_probability.append(log_prob_action)\n",
        "            values.append(value_pred)\n",
        "            rewards.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        states=torch.cat(states).to(self.device)#converts the list of individual states into a sinlem tensor that is necessary for later processing\n",
        "        #Creates a single tensor with dimensions like (N, state_dim), where: N is the number of states collected in the episode; state_dim is the dimensionality of each state.\n",
        "        #torch.cat() expects a sequence (e.g. list or tuple) of PyTorch tensors as input.\n",
        "        actions=torch.cat(actions).to(self.device)\n",
        "        #Note that, in the loop, both state and action are PyTorch tensors so that states and actions are both lists of PyTorch tensors\n",
        "        actions_log_probability=torch.cat(actions_log_probability).to(self.device)\n",
        "        values=torch.cat(values).squeeze(-1).to(self.device)# .squeeze removes a dimension of size 1 only from tensor at the specified position, in this case, -1, the last dimesion in the tensor. Note that .squeeze() does not do anything if the size of the dimension at the specified potision is not 1.\n",
        "        # print(f\"rewards NaNs: {torch.isnan(torch.tensor(rewards, dtype=torch.float32)).any()}\")\n",
        "        # print(f\"values NaNs: {torch.isnan(torch.tensor(values, dtype=torch.float32)).any()}\")\n",
        "        returns = self.calculate_returns(rewards)\n",
        "        advantages = self.calculate_advantages(returns, values)\n",
        "\n",
        "        # print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
        "        # print(f\"advantages NaNs (after calculation): {torch.isnan(advantages).any()}\")\n",
        "\n",
        "        return episode_reward, states, actions, actions_log_probability, advantages, returns, total_violations\n",
        "\n",
        "\n",
        "    def update_policy(self,\n",
        "            states,\n",
        "            actions,\n",
        "            actions_log_probability_old,\n",
        "            advantages,\n",
        "            returns):\n",
        "        #print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
        "        total_policy_loss = 0\n",
        "        total_value_loss = 0\n",
        "        actions_log_probability_old = actions_log_probability_old.detach()\n",
        "        actions=actions.detach()\n",
        "\n",
        "        # print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
        "        # print(f\"advantages NaNs (after calculation): {torch.isnan(advantages).any()}\")\n",
        "\n",
        "\n",
        "        #detach() is used to remove the tensor from the computation graph, meaning no gradients will be calculated for that tensor when performing backpropagation.\n",
        "        #In this context, it's used to ensure that the old actions and log probabilities do not participate in the gradient computation during the optimization of the policy, as we want to update the model based on the current policy rather than the old one.\n",
        "        #print(type(states), type(actions),type(actions_log_probability_old), type(advantages), type(returns))\n",
        "        training_results_dataset= TensorDataset(\n",
        "                states,\n",
        "                actions,\n",
        "                actions_log_probability_old,\n",
        "                advantages,\n",
        "                returns) #TensorDataset class expects all the arguments passed to it to be tensors (or other compatible types like NumPy arrays, which will be automatically converted to tensor\n",
        "        batch_dataset = DataLoader(\n",
        "                training_results_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False)\n",
        "        #creates a DataLoader instance in PyTorch, which is used to load the training_results_dataset in batches during training.\n",
        "        #batch_size defines how many samples will be included in each batch. The dataset will be divided into batches of size BATCH_SIZE. The model will then process one batch at a time, rather than all of the data at once,\n",
        "        #shuffle argument controls whether or not the data will be shuffled before being split into batches.\n",
        "        #Because shuffle is false, dataloader will provide the batches in the order the data appears in training_results_dataset. In this case, the batches will be formed from consecutive entries in the dataset, and the observations will appear in the same sequence as they are stored in the dataset.\n",
        "        for _ in range(self.PPO_steps):\n",
        "            for batch_idx, (states,actions,actions_log_probability_old, advantages, returns) in enumerate(batch_dataset):\n",
        "                #get new log prob of actions for all input states\n",
        "                action_mean, value_pred = self.model(states)\n",
        "                value_pred = value_pred.squeeze(-1)\n",
        "\n",
        "                # For continuous actions with Beta distribution\n",
        "                action_dim = self.env.action_space.shape[0]\n",
        "                alpha_raw, beta_raw = torch.split(action_mean, action_dim, dim=-1)\n",
        "\n",
        "                # Ensure alpha, beta > 1 for well-behaved Beta distribution\n",
        "                alpha = torch.nn.functional.softplus(alpha_raw) + 1.0\n",
        "                beta = torch.nn.functional.softplus(beta_raw) + 1.0\n",
        "\n",
        "                probability_distribution_new = torch.distributions.Beta(alpha, beta)\n",
        "                entropy = probability_distribution_new.entropy().sum(dim=-1)\n",
        "\n",
        "                #estimate new log probabilities using old actions\n",
        "                actions_log_probability_new = probability_distribution_new.log_prob(actions).sum(dim=-1)\n",
        "                # # Check for NaN or Inf in log probabilities\n",
        "                # if torch.isnan(actions_log_probability_old).any() or torch.isinf(actions_log_probability_old).any():\n",
        "                #     print(\"NaN or Inf detected in actions_log_probability_old!\")\n",
        "                #     return  # You can return or handle this case as needed\n",
        "\n",
        "                # if torch.isnan(actions_log_probability_new).any() or torch.isinf(actions_log_probability_new).any():\n",
        "                #     print(\"NaN or Inf detected in actions_log_probability_new!\")\n",
        "                #     return  # You can return or handle this case as needed\n",
        "\n",
        "                # print(f\"actions_log_probability_old NaNs: {torch.isnan(actions_log_probability_old).any()}\")\n",
        "                # print(f\"actions_log_probability_new NaNs: {torch.isnan(actions_log_probability_new).any()}\")\n",
        "                # print(f\"advantages NaNs: {torch.isnan(advantages).any()}\")\n",
        "\n",
        "                surrogate_loss = self.calculate_surrogate_loss(\n",
        "                    actions_log_probability_old,\n",
        "                    actions_log_probability_new,\n",
        "                    advantages\n",
        "                )\n",
        "\n",
        "                # print(f\"Surrogate Loss NaNs: {torch.isnan(surrogate_loss).any()}\")\n",
        "                # print(f\"Entropy NaNs: {torch.isnan(entropy).any()}\")\n",
        "                # print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
        "                # print(f\"Value Predictions NaNs: {torch.isnan(value_pred).any()}\")\n",
        "\n",
        "                policy_loss, value_loss = self.calculate_losses(\n",
        "                    surrogate_loss,\n",
        "                    entropy,\n",
        "                    returns,\n",
        "                    value_pred\n",
        "                )\n",
        "                self.optimizer.zero_grad() #clear existing gradietns in the optimizer (so that these don't propagate accross multiple .backward(). Ensures each optimization step uses only the gradients computed during the current batch.\n",
        "\n",
        "                # Skip backward pass if loss is NaN\n",
        "                if torch.isnan(policy_loss).any():\n",
        "                    print(\"NaN detected in policy_loss - skipping backward pass!\")\n",
        "                    continue\n",
        "                if torch.isnan(value_loss).any():\n",
        "                    print(\"NaN detected in value_loss - skipping backward pass!\")\n",
        "                    continue\n",
        "\n",
        "                policy_loss.backward() #computes gradients for policy_loss with respect to the agent's parameters\n",
        "                # #Check for NaN gradients after policy_loss backward\n",
        "                # for param in self.model.parameters():\n",
        "                #     if param.grad is not None:  # Check if gradients exist for this parameter\n",
        "                #         if torch.isnan(param.grad).any():\n",
        "                #             print(\"NaN gradient detected in policy_loss!\")\n",
        "                # #             return\n",
        "                value_loss.backward()\n",
        "                # Check for NaN gradients after value_loss backwardor param in self.model.parameters():\n",
        "                # for param in self.model.parameters():\n",
        "                #     if param.grad is not None:  # Check if gradients exist for this parameter\n",
        "                #         if torch.isnan(param.grad).any():\n",
        "                #             print(\"NaN gradient detected in value_loss!\")\n",
        "                #             return\n",
        "\n",
        "                self.optimizer.step()\n",
        "                #The update step is based on the learning rate and other hyperparameters of the optimizer\n",
        "                # The parameters of the agent are adjusted to reduce the policy and value losses.\n",
        "                total_policy_loss += policy_loss.item() #accumulate the scalar value of the policy loss for logging/ analysis\n",
        "                #policy_loss.item() extracts the numerical value of the loss tensor (detaching it from the computational graph).\n",
        "                #This value is added to total_policy_loss to compute the cumulative loss over all batches in the current PPO step.\n",
        "                #Result: tracks the total policy loss for the current training epoch\n",
        "                # The loss over the whole dataset is the sum of the losses over all batches.\n",
        "                #The training dataset is split into batches during the training process. Each batch represents a subset of the collected training data from one episode.\n",
        "                # Loss calculation is performed for each batch (policy loss and value loss)\n",
        "                # for each batch, gradients are calculated with respect to the total loss for that batch and the optimizer then updates the network parameters using these gradients.\n",
        "                # this is because the surrogate loss is only calculated over a single batch of data\n",
        "                #look at the formula for surrogate loss.\n",
        "                # It is written in terms of an expectation ˆ Et[. . .] that indicates the empirical average over a finite batch of samples.\n",
        "                # This means you have collected a set of data (time steps) from the environment, and you're averaging over these data points. The hat symbol implies you're approximating the true expectation with a finite sample of data from the environment. This empirical average can be computed as the mean of values from the sampled transitions\n",
        "                # the expectation is taken over all the data you've collected\n",
        "                #If you're training with multiple batches (i.e., collecting data in chunks), then you can think of the expectation as being computed over each batch.\n",
        "                #The overall expectation can indeed be seen as the sum of expectations computed for each batch, but The expectation of the sum is generally not exactly equal to the sum of the expectations unless the samples are independent, but in practical reinforcement learning algorithms, it's typically a good enough approximation\n",
        "                #For samples to be independent, the outcome of one sample must not provide any information about the outcome of another. Specifically, in the context of reinforcement learning, this means that the states, actions, rewards, and subsequent states observed in different time steps or different episodes should be independent of each other.\n",
        "                total_value_loss += value_loss.item()\n",
        "                #Notice that we are calculating an empirical average, which is already an approximation on the true value (the true expectation would be the average over an infinite amount of data, and the empirical average is the average over the finite amount of data that we have collected).\n",
        "                #But furthermore, we are approximating even the empirical average istelf. The empirical average is the average over all our collected datal, but here we actually batch our data, calculate average over each batch and then sum these averages, which is not exaclty equal to the average of the sums (but is a decent approximation).\n",
        "        return total_policy_loss / self.PPO_steps, total_value_loss / self.PPO_steps\n",
        "\n",
        "    def train(self):\n",
        "        train_rewards = []\n",
        "        # test_rewards = []\n",
        "        # policy_losses = []\n",
        "        # value_losses = []\n",
        "        #lens = []\n",
        "\n",
        "        for episode in range(1, self.max_episodes + 1):\n",
        "            # Perform a forward pass and collect experience\n",
        "            train_reward, states, actions, actions_log_probability, advantages, returns, violations = self.forward_pass()\n",
        "\n",
        "            # Update the policy using the experience collected\n",
        "            policy_loss, value_loss = self.update_policy(\n",
        "                states,\n",
        "                actions,\n",
        "                actions_log_probability,\n",
        "                advantages,\n",
        "                returns)\n",
        "            # test_reward = self.evaluate()\n",
        "\n",
        "            # # Visualize the environment if it supports rendering (currently this is done once each episode - might want to change to once every multiple of episodes)\n",
        "            # if hasattr(self.env, \"render\") and callable(getattr(self.env, \"render\", None)):\n",
        "            #   self.env.render()\n",
        "\n",
        "            # Log the results\n",
        "            # policy_losses.append(policy_loss)\n",
        "            # value_losses.append(value_loss)\n",
        "            train_rewards.append(train_reward)\n",
        "            # # run these when back online\n",
        "            # self.run[\"policy_loss\"].log(policy_loss)\n",
        "            # self.run[\"value_loss\"].log(value_loss)\n",
        "            self.run[\"train_reward\"].log(train_reward)\n",
        "            self.run[\"total_violation\"].log(violations)\n",
        "\n",
        "            # Calculate the mean of recent rewards and losses for display\n",
        "            mean_train_rewards = np.mean(train_rewards[-self.n_trials:])\n",
        "            #mean_test_rewards = np.mean(test_rewards[-self.n_trials:])\n",
        "            # mean_abs_policy_loss = np.mean(np.abs(policy_losses[-self.n_trials:]))\n",
        "            # mean_abs_value_loss = np.mean(np.abs(value_losses[-self.n_trials:]))\n",
        "\n",
        "            # Print results at specified intervals\n",
        "            if episode % self.print_interval == 0:\n",
        "                print(f'Episode: {episode:3} | \\\n",
        "                    Train Rewards: {train_reward:3.1f} \\\n",
        "                    Violations: {violations}\\\n",
        "                    Mean Train Rewards: {mean_train_rewards:3.1f}' )\n",
        "                    # \\\n",
        "                    # | Mean Abs Policy Loss: {mean_abs_policy_loss:2.2f} \\\n",
        "                    # | Mean Abs Value Loss: {mean_abs_value_loss:2.2f} ')\n",
        "\n",
        "\n",
        "\n",
        "                                    # | Mean Test Rewards: {mean_test_rewards:3.1f} \\\n",
        "                                    #| \"Episode Len: {np.mean(lens[-self.n_trials:])}\n",
        "\n",
        "\n",
        "\n",
        "            # # Check if reward threshold is reached\n",
        "            # if mean_test_rewards >= self.reward_threshold:\n",
        "            #     print(f'Reached reward threshold in {episode} episodes')\n",
        "            #     break\n",
        "        # Check if the environment has a close method before calling it\n",
        "        # if hasattr(self.env, \"close\") and callable(getattr(self.env, \"close\", None)):\n",
        "        #   self.env.close() #Close environment visualisation after training is done.\n",
        "        return train_rewards\n",
        "\n",
        "def plot_train_rewards(train_rewards, reward_threshold):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(train_rewards, label='Training Reward')\n",
        "    plt.xlabel('Episode', fontsize=20)\n",
        "    plt.ylabel('Training Reward', fontsize=20)\n",
        "    plt.hlines(reward_threshold, 0, len(train_rewards), color='y')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_test_rewards(test_rewards, reward_threshold):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(test_rewards, label='Testing Reward')\n",
        "    plt.xlabel('Episode', fontsize=20)\n",
        "    plt.ylabel('Testing Reward', fontsize=20)\n",
        "    plt.hlines(reward_threshold, 0, len(test_rewards), color='y')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "def plot_losses(policy_losses, value_losses):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.plot(value_losses, label='Value Losses')\n",
        "    plt.plot(policy_losses, label='Policy Losses')\n",
        "    plt.xlabel('Episode', fontsize=20)\n",
        "    plt.ylabel('Loss', fontsize=20)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf82e05",
      "metadata": {
        "id": "acf82e05"
      },
      "outputs": [],
      "source": [
        "class Env2Gen1LoadConstrReplacement(Env2Gen1LoadConstr):\n",
        "    \"\"\"\n",
        "    Environment using the Replacement reward method instead of Summation.\n",
        "\n",
        "    Inherits from Env2Gen1LoadConstr but modifies the reward calculation\n",
        "    to implement the replacement method from the RL-OPF paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,network_file, episode_length=None, constraint_penalty_factor=100, offset_k=2500, input_dir=\"var_constraint_map\"):\n",
        "        super().__init__(network_file, episode_length=episode_length, constraint_penalty_factor=constraint_penalty_factor, input_dir=input_dir)\n",
        "        self.offset_k = offset_k\n",
        "\n",
        "    def calculate_constrained_reward(self):\n",
        "        \"\"\"\n",
        "        Calculate reward using replacement method with pre-calculated offset k.\n",
        "\n",
        "        Replacement method:\n",
        "        - If all constraints satisfied: return -J(s) + k\n",
        "        - If constraints violated: return -P(s)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tuple: (reward, constraint_results)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Get base reward from objective function (negative for minimization)\n",
        "            base_reward = self._calculate_reward()\n",
        "\n",
        "            # Evaluate constraints\n",
        "            constraint_results = self._evaluate_all_constraints()\n",
        "\n",
        "            # Apply replacement method\n",
        "            if constraint_results['all_satisfied']:\n",
        "                # All constraints satisfied: return optimization reward + offset k\n",
        "                constrained_reward = base_reward + self.offset_k\n",
        "            else:\n",
        "                # Constraints violated: return only penalty (negative)\n",
        "                total_violation = float(constraint_results['total_violation'])\n",
        "                constrained_reward = -self.penalty_factor * total_violation\n",
        "\n",
        "            # Ensure reward is a scalar\n",
        "            if hasattr(constrained_reward, '__len__'):\n",
        "                constrained_reward = float(constrained_reward)\n",
        "\n",
        "            return constrained_reward, constraint_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating replacement reward: {e}\")\n",
        "            # Fall back to base reward on error\n",
        "            return self._calculate_reward(), {\n",
        "                'all_satisfied': True,\n",
        "                'violations': {},\n",
        "                'total_violation': 0.0\n",
        "            }\n",
        "\n",
        "    def get_reward_method_info(self):\n",
        "        \"\"\"\n",
        "        Get information about the reward method being used.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict: Information about the reward method\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'method': 'replacement',\n",
        "            'offset_k': self.offset_k,\n",
        "            'k_method': self.k_method,\n",
        "            'k_samples': self.k_samples,\n",
        "            'penalty_factor': self.penalty_factor\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EvvFGCFapPvW",
      "metadata": {
        "id": "EvvFGCFapPvW"
      },
      "outputs": [],
      "source": [
        "# Add this to your main function or create a new sweep script\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def run_sweep_replacement():\n",
        "    # Base parameters (non-swept parameters)\n",
        "    base_params = {\n",
        "        \"optimizer_name\": \"Adam\",\n",
        "        \"MAX_EPISODES\": 100,\n",
        "        \"PRINT_INTERVAL\": 10,  # Print less frequently during sweep\n",
        "        \"N_TRIALS\": 2,\n",
        "        \"DROPOUT\": 0,\n",
        "        \"network_file\": \"elec_s_5_ec_lc1.0_3h.nc\",\n",
        "        \"input_dir\": \"var_constraint_map\"\n",
        "    }\n",
        "\n",
        "    # Parameters to sweep (key parameters that affect learning)\n",
        "    sweep_params = {\n",
        "        \"LEARNING_RATE\": [1e-4],\n",
        "        \"EPSILON\": [0.1],\n",
        "        \"ENTROPY_COEFFICIENT\": [0.01],\n",
        "        \"HIDDEN_DIMENSIONS\": [32],\n",
        "        \"PPO_STEPS\": [2],\n",
        "        \"BATCH_SIZE\": [64],\n",
        "        \"DISCOUNT_FACTOR\": [0.99],\n",
        "        \"constraint_penalty_factor\":[0,100],\n",
        "        \"episode_length\": [1000],\n",
        "        \"env_class\": [\"Env2Gen1LoadConstr\", \"Env2Gen1LoadConstrReplacement\"] #Will need to change this if you change the env class names\n",
        "    }\n",
        "\n",
        "    replacement_env=Env2Gen1LoadConstrReplacement(network_file=base_params[\"network_file\"], input_dir=base_params[\"input_dir\"]) #this is used to calculate the offset k for the replacement reward method\n",
        "\n",
        "    # Seeds to use for each configuration\n",
        "    #seeds = [42, 123, 7]  # Using 3 seeds for statistical significance\n",
        "    seeds = [42]\n",
        "    #seeds= [123, 7]\n",
        "\n",
        "    # Generate a subset of combinations to keep the sweep manageable\n",
        "    # We'll use a more focused approach rather than a full grid search\n",
        "\n",
        "    # Add some random combinations to explore the space more broadly\n",
        "    config_seed=42\n",
        "    import random\n",
        "    random.seed(config_seed)  # For reproducible random configs\n",
        "\n",
        "    #after setting the seed, perform sampling to determine k and store the result\n",
        "\n",
        "    num_random_configs = 1  # Adjust based on how many total runs you want\n",
        "    random_configs = []\n",
        "\n",
        "    for _ in range(num_random_configs):\n",
        "        config = {param: random.choice(values) for param, values in sweep_params.items()}\n",
        "        random_configs.append(config)\n",
        "\n",
        "    # Combine priority and random configs\n",
        "    all_configs = random_configs\n",
        "\n",
        "    # Print sweep summary\n",
        "    print(f\"Running sweep with {len(all_configs)} configurations and {len(seeds)} seeds\")\n",
        "    print(f\"Total runs: {len(all_configs) * len(seeds)}\")\n",
        "\n",
        "    # Run all configurations\n",
        "    for seed in seeds:\n",
        "        # Set all random seeds\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "        #after setting the seed, perform sampling to determine k and store the result\n",
        "        replacement_reward_offset=calculate_offset_k_initialization(envClass=replacement_env)\n",
        "        for config_idx, config in enumerate(all_configs):\n",
        "              # Create a unique run ID\n",
        "            run_id = f\"sweep_{datetime.now().strftime('%Y%m%d')}_{config_idx}_{seed}\"\n",
        "\n",
        "            my_api=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZDg3ZjNlYi04MWI3LTQ1ODctOGIxNS1iNTY3ZjgzMGYzMzYifQ==\"\n",
        "\n",
        "            # Initialize Neptune run\n",
        "            run = neptune.init_run(\n",
        "                project='EnergyGridRL/elec-s-5-ec-lc10-3h-GenDispatch',\n",
        "                api_token=my_api,\n",
        "                name=f\"Sweep-Config{config_idx}-Seed{seed}\",\n",
        "                tags=[\"hyperparameter_sweep\"]\n",
        "            )\n",
        "\n",
        "            # Combine base params with this config\n",
        "            params = {**base_params, **config}\n",
        "            params[\"SEED\"] = seed\n",
        "\n",
        "            # Log all parameters\n",
        "            for key, value in params.items():\n",
        "                run[f\"parameters/{key}\"] = value\n",
        "\n",
        "            run[\"env_class\"]=params[\"env_class\"]\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Starting run {config_idx+1}/{len(all_configs)}, seed {seed}\")\n",
        "            print(f\"Parameters: {params}\")\n",
        "            print(f\"{'='*50}\\n\")\n",
        "\n",
        "            var_id_to_name, constraints = load_mappings(network_file=params[\"network_file\"], input_dir=\"var_constraint_map\")\n",
        "\n",
        "            try:\n",
        "\n",
        "                # Create environment and agent\n",
        "                if params[\"env_class\"]==\"Env2Gen1LoadConstr\":\n",
        "                    env = Env2Gen1LoadConstr(network_file=params[\"network_file\"], episode_length=params[\"episode_length\"], constraint_penalty_factor=params[\"constraint_penalty_factor\"],input_dir=base_params[\"input_dir\"])\n",
        "                elif params[\"env_class\"]==\"Env2Gen1LoadConstrReplacement\":\n",
        "                    env = Env2Gen1LoadConstrReplacement(network_file=params[\"network_file\"], episode_length=params[\"episode_length\"], constraint_penalty_factor=params[\"constraint_penalty_factor\"], offset_k=replacement_reward_offset, input_dir=base_params[\"input_dir\"])\n",
        "                env.seed(seed)\n",
        "\n",
        "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "                agent = PPO_agent(\n",
        "                    env=env,\n",
        "                    run=run,\n",
        "                    device=device,\n",
        "                    hidden_dimensions=params[\"HIDDEN_DIMENSIONS\"],\n",
        "                    dropout=params[\"DROPOUT\"],\n",
        "                    discount_factor=params[\"DISCOUNT_FACTOR\"],\n",
        "                    optimizer_name=params[\"optimizer_name\"],\n",
        "                    max_episodes=params[\"MAX_EPISODES\"],\n",
        "                    print_interval=params[\"PRINT_INTERVAL\"],\n",
        "                    PPO_steps=params[\"PPO_STEPS\"],\n",
        "                    n_trials=params[\"N_TRIALS\"],\n",
        "                    epsilon=params[\"EPSILON\"],\n",
        "                    entropy_coefficient=params[\"ENTROPY_COEFFICIENT\"],\n",
        "                    learning_rate=params[\"LEARNING_RATE\"],\n",
        "                    batch_size=params[\"BATCH_SIZE\"],\n",
        "                    seed=seed\n",
        "                )\n",
        "\n",
        "                run[\"replacement_reward\"].log(replacement_reward_offset)\n",
        "\n",
        "                # Train the agent\n",
        "                train_rewards = agent.train()\n",
        "\n",
        "                # Log final performance metrics\n",
        "                run[\"results/final_reward\"] = train_rewards[-1]\n",
        "                run[\"results/mean_last_100_reward\"] = np.mean(train_rewards[-100:])\n",
        "                run[\"results/best_reward\"] = np.max(train_rewards)\n",
        "                run[\"results/baseline_reward\"]= evaluate_baseline_reward(network_file=params[\"network_file\"], env=env, var_id_to_name=var_id_to_name)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in run: {e}\")\n",
        "                run[\"results/error\"] = str(e)\n",
        "\n",
        "            # Close the Neptune run\n",
        "            run.stop()\n",
        "\n",
        "            # Small delay to avoid API rate limits\n",
        "            time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7fb26605",
      "metadata": {
        "id": "7fb26605"
      },
      "outputs": [],
      "source": [
        "# # Add this to your main function or create a new sweep script\n",
        "\n",
        "# import time\n",
        "# from datetime import datetime\n",
        "\n",
        "# def run_sweep_replacement():\n",
        "#     # Base parameters (non-swept parameters)\n",
        "#     base_params = {\n",
        "#         \"optimizer_name\": \"Adam\",\n",
        "#         \"MAX_EPISODES\": 1000,\n",
        "#         \"PRINT_INTERVAL\": 10,  # Print less frequently during sweep\n",
        "#         \"N_TRIALS\": 100,\n",
        "#         \"DROPOUT\": 0,\n",
        "#         \"network_file\": \"elec_s_5_ec_lcopt_3h.nc\",\n",
        "#     }\n",
        "\n",
        "#     # Parameters to sweep (key parameters that affect learning)\n",
        "#     sweep_params = {\n",
        "#         \"LEARNING_RATE\": [1e-4, 3e-4, 1e-3, 3e-3],\n",
        "#         \"EPSILON\": [0.1, 0.2, 0.3],\n",
        "#         \"ENTROPY_COEFFICIENT\": [0.01, 0.05, 0.1],\n",
        "#         \"HIDDEN_DIMENSIONS\": [32, 64, 128],\n",
        "#         \"PPO_STEPS\": [8, 16],\n",
        "#         \"BATCH_SIZE\": [128, 256],\n",
        "#         \"DISCOUNT_FACTOR\": [0.95, 0.99],\n",
        "#         \"constraint_penalty_factor\":[0,25,50,100],\n",
        "#         \"episode_length\": [2,4,6],\n",
        "#         \"env_class\": [\"Env2Gen1LoadConstr\", \"Env2Gen1LoadConstrReplacement\"] #Will need to change this if you change the env class names\n",
        "#     }\n",
        "\n",
        "#     replacement_env=Env2Gen1LoadConstrReplacement() #this is used to calculate the offset k for the replacement reward method\n",
        "\n",
        "#     # Seeds to use for each configuration\n",
        "#     #seeds = [42, 123, 7]  # Using 3 seeds for statistical significance\n",
        "#     #seeds = [42]\n",
        "#     seeds= [123, 7]\n",
        "\n",
        "#     # Generate a subset of combinations to keep the sweep manageable\n",
        "#     # We'll use a more focused approach rather than a full grid search\n",
        "\n",
        "#     # Priority configurations based on most promising parameter values\n",
        "#     # Priority configurations based on most promising parameter values\n",
        "#     priority_configs = [\n",
        "#         # Config 1: Higher learning rate, exploration focus\n",
        "#         {\"LEARNING_RATE\": 1e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFICIENT\": 0.1,\n",
        "#          \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":0, \"episode_length\": 4},\n",
        "\n",
        "#         # Config 2: Medium learning rate, balanced approach\n",
        "#         {\"LEARNING_RATE\": 3e-4, \"EPSILON\": 0.2, \"ENTROPY_COEFFICIENT\": 0.05,\n",
        "#          \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":0,\"episode_length\": 4},\n",
        "\n",
        "#         # Config 3: Conservative updates, larger network\n",
        "#         {\"LEARNING_RATE\": 1e-4, \"EPSILON\": 0.1, \"ENTROPY_COEFFICIENT\": 0.01,\n",
        "#          \"HIDDEN_DIMENSIONS\": 128, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":0,\"episode_length\": 4},\n",
        "\n",
        "#         # Config 4: Aggressive learning, smaller network\n",
        "#         {\"LEARNING_RATE\": 3e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFICIENT\": 0.1,\n",
        "#          \"HIDDEN_DIMENSIONS\": 32, \"PPO_STEPS\": 8, \"BATCH_SIZE\": 128,\n",
        "#          \"DISCOUNT_FACTOR\": 0.95, \"constraint_penalty_factor\":0,\"episode_length\": 4},\n",
        "\n",
        "#         # Config 1: Higher learning rate, exploration focus\n",
        "#         {\"LEARNING_RATE\": 1e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFICIENT\": 0.1,\n",
        "#          \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":100,\"episode_length\": 4},\n",
        "\n",
        "#         # Config 2: Medium learning rate, balanced approach\n",
        "#         {\"LEARNING_RATE\": 3e-4, \"EPSILON\": 0.2, \"ENTROPY_COEFFICIENT\": 0.05,\n",
        "#          \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":100,\"episode_length\": 4},\n",
        "\n",
        "#         # Config 3: Conservative updates, larger network\n",
        "#         {\"LEARNING_RATE\": 1e-4, \"EPSILON\": 0.1, \"ENTROPY_COEFFICIENT\": 0.01,\n",
        "#          \"HIDDEN_DIMENSIONS\": 128, \"PPO_STEPS\": 16, \"BATCH_SIZE\": 256,\n",
        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":100,\"episode_length\": 4},\n",
        "\n",
        "#         # Config 4: Aggressive learning, smaller network\n",
        "#         {\"LEARNING_RATE\": 3e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFICIENT\": 0.1,\n",
        "#          \"HIDDEN_DIMENSIONS\": 32, \"PPO_STEPS\": 8, \"BATCH_SIZE\": 128,\n",
        "#          \"DISCOUNT_FACTOR\": 0.95, \"constraint_penalty_factor\":100,\"episode_length\": 4},\n",
        "#     ]\n",
        "\n",
        "#     # Add some random combinations to explore the space more broadly\n",
        "#     config_seed=42\n",
        "#     import random\n",
        "#     random.seed(config_seed)  # For reproducible random configs\n",
        "\n",
        "#     #after setting the seed, perform sampling to determine k and store the result\n",
        "\n",
        "#     num_random_configs = 8  # Adjust based on how many total runs you want\n",
        "#     random_configs = []\n",
        "\n",
        "#     for _ in range(num_random_configs):\n",
        "#         config = {param: random.choice(values) for param, values in sweep_params.items()}\n",
        "#         random_configs.append(config)\n",
        "\n",
        "#     # Combine priority and random configs\n",
        "#     all_configs = priority_configs + random_configs\n",
        "\n",
        "#     # Print sweep summary\n",
        "#     print(f\"Running sweep with {len(all_configs)} configurations and {len(seeds)} seeds\")\n",
        "#     print(f\"Total runs: {len(all_configs) * len(seeds)}\")\n",
        "\n",
        "#     # Run all configurations\n",
        "#     for seed in seeds:\n",
        "#         # Set all random seeds\n",
        "#         random.seed(seed)\n",
        "#         np.random.seed(seed)\n",
        "#         torch.manual_seed(seed)\n",
        "#         if torch.cuda.is_available():\n",
        "#             torch.cuda.manual_seed(seed)\n",
        "#             torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "#         #after setting the seed, perform sampling to determine k and store the result\n",
        "#         replacement_reward_offset=calculate_offset_k_initialization(envClass=replacement_env)\n",
        "#         for config_idx, config in enumerate(all_configs):\n",
        "#               # Create a unique run ID\n",
        "#             run_id = f\"sweep_{datetime.now().strftime('%Y%m%d')}_{config_idx}_{seed}\"\n",
        "\n",
        "#             my_api=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZDg3ZjNlYi04MWI3LTQ1ODctOGIxNS1iNTY3ZjgzMGYzMzYifQ==\"\n",
        "\n",
        "#             # Initialize Neptune run\n",
        "#             run = neptune.init_run(\n",
        "#                 project='EnergyGridRL/PPO-2snapshots-replacement',\n",
        "#                 api_token=my_api,\n",
        "#                 name=f\"Sweep-Config{config_idx}-Seed{seed}\",\n",
        "#                 tags=[\"hyperparameter_sweep\"]\n",
        "#             )\n",
        "\n",
        "#             # Combine base params with this config\n",
        "#             params = {**base_params, **config}\n",
        "#             params[\"SEED\"] = seed\n",
        "\n",
        "#             # Log all parameters\n",
        "#             for key, value in params.items():\n",
        "#                 run[f\"parameters/{key}\"] = value\n",
        "\n",
        "#             run[\"env_class\"]=params[\"env_class\"]\n",
        "#             print(f\"\\n{'='*50}\")\n",
        "#             print(f\"Starting run {config_idx+1}/{len(all_configs)}, seed {seed}\")\n",
        "#             print(f\"Parameters: {params}\")\n",
        "#             print(f\"{'='*50}\\n\")\n",
        "\n",
        "#             try:\n",
        "\n",
        "#                 # Create environment and agent\n",
        "#                 if params[\"env_class\"]==\"Env2Gen1LoadConstr\":\n",
        "#                     env = Env2Gen1LoadConstr(network_file=params[\"network_file\"], episode_length=params[\"episode_length\"], constraint_penalty_factor=params[\"constraint_penalty_factor\"])\n",
        "#                 elif params[\"env_class\"]==\"Env2Gen1LoadConstrReplacement\":\n",
        "#                     env = Env2Gen1LoadConstrReplacement(network_file=params[\"network_file\"], episode_length=params[\"episode_length\"], constraint_penalty_factor=params[\"constraint_penalty_factor\"], offset_k=replacement_reward_offset)\n",
        "#                 else:\n",
        "#                     raise ValueError(f\"Invalid environment class: {params[\"env_class\"]}\")\n",
        "#                 env.seed(seed)\n",
        "\n",
        "#                 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#                 agent = PPO_agent(\n",
        "#                     env=env,\n",
        "#                     run=run,\n",
        "#                     device=device,\n",
        "#                     hidden_dimensions=params[\"HIDDEN_DIMENSIONS\"],\n",
        "#                     dropout=params[\"DROPOUT\"],\n",
        "#                     discount_factor=params[\"DISCOUNT_FACTOR\"],\n",
        "#                     optimizer_name=params[\"optimizer_name\"],\n",
        "#                     max_episodes=params[\"MAX_EPISODES\"],\n",
        "#                     print_interval=params[\"PRINT_INTERVAL\"],\n",
        "#                     PPO_steps=params[\"PPO_STEPS\"],\n",
        "#                     n_trials=params[\"N_TRIALS\"],\n",
        "#                     epsilon=params[\"EPSILON\"],\n",
        "#                     entropy_coefficient=params[\"ENTROPY_COEFFICIENT\"],\n",
        "#                     learning_rate=params[\"LEARNING_RATE\"],\n",
        "#                     batch_size=params[\"BATCH_SIZE\"],\n",
        "#                     seed=seed\n",
        "#                 )\n",
        "\n",
        "#                 run[\"replacement_reward\"].log(replacement_reward_offset)\n",
        "\n",
        "#                 # Train the agent\n",
        "#                 train_rewards = agent.train()\n",
        "\n",
        "#                 # Log final performance metrics\n",
        "#                 run[\"results/final_reward\"] = train_rewards[-1]\n",
        "#                 run[\"results/mean_last_100_reward\"] = np.mean(train_rewards[-100:])\n",
        "#                 run[\"results/best_reward\"] = np.max(train_rewards)\n",
        "#                 run[\"results/baseline_reward\"]= evaluate_baseline_reward(network_file=params[\"network_file\"], env=env, agent=agent)\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error in run: {e}\")\n",
        "#                 run[\"results/error\"] = str(e)\n",
        "\n",
        "#             # Close the Neptune run\n",
        "#             run.stop()\n",
        "\n",
        "#             # Small delay to avoid API rate limits\n",
        "#             time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "99a34ee8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "99a34ee8",
        "outputId": "bb2c59c9-a1f2-4e7a-990e-17654a720a09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pypsa.network.io:Importing network from PyPSA version v0.0.0 while current version is v0.35.1. Read the release notes at https://pypsa.readthedocs.io/en/latest/release_notes.html to prepare your network for import.\n",
            "WARNING:pypsa.consistency:The following lines have carriers which are not defined:\n",
            "Index(['0', '1', '2', '3', '4', '5', '6'], dtype='object', name='Line')\n",
            "WARNING:pypsa.consistency:The following buses have carriers which are not defined:\n",
            "Index(['ZA0 0', 'ZA0 1', 'ZA0 2', 'ZA0 3', 'ZA0 4', 'ZA1 0', 'ZA2 0', 'ZA3 0'], dtype='object', name='Bus')\n",
            "Writing constraints.: 100%|\u001b[38;2;128;191;255m██████████\u001b[0m| 13/13 [00:01<00:00, 10.09it/s]\n",
            "Writing continuous variables.: 100%|\u001b[38;2;128;191;255m██████████\u001b[0m| 6/6 [00:00<00:00, 21.45it/s]\n",
            "WARNING:linopy.constants:Optimization potentially failed: \n",
            "Status: warning\n",
            "Termination condition: infeasible\n",
            "Solution: 0 primals, 0 duals\n",
            "Objective: nan\n",
            "Solver model: available\n",
            "Solver message: Infeasible\n",
            "\n",
            "WARNING:pypsa.consistency:The following sub_networks have carriers which are not defined:\n",
            "Index(['0'], dtype='object', name='SubNetwork')\n",
            "WARNING:pypsa.consistency:The following lines have carriers which are not defined:\n",
            "Index(['0', '1', '2', '3', '4', '5', '6'], dtype='object', name='Line')\n",
            "WARNING:pypsa.consistency:The following buses have carriers which are not defined:\n",
            "Index(['ZA0 0', 'ZA0 1', 'ZA0 2', 'ZA0 3', 'ZA0 4', 'ZA1 0', 'ZA2 0', 'ZA3 0'], dtype='object', name='Bus')\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'min_load' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3330791089.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_sweep_replacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1102575295.py\u001b[0m in \u001b[0;36mrun_sweep_replacement\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     }\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mreplacement_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEnv2Gen1LoadConstrReplacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"network_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this is used to calculate the offset k for the replacement reward method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Seeds to use for each configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-237244066.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, network_file, episode_length, constraint_penalty_factor, offset_k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetwork_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint_penalty_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint_penalty_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint_penalty_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-416271505.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, network_file, episode_length, constraint_penalty_factor)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Create observation space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mlow_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_bounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_observation_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         self.observation_space = spaces.Box(\n\u001b[1;32m     88\u001b[0m             \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-416271505.py\u001b[0m in \u001b[0;36mcreate_observation_bounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mmax_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mload_p_lower\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0mload_p_upper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'min_load' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "run_sweep_replacement()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

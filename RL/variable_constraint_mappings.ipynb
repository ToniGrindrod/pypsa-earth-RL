{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXV2cPQuRbis",
        "outputId": "d3ccfa3d-f4ce-4393-874e-120118186b34"
      },
      "id": "NXV2cPQuRbis",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypsa in /usr/local/lib/python3.12/dist-packages (0.35.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pypsa) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pypsa) (1.16.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from pypsa) (2.2.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (from pypsa) (2025.8.0)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.12/dist-packages (from pypsa) (1.7.2)\n",
            "Requirement already satisfied: linopy>=0.4 in /usr/local/lib/python3.12/dist-packages (from pypsa) (0.5.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pypsa) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from pypsa) (5.24.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from pypsa) (0.13.2)\n",
            "Requirement already satisfied: geopandas>=0.9 in /usr/local/lib/python3.12/dist-packages (from pypsa) (1.1.1)\n",
            "Requirement already satisfied: shapely<2.1 in /usr/local/lib/python3.12/dist-packages (from pypsa) (2.0.7)\n",
            "Requirement already satisfied: networkx>=2 in /usr/local/lib/python3.12/dist-packages (from pypsa) (3.5)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.12/dist-packages (from pypsa) (2.1.0)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.12/dist-packages (from pypsa) (0.35.0)\n",
            "Requirement already satisfied: highspy in /usr/local/lib/python3.12/dist-packages (from pypsa) (1.11.0)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.9->pypsa) (0.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.9->pypsa) (25.0)\n",
            "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.9->pypsa) (3.7.2)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (1.4.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (0.12.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (2.11.0)\n",
            "Requirement already satisfied: dask>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (2025.5.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from linopy>=0.4->pypsa) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->pypsa) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->pypsa) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->pypsa) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pypsa) (3.2.3)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.12/dist-packages (from netcdf4->pypsa) (1.6.4.post1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netcdf4->pypsa) (2025.8.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->pypsa) (8.5.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (2025.3.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask>=0.18.0->linopy>=0.4->pypsa) (6.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.24->pypsa) (1.17.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask>=0.18.0->linopy>=0.4->pypsa) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5959066e",
      "metadata": {
        "id": "5959066e"
      },
      "outputs": [],
      "source": [
        "import pypsa\n",
        "import numpy as np\n",
        "import gc\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "import xarray as xr # Import xarray here\n",
        "\n",
        "# Define a simple, picklable representation for linear expressions\n",
        "class PicklableLinearExpr:\n",
        "    def __init__(self, vars, coeffs, const=0):\n",
        "        self.vars = vars\n",
        "        self.coeffs = coeffs\n",
        "        self.const = const\n",
        "\n",
        "    def __repr__(self):\n",
        "        terms = [f\"{coeff}*var_id_{var}\" for var, coeff in zip(self.vars, self.coeffs)]\n",
        "        expr_str = \" + \".join(terms)\n",
        "        if self.const != 0:\n",
        "            expr_str += f\" + {self.const}\"\n",
        "        return expr_str\n",
        "\n",
        "def fix_artificial_lines_hardcoded(network):\n",
        "    \"\"\"\n",
        "    Fix artificial lines with hardcoded values to match existing non-extendable lines:\n",
        "    - s_nom = 442.4 (your desired capacity)\n",
        "    - s_nom_extendable = False\n",
        "    - s_nom_min = 0\n",
        "    - s_nom_max = inf\n",
        "    - extendable = False (if column exists)\n",
        "    \"\"\"\n",
        "    print(\"=== FIXING ARTIFICIAL LINES WITH HARDCODED VALUES ===\")\n",
        "    fixed_capacity=442.4\n",
        "\n",
        "    # Find artificial lines\n",
        "    artificial_lines = [line for line in network.lines.index\n",
        "                       if any(keyword in str(line).lower() for keyword in ['new', '<->', 'artificial'])]\n",
        "\n",
        "    print(f\"Found {len(artificial_lines)} artificial lines to fix:\")\n",
        "\n",
        "    # Fix each artificial line with hardcoded values\n",
        "    for line_name in artificial_lines:\n",
        "        print(f\"\\nðŸ”§ Fixing: {line_name}\")\n",
        "\n",
        "        # s_nom = 442.4 (your fixed capacity)\n",
        "        old_s_nom = network.lines.loc[line_name, 's_nom']\n",
        "        network.lines.loc[line_name, 's_nom'] = fixed_capacity\n",
        "        print(f\"    s_nom: {old_s_nom} â†’ {fixed_capacity}\")\n",
        "\n",
        "        # s_nom_extendable = False\n",
        "        if 's_nom_extendable' not in network.lines.columns:\n",
        "            network.lines['s_nom_extendable'] = False\n",
        "        network.lines.loc[line_name, 's_nom_extendable'] = False\n",
        "        print(f\"    s_nom_extendable: â†’ False\")\n",
        "\n",
        "        # s_nom_min = 0\n",
        "        if 's_nom_min' not in network.lines.columns:\n",
        "            network.lines['s_nom_min'] = 0.0\n",
        "        network.lines.loc[line_name, 's_nom_min'] = 0.0\n",
        "        print(f\"    s_nom_min: â†’ 0.0\")\n",
        "\n",
        "        # s_nom_max = inf\n",
        "        if 's_nom_max' not in network.lines.columns:\n",
        "            network.lines['s_nom_max'] = float('inf')\n",
        "        network.lines.loc[line_name, 's_nom_max'] = float('inf')\n",
        "        print(f\"    s_nom_max: â†’ inf\")\n",
        "\n",
        "    return network\n",
        "\n",
        "def create_pypsa_network(network_file):\n",
        "    \"\"\"Create a PyPSA network from the .nc file.\"\"\"\n",
        "    # Initialize network\n",
        "    network = pypsa.Network(network_file)\n",
        "    for storage_name in network.storage_units.index:\n",
        "        # Use .loc for direct assignment to avoid SettingWithCopyWarning\n",
        "        network.storage_units.loc[storage_name, 'cyclic_state_of_charge'] = False\n",
        "\n",
        "    fix_artificial_lines_hardcoded(network)\n",
        "\n",
        "    return network\n",
        "\n",
        "def _variable_constraint_mapping(network_file, constraints_to_skip):\n",
        "    \"\"\"\n",
        "    Initialize all optimization components in one pass to avoid creating multiple models.\n",
        "    This method:\n",
        "    1. Creates the optimization model once\n",
        "    2. Extracts objective components (vars, coeffs, const)\n",
        "    3. Creates the variable ID to name mapping\n",
        "    4. Extracts constraints\n",
        "    5. Cleans up the model\n",
        "    \"\"\"\n",
        "    network = create_pypsa_network(network_file)\n",
        "    # Create model once - this is an expensive operation\n",
        "    temp_model = network.optimize.create_model()\n",
        "\n",
        "    # Create variable ID to name mapping\n",
        "    var_id_to_name = {}\n",
        "    for var_name, variable in temp_model.variables.items():\n",
        "        # Get the variable labels (IDs) for this variable\n",
        "        var_labels = variable.labels\n",
        "\n",
        "        if hasattr(var_labels, 'values'):\n",
        "            # Multi-dimensional variable\n",
        "            labels_flat = var_labels.values.flatten()\n",
        "            coords = variable.labels.coords\n",
        "            for i, label in enumerate(labels_flat):\n",
        "                if label != -1:  # -1 means no variable\n",
        "                    # Create a name that includes the index for multi-dim variables\n",
        "                    if len(coords) > 0:\n",
        "                        # Get the coordinate values for this flat index\n",
        "                        unravel_idx = np.unravel_index(i, var_labels.shape)\n",
        "                        coord_values = []\n",
        "                        for dim_idx, dim_name in enumerate(var_labels.dims):\n",
        "                            coord_val = coords[dim_name].values[unravel_idx[dim_idx]]\n",
        "\n",
        "                            # Handle datetime64 values properly\n",
        "                            if isinstance(coord_val, np.datetime64) or hasattr(coord_val, 'strftime'):\n",
        "                                # Convert datetime to string in ISO format\n",
        "                                try:\n",
        "                                    coord_val = pd.Timestamp(coord_val).isoformat()\n",
        "                                except:\n",
        "                                    # Fallback if conversion fails\n",
        "                                    coord_val = str(coord_val)\n",
        "\n",
        "                            coord_values.append(f\"{dim_name}={coord_val}\")\n",
        "\n",
        "                        full_name = f\"{var_name}[{','.join(coord_values)}]\"\n",
        "                    else:\n",
        "                        full_name = f\"{var_name}[{i}]\"\n",
        "                    var_id_to_name[label] = full_name\n",
        "        else:\n",
        "            # Scalar variable\n",
        "            var_id_to_name[var_labels] = var_name\n",
        "\n",
        "    # Store constraint information\n",
        "    constraints = {}\n",
        "    for name, constraint_group in temp_model.constraints.items():\n",
        "        # Corrected condition to skip desired constraints\n",
        "        if name in constraints_to_skip:\n",
        "            continue\n",
        "        # Check if this is a constraint group with multiple individual constraints\n",
        "        if hasattr(constraint_group.lhs, 'shape') and len(constraint_group.lhs.shape) > 0:\n",
        "            # This is a constraint group with multiple individual constraints\n",
        "            # We need to extract each individual constraint\n",
        "\n",
        "            # Get the dimensions of the constraint group\n",
        "            dims = constraint_group.lhs.dims if hasattr(constraint_group.lhs, 'dims') else []\n",
        "\n",
        "            # If it has dimensions, iterate through each individual constraint\n",
        "            if dims:\n",
        "                # Get coordinate values for each dimension\n",
        "                coords = {}\n",
        "                for dim in dims:\n",
        "                    if hasattr(constraint_group.lhs, 'coords') and dim in constraint_group.lhs.coords:\n",
        "                        coords[dim] = constraint_group.lhs.coords[dim].values\n",
        "\n",
        "                # Create a flat iterator through all combinations of coordinates\n",
        "                if coords:\n",
        "                    try:\n",
        "                        # Create all combinations of coordinate indices - only use dimensions that exist in coords\n",
        "                        valid_dims = [dim for dim in dims if dim in coords]\n",
        "                        if not valid_dims:\n",
        "                            # No valid dimensions found, skip this constraint group\n",
        "                            print(f\"Warning: No valid dimensions found for constraint {name}\")\n",
        "                            continue\n",
        "\n",
        "                        # Create shape tuple for ndindex\n",
        "                        shape_tuple = tuple(len(coords[dim]) for dim in valid_dims)\n",
        "                        if not shape_tuple:\n",
        "                            # Empty shape tuple, skip this constraint group\n",
        "                            print(f\"Warning: Empty shape tuple for constraint {name}\")\n",
        "                            continue\n",
        "\n",
        "                        # Create iterator\n",
        "                        indices = np.ndindex(shape_tuple)\n",
        "\n",
        "                        # Iterate through all combinations\n",
        "                        for idx in indices:\n",
        "                            try:\n",
        "                                # Create a key for this specific constraint\n",
        "                                coord_values = []\n",
        "                                for i, dim in enumerate(valid_dims):\n",
        "                                    coord_values.append(f\"{dim}={coords[dim][idx[i]]}\")\n",
        "\n",
        "                                specific_key = f\"{name}[{','.join(coord_values)}]\"\n",
        "\n",
        "                                # Extract the specific constraint values - with error handling\n",
        "                                try:\n",
        "                                    # For LHS\n",
        "                                    if hasattr(constraint_group.lhs.vars, '__getitem__') and hasattr(constraint_group.lhs, 'coeffs'):\n",
        "                                        # For linear expressions - safely get values\n",
        "                                        try:\n",
        "                                            lhs_vars_indexed = constraint_group.lhs.vars[idx]\n",
        "                                            lhs_coeffs_indexed = constraint_group.lhs.coeffs[idx]\n",
        "\n",
        "                                            # Check if they are DataArrays and flatten accordingly\n",
        "                                            lhs_vars = lhs_vars_indexed.values.flatten() if isinstance(lhs_vars_indexed, xr.DataArray) else lhs_vars_indexed.flatten()\n",
        "                                            lhs_coeffs = lhs_coeffs_indexed.values.flatten() if isinstance(lhs_coeffs_indexed, xr.DataArray) else lhs_coeffs_indexed.flatten()\n",
        "\n",
        "                                        except Exception as e:\n",
        "                                            print(f\"Warning: Error accessing constraint values for {specific_key}: {e}\")\n",
        "                                            continue\n",
        "\n",
        "                                        # Create a picklable representation\n",
        "                                        specific_lhs = PicklableLinearExpr(lhs_vars, lhs_coeffs)\n",
        "\n",
        "                                        # Add constant if it exists - safely\n",
        "                                        if hasattr(constraint_group.lhs, 'const'):\n",
        "                                            try:\n",
        "                                                if hasattr(constraint_group.lhs.const, '__getitem__'):\n",
        "                                                    const_val = constraint_group.lhs.const[idx]\n",
        "                                                else:\n",
        "                                                    const_val = constraint_group.lhs.const\n",
        "                                                specific_lhs.const = const_val\n",
        "                                            except Exception as e:\n",
        "                                                # If error accessing const, just use 0\n",
        "                                                specific_lhs.const = 0\n",
        "                                                print(f\"Warning: Error accessing const for {specific_key}: {e}\")\n",
        "                                    else:\n",
        "                                        # For simple values that are not linear expressions\n",
        "                                        try:\n",
        "                                            if hasattr(constraint_group.lhs, '__getitem__'):\n",
        "                                                specific_lhs = constraint_group.lhs[idx]\n",
        "                                            else:\n",
        "                                                specific_lhs = constraint_group.lhs\n",
        "                                        except Exception as e:\n",
        "                                            print(f\"Warning: Error accessing LHS for {specific_key}: {e}\")\n",
        "                                            continue\n",
        "\n",
        "                                    # For RHS - safely\n",
        "                                    try:\n",
        "                                        if hasattr(constraint_group.rhs, '__getitem__'):\n",
        "                                            rhs_val = constraint_group.rhs[idx]\n",
        "                                        else:\n",
        "                                            rhs_val = constraint_group.rhs\n",
        "                                        specific_rhs = rhs_val\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"Warning: Error accessing RHS for {specific_key}: {e}\")\n",
        "                                        continue\n",
        "\n",
        "                                    # For sign - safely\n",
        "                                    try:\n",
        "                                        if hasattr(constraint_group.sign, '__getitem__'):\n",
        "                                            sign_val = constraint_group.sign[idx].values.item()\n",
        "                                        else:\n",
        "                                            sign_val = constraint_group.sign\n",
        "                                        specific_sign = sign_val\n",
        "                                    except Exception as e:\n",
        "                                        print(f\"Warning: Error accessing sign for {specific_key}: {e}\")\n",
        "                                        specific_sign = '>=' # Default sign\n",
        "\n",
        "                                    # Store this specific constraint\n",
        "                                    constraints[specific_key] = {\n",
        "                                        'lhs': specific_lhs,\n",
        "                                        'rhs': specific_rhs,\n",
        "                                        'sign': specific_sign\n",
        "                                    }\n",
        "                                except Exception as e:\n",
        "                                    print(f\"Warning: Error processing constraint {specific_key}: {e}\")\n",
        "                                    continue\n",
        "                            except Exception as e:\n",
        "                                print(f\"Warning: Error creating key for constraint: {e}\")\n",
        "                                continue\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Error creating indices for constraint {name}: {e}\")\n",
        "                        continue\n",
        "            else: #no case handling for when no dimensions but still has shape\n",
        "                print(f\"Warning: No dimensions found for constraint {name}\")\n",
        "                continue\n",
        "        else:\n",
        "            # This is a single constraint, store it directly\n",
        "            try:\n",
        "                constraints[name] = {\n",
        "                    'lhs': constraint_group.lhs.copy() if hasattr(constraint_group.lhs, 'copy') else constraint_group.lhs,\n",
        "                    'rhs': constraint_group.rhs.copy() if hasattr(constraint_group.rhs, 'copy') else constraint_group.rhs,\n",
        "                    'sign': constraint_group.sign\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error storing single constraint {name}: {e}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "    # Clean up to free memory\n",
        "    del temp_model\n",
        "    gc.collect()\n",
        "\n",
        "    return var_id_to_name, constraints\n",
        "\n",
        "def save_mappings(var_id_to_name, constraints, network_file, output_dir=\"var_constraint_map\"):\n",
        "    \"\"\"\n",
        "    Save the variable ID to name mapping and constraints to files in Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    var_id_to_name : dict\n",
        "        Mapping from variable IDs to variable names\n",
        "    constraints : dict\n",
        "        Dictionary of constraints\n",
        "    network_file : str\n",
        "        Path to the network file used to create the mappings\n",
        "    output_dir : str, optional\n",
        "        Directory relative to Google Drive MyDrive to save the mapping files to\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (var_map_path, constraints_path) - Paths to the saved mapping files\n",
        "    \"\"\"\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Create the full path to Google Drive directory\n",
        "    gdrive_base = '/content/drive/MyDrive/Colab_Notebooks'\n",
        "    full_output_dir = os.path.join(gdrive_base, output_dir)\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(full_output_dir, exist_ok=True)\n",
        "\n",
        "    # Get the network filename without path or extension\n",
        "    network_name = os.path.basename(network_file)\n",
        "    network_name = os.path.splitext(network_name)[0]\n",
        "\n",
        "    # Create output filenames\n",
        "    var_map_file = os.path.join(full_output_dir, f\"{network_name}_var_id_to_name.pkl\")\n",
        "    constraints_file = os.path.join(full_output_dir, f\"{network_name}_constraints.pkl\")\n",
        "\n",
        "    # Save mappings to files\n",
        "    with open(var_map_file, 'wb') as f:\n",
        "        pickle.dump(var_id_to_name, f)\n",
        "\n",
        "    with open(constraints_file, 'wb') as f:\n",
        "        pickle.dump(constraints, f)\n",
        "\n",
        "    print(f\"Saved variable mapping to: {var_map_file}\")\n",
        "    print(f\"Saved constraints to: {constraints_file}\")\n",
        "\n",
        "    return var_map_file, constraints_file\n",
        "\n",
        "def load_mappings(network_file, input_dir=\"var_constraint_map\"):\n",
        "    \"\"\"\n",
        "    Load previously saved variable ID to name mapping and constraints from files in Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    network_file : str\n",
        "        Path to the network file used to create the mappings\n",
        "    input_dir : str, optional\n",
        "        Directory relative to Google Drive MyDrive where the mapping files are stored\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (var_id_to_name, constraints) - The loaded mappings\n",
        "    \"\"\"\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Create the full path to Google Drive directory\n",
        "    gdrive_base = '/content/drive/MyDrive/Colab_Notebooks'\n",
        "    full_input_dir = os.path.join(gdrive_base, input_dir)\n",
        "\n",
        "    # Get the network filename without path or extension\n",
        "    network_name = os.path.basename(network_file)\n",
        "    network_name = os.path.splitext(network_name)[0]\n",
        "\n",
        "    # Create input filenames\n",
        "    var_map_file = os.path.join(full_input_dir, f\"{network_name}_var_id_to_name.pkl\")\n",
        "    constraints_file = os.path.join(full_input_dir, f\"{network_name}_constraints.pkl\")\n",
        "\n",
        "    # Check if files exist\n",
        "    if not os.path.exists(var_map_file) or not os.path.exists(constraints_file):\n",
        "        raise FileNotFoundError(f\"Mapping files for {network_name} not found in {full_input_dir}\")\n",
        "\n",
        "    # Load mappings from files\n",
        "    with open(var_map_file, 'rb') as f:\n",
        "        var_id_to_name = pickle.load(f)\n",
        "\n",
        "    with open(constraints_file, 'rb') as f:\n",
        "        constraints = pickle.load(f)\n",
        "\n",
        "    print(f\"Loaded variable mapping from: {var_map_file}\")\n",
        "    print(f\"Loaded constraints from: {constraints_file}\")\n",
        "\n",
        "    return var_id_to_name, constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c253e4",
      "metadata": {
        "id": "44c253e4"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Example usage\n",
        "    drive.mount('/content/drive')\n",
        "    network_file = \"/content/drive/MyDrive/Colab_Notebooks/networks_1_year_connected/elec_s_10_ec_lc1.0_1h.nc\"\n",
        "    # constraints_to_skip = [\n",
        "    #     \"StorageUnit-fix-p_dispatch-lower\",\n",
        "    #     \"StorageUnit-fix-p_dispatch-upper\",\n",
        "    #     \"StorageUnit-fix-p_store-lower\",\n",
        "    #     \"StorageUnit-fix-p_store-upper\",\n",
        "    #     \"StorageUnit-fix-state_of_charge-lower\",\n",
        "    #     \"StorageUnit-fix-state_of_charge-upper\",\n",
        "    #     \"StorageUnit-energy_balance\"\n",
        "    # ]\n",
        "    constraints_to_skip = []\n",
        "\n",
        "    # Create mappings\n",
        "    var_id_to_name, constraints = _variable_constraint_mapping(network_file, constraints_to_skip)\n",
        "\n",
        "    # Save mappings\n",
        "    var_map_file, constraints_file = save_mappings(var_id_to_name, constraints, network_file)\n",
        "\n",
        "    print(f\"Created variable ID to name mapping with {len(var_id_to_name)} entries\")\n",
        "    print(f\"Created constraints mapping with {len(constraints)} entries\")\n",
        "    print(f\"Saved variable mapping to {var_map_file}\")\n",
        "    print(f\"Saved constraints mapping to {constraints_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kASMIIT_RpD6",
        "outputId": "5c91966b-056a-4159-812b-2906e7ff7e39"
      },
      "id": "kASMIIT_RpD6",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pypsa.network.io:Importing network from PyPSA version v0.0.0 while current version is v0.35.2. Read the release notes at https://pypsa.readthedocs.io/en/latest/release_notes.html to prepare your network for import.\n",
            "WARNING:pypsa.consistency:The following lines have carriers which are not defined:\n",
            "Index(['0', '1', '10', '11', '12', '13', '14', '15', '2', '3', '4', '5', '6',\n",
            "       '7', '8', '9', 'lines new ZA0 4 <-> ZA2 0 AC',\n",
            "       'lines new ZA0 0 <-> ZA1 0 AC', 'lines new ZA0 0 <-> ZA3 0 AC'],\n",
            "      dtype='object', name='Line')\n",
            "WARNING:pypsa.consistency:The following buses have carriers which are not defined:\n",
            "Index(['ZA0 0', 'ZA0 1', 'ZA0 2', 'ZA0 3', 'ZA0 4', 'ZA0 5', 'ZA0 6', 'ZA0 7',\n",
            "       'ZA0 8', 'ZA0 9', 'ZA1 0', 'ZA2 0', 'ZA3 0'],\n",
            "      dtype='object', name='Bus')\n",
            "/usr/local/lib/python3.12/dist-packages/linopy/expressions.py:1861: FutureWarning:\n",
            "\n",
            "In a future version of xarray the default value for join will change from join='outer' to join='exact'. This change will result in the following ValueError: cannot be aligned with join='exact' because index/labels/sizes are not equal along these coordinates (dimensions): '_term' ('_term',) The recommendation is to set join explicitly for this case.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "network_file = \"/content/drive/MyDrive/Colab_Notebooks/networks_1_year_connected/elec_s_10_ec_lc1.0_1h.nc\"\n",
        "network = create_pypsa_network(network_file)\n",
        "# Create model once - this is an expensive operation\n",
        "temp_model = network.optimize.create_model()\n",
        "\n",
        "for name, constraint_group in temp_model.constraints.items():\n",
        "    print(name)"
      ],
      "metadata": {
        "id": "qf2QxV5H_99m",
        "outputId": "00c94606-a60d-4e5d-9977-084e8729d74b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qf2QxV5H_99m",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypsa.network.io:Importing network from PyPSA version v0.0.0 while current version is v0.35.2. Read the release notes at https://pypsa.readthedocs.io/en/latest/release_notes.html to prepare your network for import.\n",
            "WARNING:pypsa.consistency:The following buses have carriers which are not defined:\n",
            "Index(['ZA0 0', 'ZA0 1', 'ZA0 2', 'ZA0 3', 'ZA0 4', 'ZA0 5', 'ZA0 6', 'ZA0 7',\n",
            "       'ZA0 8', 'ZA0 9', 'ZA1 0', 'ZA2 0', 'ZA3 0'],\n",
            "      dtype='object', name='Bus')\n",
            "WARNING:pypsa.consistency:The following lines have carriers which are not defined:\n",
            "Index(['0', '1', '10', '11', '12', '13', '14', '15', '2', '3', '4', '5', '6',\n",
            "       '7', '8', '9', 'lines new ZA0 4 <-> ZA2 0 AC',\n",
            "       'lines new ZA0 0 <-> ZA1 0 AC', 'lines new ZA0 0 <-> ZA3 0 AC'],\n",
            "      dtype='object', name='Line')\n",
            "/usr/local/lib/python3.12/dist-packages/linopy/expressions.py:1861: FutureWarning:\n",
            "\n",
            "In a future version of xarray the default value for join will change from join='outer' to join='exact'. This change will result in the following ValueError: cannot be aligned with join='exact' because index/labels/sizes are not equal along these coordinates (dimensions): '_term' ('_term',) The recommendation is to set join explicitly for this case.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line-ext-s_nom-lower\n",
            "Line-ext-s_nom-upper\n",
            "Generator-fix-p-lower\n",
            "Generator-fix-p-upper\n",
            "Line-fix-s-lower\n",
            "Line-fix-s-upper\n",
            "Line-ext-s-lower\n",
            "Line-ext-s-upper\n",
            "StorageUnit-fix-p_dispatch-lower\n",
            "StorageUnit-fix-p_dispatch-upper\n",
            "StorageUnit-fix-p_store-lower\n",
            "StorageUnit-fix-p_store-upper\n",
            "StorageUnit-fix-state_of_charge-lower\n",
            "StorageUnit-fix-state_of_charge-upper\n",
            "Bus-nodal_balance\n",
            "Kirchhoff-Voltage-Law\n",
            "StorageUnit-energy_balance\n",
            "GlobalConstraint-lc_limit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_model.constraints[\"Line-ext-s_nom-lower\"]"
      ],
      "metadata": {
        "id": "l7Qf5L1mOO-F",
        "outputId": "fd307c7f-1875-485e-d0be-f246b7e1707d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "l7Qf5L1mOO-F",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Constraint `Line-ext-s_nom-lower` [Line-ext: 3]:\n",
              "------------------------------------------------\n",
              "[lines new ZA0 4 <-> ZA2 0 AC]: +1 Line-s_nom[lines new ZA0 4 <-> ZA2 0 AC] â‰¥ 1.0\n",
              "[lines new ZA0 0 <-> ZA1 0 AC]: +1 Line-s_nom[lines new ZA0 0 <-> ZA1 0 AC] â‰¥ 1.0\n",
              "[lines new ZA0 0 <-> ZA3 0 AC]: +1 Line-s_nom[lines new ZA0 0 <-> ZA3 0 AC] â‰¥ 1.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp_model.objective)"
      ],
      "metadata": {
        "id": "CfjHIB87AB_G",
        "outputId": "6e0d4b30-840f-43a7-a6ab-552d6f5460ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CfjHIB87AB_G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objective:\n",
            "----------\n",
            "LinearExpression: +30.1 Generator-p[2013-01-01 00:00:00, ZA0 0 coal] + 0.015 Generator-p[2013-01-01 00:00:00, ZA0 0 onwind] + 0.01 Generator-p[2013-01-01 00:00:00, ZA0 0 solar] ... +5773 Line-s_nom[lines new ZA0 4 <-> ZA2 0 AC] + 6033 Line-s_nom[lines new ZA0 0 <-> ZA1 0 AC] + 1.482e+04 Line-s_nom[lines new ZA0 0 <-> ZA3 0 AC]\n",
            "Sense: min\n",
            "Value: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(network.lines.s_nom)\n",
        "# for attr in dir(network.lines):\n",
        "#   print(attr)\n",
        "print(network.lines.s_nom_extendable)\n"
      ],
      "metadata": {
        "id": "UHb7Ur3cbf2_",
        "outputId": "f354514d-aa9d-4bbb-a94e-c042d7a358c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UHb7Ur3cbf2_",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line\n",
            "0                               False\n",
            "1                               False\n",
            "10                              False\n",
            "11                              False\n",
            "12                              False\n",
            "13                              False\n",
            "14                              False\n",
            "15                              False\n",
            "2                               False\n",
            "3                               False\n",
            "4                               False\n",
            "5                               False\n",
            "6                               False\n",
            "7                               False\n",
            "8                               False\n",
            "9                               False\n",
            "lines new ZA0 4 <-> ZA2 0 AC     True\n",
            "lines new ZA0 0 <-> ZA1 0 AC     True\n",
            "lines new ZA0 0 <-> ZA3 0 AC     True\n",
            "Name: s_nom_extendable, dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix network with hardcoded values to match existing non-extendable lines\n",
        "\n",
        "import pypsa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "def fix_artificial_lines_hardcoded(network):\n",
        "    \"\"\"\n",
        "    Fix artificial lines with hardcoded values to match existing non-extendable lines:\n",
        "    - s_nom = 442.4 (your desired capacity)\n",
        "    - s_nom_extendable = False\n",
        "    - s_nom_min = 0\n",
        "    - s_nom_max = inf\n",
        "    - extendable = False (if column exists)\n",
        "    \"\"\"\n",
        "    print(\"=== FIXING ARTIFICIAL LINES WITH HARDCODED VALUES ===\")\n",
        "    fixed_capacity=442.4\n",
        "\n",
        "    # Find artificial lines\n",
        "    artificial_lines = [line for line in network.lines.index\n",
        "                       if any(keyword in str(line).lower() for keyword in ['new', '<->', 'artificial'])]\n",
        "\n",
        "    print(f\"Found {len(artificial_lines)} artificial lines to fix:\")\n",
        "\n",
        "    # Fix each artificial line with hardcoded values\n",
        "    for line_name in artificial_lines:\n",
        "        print(f\"\\nðŸ”§ Fixing: {line_name}\")\n",
        "\n",
        "        # s_nom = 442.4 (your fixed capacity)\n",
        "        old_s_nom = network.lines.loc[line_name, 's_nom']\n",
        "        network.lines.loc[line_name, 's_nom'] = fixed_capacity\n",
        "        print(f\"    s_nom: {old_s_nom} â†’ {fixed_capacity}\")\n",
        "\n",
        "        # s_nom_extendable = False\n",
        "        if 's_nom_extendable' not in network.lines.columns:\n",
        "            network.lines['s_nom_extendable'] = False\n",
        "        network.lines.loc[line_name, 's_nom_extendable'] = False\n",
        "        print(f\"    s_nom_extendable: â†’ False\")\n",
        "\n",
        "        # s_nom_min = 0\n",
        "        if 's_nom_min' not in network.lines.columns:\n",
        "            network.lines['s_nom_min'] = 0.0\n",
        "        network.lines.loc[line_name, 's_nom_min'] = 0.0\n",
        "        print(f\"    s_nom_min: â†’ 0.0\")\n",
        "\n",
        "        # s_nom_max = inf\n",
        "        if 's_nom_max' not in network.lines.columns:\n",
        "            network.lines['s_nom_max'] = float('inf')\n",
        "        network.lines.loc[line_name, 's_nom_max'] = float('inf')\n",
        "        print(f\"    s_nom_max: â†’ inf\")\n",
        "\n",
        "    return network"
      ],
      "metadata": {
        "id": "WrjuO0IsbPQa"
      },
      "id": "WrjuO0IsbPQa",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find artificial lines\n",
        "network= fix_artificial_lines_hardcoded(network)\n",
        "artificial_lines = [line for line in network.lines.index\n",
        "                   if any(keyword in str(line).lower() for keyword in ['new', '<->', 'artificial'])]\n",
        "\n",
        "print(f\"\\nArtificial lines found: {len(artificial_lines)}\")\n",
        "for line in artificial_lines:\n",
        "    s_nom = network.lines.loc[line, 's_nom']\n",
        "    print(f\"  {line}:\")\n",
        "    print(f\"    s_nom: {s_nom}\")"
      ],
      "metadata": {
        "id": "BMaTALcVZ2cu",
        "outputId": "13cf2b63-ee0c-4d7f-858b-fefe8be96e0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BMaTALcVZ2cu",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FIXING ARTIFICIAL LINES WITH HARDCODED VALUES ===\n",
            "Found 3 artificial lines to fix:\n",
            "\n",
            "ðŸ”§ Fixing: lines new ZA0 4 <-> ZA2 0 AC\n",
            "    s_nom: 442.4 â†’ 442.4\n",
            "    s_nom_extendable: â†’ False\n",
            "    s_nom_min: â†’ 0.0\n",
            "    s_nom_max: â†’ inf\n",
            "\n",
            "ðŸ”§ Fixing: lines new ZA0 0 <-> ZA1 0 AC\n",
            "    s_nom: 442.4 â†’ 442.4\n",
            "    s_nom_extendable: â†’ False\n",
            "    s_nom_min: â†’ 0.0\n",
            "    s_nom_max: â†’ inf\n",
            "\n",
            "ðŸ”§ Fixing: lines new ZA0 0 <-> ZA3 0 AC\n",
            "    s_nom: 442.4 â†’ 442.4\n",
            "    s_nom_extendable: â†’ False\n",
            "    s_nom_min: â†’ 0.0\n",
            "    s_nom_max: â†’ inf\n",
            "\n",
            "Artificial lines found: 3\n",
            "  lines new ZA0 4 <-> ZA2 0 AC:\n",
            "    s_nom: 442.4\n",
            "  lines new ZA0 0 <-> ZA1 0 AC:\n",
            "    s_nom: 442.4\n",
            "  lines new ZA0 0 <-> ZA3 0 AC:\n",
            "    s_nom: 442.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_model = network.optimize.create_model()\n",
        "\n",
        "for name, constraint_group in temp_model.constraints.items():\n",
        "    print(name)"
      ],
      "metadata": {
        "id": "UEpKUG1adjTS",
        "outputId": "1fcae89b-ac67-4044-ee14-7c073d962e45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UEpKUG1adjTS",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypsa.consistency:The following buses have carriers which are not defined:\n",
            "Index(['ZA0 0', 'ZA0 1', 'ZA0 2', 'ZA0 3', 'ZA0 4', 'ZA0 5', 'ZA0 6', 'ZA0 7',\n",
            "       'ZA0 8', 'ZA0 9', 'ZA1 0', 'ZA2 0', 'ZA3 0'],\n",
            "      dtype='object', name='Bus')\n",
            "WARNING:pypsa.consistency:The following lines have carriers which are not defined:\n",
            "Index(['0', '1', '10', '11', '12', '13', '14', '15', '2', '3', '4', '5', '6',\n",
            "       '7', '8', '9', 'lines new ZA0 4 <-> ZA2 0 AC',\n",
            "       'lines new ZA0 0 <-> ZA1 0 AC', 'lines new ZA0 0 <-> ZA3 0 AC'],\n",
            "      dtype='object', name='Line')\n",
            "WARNING:pypsa.consistency:The following sub_networks have carriers which are not defined:\n",
            "Index(['0'], dtype='object', name='SubNetwork')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator-fix-p-lower\n",
            "Generator-fix-p-upper\n",
            "Line-fix-s-lower\n",
            "Line-fix-s-upper\n",
            "StorageUnit-fix-p_dispatch-lower\n",
            "StorageUnit-fix-p_dispatch-upper\n",
            "StorageUnit-fix-p_store-lower\n",
            "StorageUnit-fix-p_store-upper\n",
            "StorageUnit-fix-state_of_charge-lower\n",
            "StorageUnit-fix-state_of_charge-upper\n",
            "Bus-nodal_balance\n",
            "Kirchhoff-Voltage-Law\n",
            "StorageUnit-energy_balance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/linopy/expressions.py:1861: FutureWarning:\n",
            "\n",
            "In a future version of xarray the default value for join will change from join='outer' to join='exact'. This change will result in the following ValueError: cannot be aligned with join='exact' because index/labels/sizes are not equal along these coordinates (dimensions): '_term' ('_term',) The recommendation is to set join explicitly for this case.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp_model.constraints[\"Line-fix-s-lower\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7R33FyMdoMu",
        "outputId": "3bde81d7-8d47-40c0-ea80-832f5b50555a"
      },
      "id": "c7R33FyMdoMu",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constraint `Line-fix-s-lower` [snapshot: 8760, Line-fix: 19]:\n",
            "-------------------------------------------------------------\n",
            "[2013-01-01 00:00:00, 0]: +1 Line-s[2013-01-01 00:00:00, 0]                                                       â‰¥ -29174.855194304404\n",
            "[2013-01-01 00:00:00, 1]: +1 Line-s[2013-01-01 00:00:00, 1]                                                       â‰¥ -36991.936506719416\n",
            "[2013-01-01 00:00:00, 10]: +1 Line-s[2013-01-01 00:00:00, 10]                                                     â‰¥ -63094.68609260425\n",
            "[2013-01-01 00:00:00, 11]: +1 Line-s[2013-01-01 00:00:00, 11]                                                     â‰¥ -11575.364829015109\n",
            "[2013-01-01 00:00:00, 12]: +1 Line-s[2013-01-01 00:00:00, 12]                                                     â‰¥ -8126.761604503482\n",
            "[2013-01-01 00:00:00, 13]: +1 Line-s[2013-01-01 00:00:00, 13]                                                     â‰¥ -34411.831188520024\n",
            "[2013-01-01 00:00:00, 14]: +1 Line-s[2013-01-01 00:00:00, 14]                                                     â‰¥ -4476.287858369701\n",
            "\t\t...\n",
            "[2013-12-31 23:00:00, 6]: +1 Line-s[2013-12-31 23:00:00, 6]                                                       â‰¥ -8539.66866062144\n",
            "[2013-12-31 23:00:00, 7]: +1 Line-s[2013-12-31 23:00:00, 7]                                                       â‰¥ -4889.19491448766\n",
            "[2013-12-31 23:00:00, 8]: +1 Line-s[2013-12-31 23:00:00, 8]                                                       â‰¥ -5973.075936797304\n",
            "[2013-12-31 23:00:00, 9]: +1 Line-s[2013-12-31 23:00:00, 9]                                                       â‰¥ -34250.171092421\n",
            "[2013-12-31 23:00:00, lines new ZA0 4 <-> ZA2 0 AC]: +1 Line-s[2013-12-31 23:00:00, lines new ZA0 4 <-> ZA2 0 AC] â‰¥ -309.67999999999995\n",
            "[2013-12-31 23:00:00, lines new ZA0 0 <-> ZA1 0 AC]: +1 Line-s[2013-12-31 23:00:00, lines new ZA0 0 <-> ZA1 0 AC] â‰¥ -309.67999999999995\n",
            "[2013-12-31 23:00:00, lines new ZA0 0 <-> ZA3 0 AC]: +1 Line-s[2013-12-31 23:00:00, lines new ZA0 0 <-> ZA3 0 AC] â‰¥ -309.67999999999995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(temp_model.constraints[\"Line-fix-s-upper\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNrUr8MXdxs-",
        "outputId": "51809e82-1ab8-4e28-8538-d7affd6306d7"
      },
      "id": "ZNrUr8MXdxs-",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constraint `Line-fix-s-upper` [snapshot: 8760, Line-fix: 19]:\n",
            "-------------------------------------------------------------\n",
            "[2013-01-01 00:00:00, 0]: +1 Line-s[2013-01-01 00:00:00, 0]                                                       â‰¤ 29174.855194304404\n",
            "[2013-01-01 00:00:00, 1]: +1 Line-s[2013-01-01 00:00:00, 1]                                                       â‰¤ 36991.936506719416\n",
            "[2013-01-01 00:00:00, 10]: +1 Line-s[2013-01-01 00:00:00, 10]                                                     â‰¤ 63094.68609260425\n",
            "[2013-01-01 00:00:00, 11]: +1 Line-s[2013-01-01 00:00:00, 11]                                                     â‰¤ 11575.364829015109\n",
            "[2013-01-01 00:00:00, 12]: +1 Line-s[2013-01-01 00:00:00, 12]                                                     â‰¤ 8126.761604503482\n",
            "[2013-01-01 00:00:00, 13]: +1 Line-s[2013-01-01 00:00:00, 13]                                                     â‰¤ 34411.831188520024\n",
            "[2013-01-01 00:00:00, 14]: +1 Line-s[2013-01-01 00:00:00, 14]                                                     â‰¤ 4476.287858369701\n",
            "\t\t...\n",
            "[2013-12-31 23:00:00, 6]: +1 Line-s[2013-12-31 23:00:00, 6]                                                       â‰¤ 8539.66866062144\n",
            "[2013-12-31 23:00:00, 7]: +1 Line-s[2013-12-31 23:00:00, 7]                                                       â‰¤ 4889.19491448766\n",
            "[2013-12-31 23:00:00, 8]: +1 Line-s[2013-12-31 23:00:00, 8]                                                       â‰¤ 5973.075936797304\n",
            "[2013-12-31 23:00:00, 9]: +1 Line-s[2013-12-31 23:00:00, 9]                                                       â‰¤ 34250.171092421\n",
            "[2013-12-31 23:00:00, lines new ZA0 4 <-> ZA2 0 AC]: +1 Line-s[2013-12-31 23:00:00, lines new ZA0 4 <-> ZA2 0 AC] â‰¤ 309.67999999999995\n",
            "[2013-12-31 23:00:00, lines new ZA0 0 <-> ZA1 0 AC]: +1 Line-s[2013-12-31 23:00:00, lines new ZA0 0 <-> ZA1 0 AC] â‰¤ 309.67999999999995\n",
            "[2013-12-31 23:00:00, lines new ZA0 0 <-> ZA3 0 AC]: +1 Line-s[2013-12-31 23:00:00, lines new ZA0 0 <-> ZA3 0 AC] â‰¤ 309.67999999999995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(network.lines.s_nom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3znPMIYqd9Bq",
        "outputId": "7e3a7f45-9707-4704-c9ce-601f6044915f"
      },
      "id": "3znPMIYqd9Bq",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Line\n",
            "0                               41678.364563\n",
            "1                               52845.623581\n",
            "10                              90135.265847\n",
            "11                              16536.235470\n",
            "12                              11609.659435\n",
            "13                              49159.758841\n",
            "14                               6394.696941\n",
            "15                               5362.429300\n",
            "2                                 294.933612\n",
            "3                               17414.489153\n",
            "4                               12954.735451\n",
            "5                                 516.133820\n",
            "6                               12199.526658\n",
            "7                                6984.564164\n",
            "8                                8532.965624\n",
            "9                               48928.815846\n",
            "lines new ZA0 4 <-> ZA2 0 AC      442.400000\n",
            "lines new ZA0 0 <-> ZA1 0 AC      442.400000\n",
            "lines new ZA0 0 <-> ZA3 0 AC      442.400000\n",
            "Name: s_nom, dtype: float64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypsa\n",
    "import numpy as np\n",
    "import gc\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from google.colab import drive\n",
    "\n",
    "def _variable_constraint_mapping(network_file, constraints_to_skip):\n",
    "    \"\"\"\n",
    "    Initialize all optimization components in one pass to avoid creating multiple models.\n",
    "    This method:\n",
    "    1. Creates the optimization model once\n",
    "    2. Extracts objective components (vars, coeffs, const)\n",
    "    3. Creates the variable ID to name mapping\n",
    "    4. Extracts constraints\n",
    "    5. Cleans up the model\n",
    "    \"\"\"\n",
    "    network = pypsa.Network(network_file)\n",
    "    # Create model once - this is an expensive operation\n",
    "    temp_model = network.optimize.create_model()\n",
    "\n",
    "    # Create variable ID to name mapping\n",
    "    var_id_to_name = {}\n",
    "    for var_name, variable in temp_model.variables.items():\n",
    "        # Get the variable labels (IDs) for this variable\n",
    "        var_labels = variable.labels\n",
    "\n",
    "        if hasattr(var_labels, 'values'):\n",
    "            # Multi-dimensional variable\n",
    "            labels_flat = var_labels.values.flatten()\n",
    "            coords = variable.labels.coords\n",
    "            for i, label in enumerate(labels_flat):\n",
    "                if label != -1:  # -1 means no variable\n",
    "                    # Create a name that includes the index for multi-dim variables\n",
    "                    if len(coords) > 0:\n",
    "                        # Get the coordinate values for this flat index\n",
    "                        unravel_idx = np.unravel_index(i, var_labels.shape)\n",
    "                        coord_values = []\n",
    "                        for dim_idx, dim_name in enumerate(var_labels.dims):\n",
    "                            coord_val = coords[dim_name].values[unravel_idx[dim_idx]]\n",
    "\n",
    "                            # Handle datetime64 values properly\n",
    "                            if isinstance(coord_val, np.datetime64) or hasattr(coord_val, 'strftime'):\n",
    "                                # Convert datetime to string in ISO format\n",
    "                                try:\n",
    "                                    coord_val = pd.Timestamp(coord_val).isoformat()\n",
    "                                except:\n",
    "                                    # Fallback if conversion fails\n",
    "                                    coord_val = str(coord_val)\n",
    "\n",
    "                            coord_values.append(f\"{dim_name}={coord_val}\")\n",
    "\n",
    "                        full_name = f\"{var_name}[{','.join(coord_values)}]\"\n",
    "                    else:\n",
    "                        full_name = f\"{var_name}[{i}]\"\n",
    "                    var_id_to_name[label] = full_name\n",
    "        else:\n",
    "            # Scalar variable\n",
    "            var_id_to_name[var_labels] = var_name\n",
    "\n",
    "        # Store constraint information\n",
    "        constraints = {}\n",
    "        for name, constraint_group in temp_model.constraints.items():\n",
    "            # Corrected condition to skip desired constraints\n",
    "            if name in constraints_to_skip:\n",
    "              continue\n",
    "            # Check if this is a constraint group with multiple individual constraints\n",
    "            if hasattr(constraint_group.lhs, 'shape') and len(constraint_group.lhs.shape) > 0:\n",
    "                # This is a constraint group with multiple individual constraints\n",
    "                # We need to extract each individual constraint\n",
    "\n",
    "                # Get the dimensions of the constraint group\n",
    "                dims = constraint_group.lhs.dims if hasattr(constraint_group.lhs, 'dims') else []\n",
    "\n",
    "                # If it has dimensions, iterate through each individual constraint\n",
    "                if dims:\n",
    "                    # Get coordinate values for each dimension\n",
    "                    coords = {}\n",
    "                    for dim in dims:\n",
    "                        if hasattr(constraint_group.lhs, 'coords') and dim in constraint_group.lhs.coords:\n",
    "                            coords[dim] = constraint_group.lhs.coords[dim].values\n",
    "\n",
    "                    # Create a flat iterator through all combinations of coordinates\n",
    "                    if coords:\n",
    "                        try:\n",
    "                            # Create all combinations of coordinate indices - only use dimensions that exist in coords\n",
    "                            valid_dims = [dim for dim in dims if dim in coords]\n",
    "                            if not valid_dims:\n",
    "                                # No valid dimensions found, skip this constraint group\n",
    "                                print(f\"Warning: No valid dimensions found for constraint {name}\")\n",
    "                                continue\n",
    "\n",
    "                            # Create shape tuple for ndindex\n",
    "                            shape_tuple = tuple(len(coords[dim]) for dim in valid_dims)\n",
    "                            if not shape_tuple:\n",
    "                                # Empty shape tuple, skip this constraint group\n",
    "                                print(f\"Warning: Empty shape tuple for constraint {name}\")\n",
    "                                continue\n",
    "\n",
    "                            # Create iterator\n",
    "                            indices = np.ndindex(shape_tuple)\n",
    "\n",
    "                            # Iterate through all combinations\n",
    "                            for idx in indices:\n",
    "                                try:\n",
    "                                    # Create a key for this specific constraint\n",
    "                                    coord_values = []\n",
    "                                    for i, dim in enumerate(valid_dims):\n",
    "                                        coord_values.append(f\"{dim}={coords[dim][idx[i]]}\")\n",
    "\n",
    "                                    specific_key = f\"{name}[{','.join(coord_values)}]\"\n",
    "\n",
    "                                    # Extract the specific constraint values - with error handling\n",
    "                                    try:\n",
    "                                        # For LHS\n",
    "                                        if hasattr(constraint_group.lhs.vars, '__getitem__') and hasattr(constraint_group.lhs, 'coeffs'):\n",
    "                                            # Create a proper index for this specific constraint\n",
    "                                            # We need to map our valid_dims indices to the full dims indices\n",
    "\n",
    "                                            # For linear expressions - safely get values\n",
    "                                            try:\n",
    "                                                if hasattr(constraint_group.lhs.vars, '__getitem__'):\n",
    "                                                    lhs_vars = constraint_group.lhs.vars[idx]\n",
    "                                                    #condition is evaluating to true and type(constraint_group.lhs.vars) is <class 'xarray.core.dataarray.DataArray'>\n",
    "                                                else:\n",
    "                                                    lhs_vars = constraint_group.lhs.vars\n",
    "\n",
    "                                                if hasattr(constraint_group.lhs.coeffs, '__getitem__'):\n",
    "                                                    lhs_coeffs = constraint_group.lhs.coeffs[idx]\n",
    "                                                else:\n",
    "                                                    lhs_coeffs = constraint_group.lhs.coeffs\n",
    "                                            except Exception as e:\n",
    "                                                print(f\"Warning: Error accessing constraint values for {specific_key}: {e}\")\n",
    "                                                continue\n",
    "\n",
    "                                            # Create a new linear expression for this specific constraint\n",
    "                                            specific_lhs = type('LinearExpr', (), {\n",
    "                                                'vars': np.array([[lhs_vars]]) if np.isscalar(lhs_vars) else np.array([lhs_vars]),\n",
    "                                                'coeffs': np.array([[lhs_coeffs]]) if np.isscalar(lhs_coeffs) else np.array([lhs_coeffs]),\n",
    "                                                'copy': lambda self: self\n",
    "                                            })\n",
    "\n",
    "                                            # Add constant if it exists - safely\n",
    "                                            if hasattr(constraint_group.lhs, 'const'):\n",
    "                                                try:\n",
    "                                                    if hasattr(constraint_group.lhs.const, '__getitem__'):\n",
    "                                                        const_val = constraint_group.lhs.const[idx]\n",
    "                                                    else:\n",
    "                                                        const_val = constraint_group.lhs.const\n",
    "                                                    specific_lhs.const = np.array([[const_val]]) if np.isscalar(const_val) else np.array([const_val])\n",
    "                                                except Exception as e:\n",
    "                                                    # If error accessing const, just use 0\n",
    "                                                    specific_lhs.const = 0\n",
    "                                                    print(f\"Warning: Error accessing const for {specific_key}: {e}\")\n",
    "                                        # else:\n",
    "                                        #     # For simple values\n",
    "                                        #     try:\n",
    "                                        #         if hasattr(constraint_group.lhs, '__getitem__'):\n",
    "                                        #             specific_lhs = constraint_group.lhs[idx]\n",
    "                                        #         else:\n",
    "                                        #             specific_lhs = constraint_group.lhs\n",
    "                                        #     except Exception as e:\n",
    "                                        #         print(f\"Warning: Error accessing LHS for {specific_key}: {e}\")\n",
    "                                        #         continue\n",
    "\n",
    "                                        # For RHS - safely\n",
    "                                        try:\n",
    "                                            if hasattr(constraint_group.rhs, '__getitem__'):\n",
    "                                                rhs_val = constraint_group.rhs[idx]\n",
    "                                            else:\n",
    "                                                rhs_val = constraint_group.rhs\n",
    "                                            specific_rhs = np.array([[rhs_val]]) if np.isscalar(rhs_val) else np.array([rhs_val])\n",
    "                                        except Exception as e:\n",
    "                                            print(f\"Warning: Error accessing RHS for {specific_key}: {e}\")\n",
    "                                            continue\n",
    "\n",
    "                                        # For sign - safely\n",
    "                                        try:\n",
    "                                            if hasattr(constraint_group.sign, '__getitem__'):\n",
    "                                                sign_val = constraint_group.sign[idx].values.item()\n",
    "                                            else:\n",
    "                                                sign_val = constraint_group.sign\n",
    "                                            specific_sign = np.array([sign_val]) if np.isscalar(sign_val) else np.array([sign_val])\n",
    "                                        except Exception as e:\n",
    "                                            print(f\"Warning: Error accessing sign for {specific_key}: {e}\")\n",
    "                                            specific_sign = np.array(['>=']) # Default sign\n",
    "\n",
    "                                        # Store this specific constraint\n",
    "                                        constraints[specific_key] = {\n",
    "                                            'lhs': specific_lhs,\n",
    "                                            'rhs': specific_rhs,\n",
    "                                            'sign': specific_sign\n",
    "                                        }\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"Warning: Error processing constraint {specific_key}: {e}\")\n",
    "                                        continue\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Warning: Error creating key for constraint: {e}\")\n",
    "                                    continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: Error creating indices for constraint {name}: {e}\")\n",
    "                            continue\n",
    "                else: #no case handling for when no dimensions but still has shape\n",
    "                    print(f\"Warning: No dimensions found for constraint {name}\")\n",
    "                    continue\n",
    "            else:\n",
    "                # This is a single constraint, store it directly\n",
    "                try:\n",
    "                    constraints[name] = {\n",
    "                        'lhs': constraint_group.lhs.copy() if hasattr(constraint_group.lhs, 'copy') else constraint_group.lhs,\n",
    "                        'rhs': constraint_group.rhs.copy() if hasattr(constraint_group.rhs, 'copy') else constraint_group.rhs,\n",
    "                        'sign': constraint_group.sign\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error storing single constraint {name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Clean up to free memory\n",
    "        del temp_model\n",
    "        gc.collect()\n",
    "\n",
    "        return var_id_to_name, constraints\n",
    "\n",
    "def save_mappings(var_id_to_name, constraints, network_file, output_dir=\"var_constraint_map\"):\n",
    "    \"\"\"\n",
    "    Save the variable ID to name mapping and constraints to files in Google Drive.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    var_id_to_name : dict\n",
    "        Mapping from variable IDs to variable names\n",
    "    constraints : dict\n",
    "        Dictionary of constraints\n",
    "    network_file : str\n",
    "        Path to the network file used to create the mappings\n",
    "    output_dir : str, optional\n",
    "        Directory relative to Google Drive MyDrive to save the mapping files to\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (var_map_path, constraints_path) - Paths to the saved mapping files\n",
    "    \"\"\"\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create the full path to Google Drive directory\n",
    "    gdrive_base = '/content/drive/MyDrive/Colab_Notebooks'\n",
    "    full_output_dir = os.path.join(gdrive_base, output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(full_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get the network filename without path or extension\n",
    "    network_name = os.path.basename(network_file)\n",
    "    network_name = os.path.splitext(network_name)[0]\n",
    "    \n",
    "    # Create output filenames\n",
    "    var_map_file = os.path.join(full_output_dir, f\"{network_name}_var_id_to_name.pkl\")\n",
    "    constraints_file = os.path.join(full_output_dir, f\"{network_name}_constraints.pkl\")\n",
    "    \n",
    "    # Save mappings to files\n",
    "    with open(var_map_file, 'wb') as f:\n",
    "        pickle.dump(var_id_to_name, f)\n",
    "        \n",
    "    with open(constraints_file, 'wb') as f:\n",
    "        pickle.dump(constraints, f)\n",
    "    \n",
    "    print(f\"Saved variable mapping to: {var_map_file}\")\n",
    "    print(f\"Saved constraints to: {constraints_file}\")\n",
    "    \n",
    "    return var_map_file, constraints_file\n",
    "\n",
    "def load_mappings(network_file, input_dir=\"var_constraint_map\"):\n",
    "    \"\"\"\n",
    "    Load previously saved variable ID to name mapping and constraints from files in Google Drive.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    network_file : str\n",
    "        Path to the network file used to create the mappings\n",
    "    input_dir : str, optional\n",
    "        Directory relative to Google Drive MyDrive where the mapping files are stored\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (var_id_to_name, constraints) - The loaded mappings\n",
    "    \"\"\"\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create the full path to Google Drive directory\n",
    "    gdrive_base = '/content/drive/MyDrive/Colab_Notebooks'\n",
    "    full_input_dir = os.path.join(gdrive_base, input_dir)\n",
    "    \n",
    "    # Get the network filename without path or extension\n",
    "    network_name = os.path.basename(network_file)\n",
    "    network_name = os.path.splitext(network_name)[0]\n",
    "    \n",
    "    # Create input filenames\n",
    "    var_map_file = os.path.join(full_input_dir, f\"{network_name}_var_id_to_name.pkl\")\n",
    "    constraints_file = os.path.join(full_input_dir, f\"{network_name}_constraints.pkl\")\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(var_map_file) or not os.path.exists(constraints_file):\n",
    "        raise FileNotFoundError(f\"Mapping files for {network_name} not found in {full_input_dir}\")\n",
    "    \n",
    "    # Load mappings from files\n",
    "    with open(var_map_file, 'rb') as f:\n",
    "        var_id_to_name = pickle.load(f)\n",
    "        \n",
    "    with open(constraints_file, 'rb') as f:\n",
    "        constraints = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded variable mapping from: {var_map_file}\")\n",
    "    print(f\"Loaded constraints from: {constraints_file}\")\n",
    "    \n",
    "    return var_id_to_name, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Example usage\n",
    "    network_file = \"networks/elec_s_5_ec_lc1.0_3h.nc\"\n",
    "    constraints_to_skip = [\n",
    "        \"StorageUnit-fix-p_dispatch-lower\", \n",
    "        \"StorageUnit-fix-p_dispatch-upper\", \n",
    "        \"StorageUnit-fix-p_store-lower\", \n",
    "        \"StorageUnit-fix-p_store-upper\", \n",
    "        \"StorageUnit-fix-state_of_charge-lower\", \n",
    "        \"StorageUnit-fix-state_of_charge-upper\",\n",
    "        \"StorageUnit-energy_balance\"\n",
    "    ]\n",
    "    \n",
    "    # Create mappings\n",
    "    var_id_to_name, constraints = _variable_constraint_mapping(network_file, constraints_to_skip)\n",
    "    \n",
    "    # Save mappings\n",
    "    var_map_file, constraints_file = save_mappings(var_id_to_name, constraints, network_file)\n",
    "    \n",
    "    print(f\"Created variable ID to name mapping with {len(var_id_to_name)} entries\")\n",
    "    print(f\"Created constraints mapping with {len(constraints)} entries\")\n",
    "    print(f\"Saved variable mapping to {var_map_file}\")\n",
    "    print(f\"Saved constraints mapping to {constraints_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

<!-- Generated by SpecStory -->

# Help with a task or issue (2025-08-29 09:50Z)

_**User**_

help with this:


---

_**Assistant**_

I see you're trying to use `git log -L` to find changes to a specific function in a Jupyter notebook, but you're getting an error. This command doesn't work well with Jupyter notebooks because they're JSON files rather than plain text files.

Let me help you find what you're looking for. First, I'll examine the notebook to locate the `Env2Gen1LoadConstr.get_variable_value` function.

---

Read file: undefined

---

The notebook is too large to read at once. Let me search for the specific function you're looking for:

---



---

Let me search more broadly for the class name:

---



---

Now let me search specifically for the `get_variable_value` function:

---



---

Now let me read the relevant section of the notebook to get the implementation of the `get_variable_value` function:

---

Read file: undefined

---

Read file: undefined

---

Let me search for the actual `get_variable_value` function definition:

---



---

Let me get the full implementation of the first `get_variable_value` function:

---

Read file: undefined

---

Let me get the full implementation of the `get_variable_value` function:

---

Read file: undefined

---

Read file: undefined

---

Let me search for the actual implementation of the `get_variable_value` function:

---



---

Let me get the full implementation of the first `get_variable_value` function:

---

Read file: undefined

---

Let me continue searching for the full implementation:

---

Read file: undefined

---

_**User**_

i want the history of the function

---

_**Assistant**_

```bash
cd /Users/antoniagrindrod/Documents/pypsa-earth_project/pypsa-earth-RL && git log -p -S "def get_variable_value" -- RL/PPO_replacement_reward.ipynb
```

```


                            
+        "\n",
+        "        # mem_mb = psutil.Process().memory_info().rss / 1024 / 1024\n"
,
+        "\n",
+        "        # # Get ALL objects with \"Network\" in their type name\n",
+        "        # network_objects = [obj for obj in gc.get_objects() if 'netwo
rk' in str(type(obj)).lower()]\n",
                                                        
+        "\n",
+        "        # print(f\"\\n=== EPISODE {self._episode_counter} OBJECT ANALY
SIS ===\")\n",
+        "        # print(f\"Memory: {mem_mb:.1f}MB\")\n",
+        "        # print(f\"Total objects with 'network' in type: {len(network_
objects)}\")\n",
+        "\n",
+        "        # # Count by exact type\n",
+        "        # type_counts = {}\n",
+        "        # for obj in network_objects:\n",
+        "        #     obj_type = str(type(obj))\n",
+        "        #     type_counts[obj_type] = type_counts.get(obj_type, 0) + 1
\n",
+        "\n",
+        "        # # Print breakdown\n",
+        "        # for obj_type, count in type_counts.items():\n",
+        "        #     print(f\"  {obj_type}: {count}\")\n",
+        "\n",
+        "        # # Show actual PyPSA Network objects specifically\n",
+        "        # actual_networks = [obj for obj in gc.get_objects() if type(o
bj).__name__ == 'Network' and 'pypsa' in str(type(obj))]\n",
                                                        
+        "        # print(f\"Actual PyPSA Network objects: {len(actual_networks)
}\")\n",
+        "\n",
+        "        # if len(actual_networks) <= 5:  # Only print if reasonable nu
mber\n",
+        "        #     for i, net in enumerate(actual_networks):\n",
+        "        #         print(f\"  Network {i+1}: {id(net)} - {type(net)}\")
\n",
+        "\n",
+        "        # network_id = id(self.env.network) if hasattr(self.env, 'netw
ork') else None\n",
+        "        # print(f\"Current env.network ID: {network_id}\")\n",
+        "        # print(\"=\" * 50)\n",
+        "        # # === END ANALYSIS ===\n",
+        "\n",
+        "        # Reset environment with seed\n",
+        "        if self.seed is not None:\n",
+        "            state, _ = self.env.reset(seed=self.seed)\n",
+        "        else:\n",
+        "            state, _ = self.env.reset()\n",
+        "\n",
+        "        states, actions, actions_log_probability, values, rewards, don
e, episode_reward = self.init_training()\n",
                                                        
+        "\n",
+        "        # Add this line to track violations\n",
+        "        total_violations = 0\n",
+        "\n",
+        "        # # Create fresh network for each episode to avoid memory corr
uption\n",
+        "        # fresh_network = create_pypsa_network()\n",
+        "        # self.env.network = fresh_network\n",
+        "\n",
+        "        state, _ = self.env.reset()  # Gymnasium format returns (obs,
info)\n",
+        "\n",
+        "        self.model.train() # Set model to training mode\n",
+        "\n",
+        "        while True:\n",
+        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(se
lf.device)\n",
+        "            states.append(state_tensor)\n",
+        "\n",
+        "            # Get action predictions and values\n",
+        "            action_mean, value_pred = self.model(state_tensor)\n",
+        "\n",
+        "\n",
+        "\n",
+        "            # Split actor output into alpha and beta parameters\n",
+        "            action_dim = self.env.action_space.shape[0]\n",
+        "            alpha_raw, beta_raw = torch.split(action_mean, action_dim,
 dim=-1)\n",
+        "\n",
+        "            # Ensure alpha, beta > 1 for well-behaved Beta distributio
n\n",
+        "            alpha = torch.nn.functional.softplus(alpha_raw) + 1.0\n",
+        "            beta = torch.nn.functional.softplus(beta_raw) + 1.0\n",
+        "\n",
+        "            # Create Beta distribution for continuous actions in [0,1]
\n",
+        "            dist = torch.distributions.Beta(alpha, beta)\n",
+        "            action = dist.sample()\n",
+        "\n",
+        "            # No clamping needed - Beta distribution naturally outputs
 [0,1]\n",
+        "            action_clamped = action\n",
+        "\n",
+        "            log_prob_action = dist.log_prob(action).sum(dim=-1)  # Sum
 over action dimensions\n",
+        "\n",
+        "            # Step environment with numpy action\n",
+        "            action_np = action_clamped.detach().cpu().numpy().flatten(
)\n",
+        "            state, reward, terminated, truncated, info = self.env.step
(action_np)\n",
+        "            done = terminated or truncated\n",
+        "\n",
+        "            #accumulate violations for the epsiode\n",
+        "            total_violations += sum(info['constraint_violations'].valu
es())\n",
+        "\n",
+        "            actions.append(action_clamped)\n",
+        "            actions_log_probability.append(log_prob_action)\n",
+        "            values.append(value_pred)\n",
+        "            rewards.append(reward)\n",
+        "            episode_reward += reward\n",
+        "\n",
+        "            if done:\n",
+        "                break\n",
+        "\n",
+        "        states=torch.cat(states).to(self.device)#converts the list of
individual states into a sinlem tensor that is necessary for later processing\n"
,                                                       
+        "        #Creates a single tensor with dimensions like (N, state_dim),
where: N is the number of states collected in the episode; state_dim is the dime
nsionality of each state.\n",                           
+        "        #torch.cat() expects a sequence (e.g. list or tuple) of PyTorc
h tensors as input.\n",
+        "        actions=torch.cat(actions).to(self.device)\n",
+        "        #Note that, in the loop, both state and action are PyTorch ten
sors so that states and actions are both lists of PyTorch tensors\n",
                                                        
+        "        actions_log_probability=torch.cat(actions_log_probability).to(
self.device)\n",
+        "        values=torch.cat(values).squeeze(-1).to(self.device)# .squeeze
 removes a dimension of size 1 only from tensor at the specified position, in th
is case, -1, the last dimesion in the tensor. Note that .squeeze() does not do a
nything if the size of the dimension at the specified potision is not 1.\n",
    
+        "        # print(f\"rewards NaNs: {torch.isnan(torch.tensor(rewards, dt
ype=torch.float32)).any()}\")\n",
                                                        
+        "        # print(f\"values NaNs: {torch.isnan(torch.tensor(values, dtyp
e=torch.float32)).any()}\")\n",
                                                        
+        "        returns = self.calculate_returns(rewards)\n",
+        "        advantages = self.calculate_advantages(returns, values)\n",
+        "\n",
+        "        # print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
+        "        # print(f\"advantages NaNs (after calculation): {torch.isnan(a
dvantages).any()}\")\n",
+        "\n",
+        "        return episode_reward, states, actions, actions_log_probabilit
y, advantages, returns, total_violations\n",
                                                        
+        "\n",
+        "\n",
+        "    def update_policy(self,\n",
+        "            states,\n",
+        "            actions,\n",
+        "            actions_log_probability_old,\n",
+        "            advantages,\n",
+        "            returns):\n",
+        "        #print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
+        "        total_policy_loss = 0\n",
+        "        total_value_loss = 0\n",
+        "        actions_log_probability_old = actions_log_probability_old.deta
ch()\n",
+        "        actions=actions.detach()\n",
+        "\n",
+        "        # print(f\"Returns NaNs: {torch.isnan(returns).any()}\")\n",
+        "        # print(f\"advantages NaNs (after calculation): {torch.isnan(a
dvantages).any()}\")\n",
+        "\n",
+        "\n",
+        "        #detach() is used to remove the tensor from the computation gr
aph, meaning no gradients will be calculated for that tensor when performing bac
kpropagation.\n",                                       
+        "        #In this context, it's used to ensure that the old actions and
 log probabilities do not participate in the gradient computation during the opt
imization of the policy, as we want to update the model based on the current pol
icy rather than the old one.\n",
    
+        "        #print(type(states), type(actions),type(actions_log_probabilit
y_old), type(advantages), type(returns))\n",
                                                        
+        "        training_results_dataset= TensorDataset(\n",
+        "                states,\n",
+        "                actions,\n",
+        "                actions_log_probability_old,\n",
+        "                advantages,\n",
+        "                returns) #TensorDataset class expects all the argument
s passed to it to be tensors (or other compatible types like NumPy arrays, which
 will be automatically converted to tensor\n",          
+        "        batch_dataset = DataLoader(\n",
+        "                training_results_dataset,\n",
+        "                batch_size=self.batch_size,\n",
+        "                shuffle=False)\n",
+        "        #creates a DataLoader instance in PyTorch, which is used to lo
ad the training_results_dataset in batches during training.\n",
                                                        
+        "        #batch_size defines how many samples will be included in each
batch. The dataset will be divided into batches of size BATCH_SIZE. The model wi
ll then process one batch at a time, rather than all of the data at once,\n",

    
+        "        #shuffle argument controls whether or not the data will be shu
ffled before being split into batches.\n",
                                                        
+        "        #Because shuffle is false, dataloader will provide the batches
 in the order the data appears in training_results_dataset. In this case, the ba
tches will be formed from consecutive entries in the dataset, and the observatio
ns will appear in the same sequence as they are stored in the dataset.\n",
    
+        "        for _ in range(self.PPO_steps):\n",
+        "            for batch_idx, (states,actions,actions_log_probability_old
, advantages, returns) in enumerate(batch_dataset):\n",
                                                        
+        "                #get new log prob of actions for all input states\n",
+        "                action_mean, value_pred = self.model(states)\n",
+        "                value_pred = value_pred.squeeze(-1)\n",
+        "\n",
+        "                # For continuous actions with Beta distribution\n",
+        "                action_dim = self.env.action_space.shape[0]\n",
+        "                alpha_raw, beta_raw = torch.split(action_mean, action_
dim, dim=-1)\n",
+        "\n",
+        "                # Ensure alpha, beta > 1 for well-behaved Beta distrib
ution\n",
+        "                alpha = torch.nn.functional.softplus(alpha_raw) + 1.0\
n",
+        "                beta = torch.nn.functional.softplus(beta_raw) + 1.0\n"
,
+        "\n",
+        "                probability_distribution_new = torch.distributions.Bet
a(alpha, beta)\n",
+        "                entropy = probability_distribution_new.entropy().sum(d
im=-1)\n",
+        "\n",
+        "                #estimate new log probabilities using old actions\n",
+        "                actions_log_probability_new = probability_distribution
_new.log_prob(actions).sum(dim=-1)\n",
                                                        
+        "                # # Check for NaN or Inf in log probabilities\n",
+        "                # if torch.isnan(actions_log_probability_old).any() or
 torch.isinf(actions_log_probability_old).any():\n",
                                                        
+        "                #     print(\"NaN or Inf detected in actions_log_proba
bility_old!\")\n",
+        "                #     return  # You can return or handle this case as
needed\n",
+        "\n",
+        "                # if torch.isnan(actions_log_probability_new).any() or
 torch.isinf(actions_log_probability_new).any():\n",
                                                        
+        "                #     print(\"NaN or Inf detected in actions_log_proba
bility_new!\")\n",
+        "                #     return  # You can return or handle this case as
needed\n",
+        "\n",
+        "                # print(f\"actions_log_probability_old NaNs: {torch.is
nan(actions_log_probability_old).any()}\")\n",
                                                        
+        "                # print(f\"actions_log_probability_new NaNs: {torch.is
nan(actions_log_probability_new).any()}\")\n",
                                                        
+        "                # print(f\"advantages NaNs: {torch.isnan(advantages).a
ny()}\")\n",
+        "\n",
+        "                surrogate_loss = self.calculate_surrogate_loss(\n",
+        "                    actions_log_probability_old,\n",
+        "                    actions_log_probability_new,\n",
+        "                    advantages\n",
+        "                )\n",
+        "\n",
+        "                # print(f\"Surrogate Loss NaNs: {torch.isnan(surrogate
_loss).any()}\")\n",
+        "                # print(f\"Entropy NaNs: {torch.isnan(entropy).any()}\
")\n",
+        "                # print(f\"Returns NaNs: {torch.isnan(returns).any()}\
")\n",
+        "                # print(f\"Value Predictions NaNs: {torch.isnan(value_
pred).any()}\")\n",
+        "\n",
+        "                policy_loss, value_loss = self.calculate_losses(\n",
+        "                    surrogate_loss,\n",
+        "                    entropy,\n",
+        "                    returns,\n",
+        "                    value_pred\n",
+        "                )\n",
+        "                self.optimizer.zero_grad() #clear existing gradietns i
n the optimizer (so that these don't propagate accross multiple .backward(). Ens
ures each optimization step uses only the gradients computed during the current
batch.\n",
    
+        "\n",
+        "                # Skip backward pass if loss is NaN\n",
+        "                if torch.isnan(policy_loss).any():\n",
+        "                    print(\"NaN detected in policy_loss - skipping bac
kward pass!\")\n",
+        "                    continue\n",
+        "                if torch.isnan(value_loss).any():\n",
+        "                    print(\"NaN detected in value_loss - skipping back
ward pass!\")\n",
+        "                    continue\n",
+        "\n",
+        "                policy_loss.backward() #computes gradients for policy_
loss with respect to the agent's parameters\n",
                                                        
+        "                # #Check for NaN gradients after policy_loss backward\
n",
+        "                # for param in self.model.parameters():\n",
+        "                #     if param.grad is not None:  # Check if gradients
 exist for this parameter\n",
                                                        
+        "                #         if torch.isnan(param.grad).any():\n",
+        "                #             print(\"NaN gradient detected in policy_
loss!\")\n",
+        "                # #             return\n",
+        "                value_loss.backward()\n",
+        "                # Check for NaN gradients after value_loss backwardor
param in self.model.parameters():\n",
                                                        
+        "                # for param in self.model.parameters():\n",
+        "                #     if param.grad is not None:  # Check if gradients
 exist for this parameter\n",
                                                        
+        "                #         if torch.isnan(param.grad).any():\n",
+        "                #             print(\"NaN gradient detected in value_l
oss!\")\n",
+        "                #             return\n",
+        "\n",
+        "                self.optimizer.step()\n",
+        "                #The update step is based on the learning rate and oth
er hyperparameters of the optimizer\n",
                                                        
+        "                # The parameters of the agent are adjusted to reduce t
he policy and value losses.\n",
                                                        
+        "                total_policy_loss += policy_loss.item() #accumulate th
e scalar value of the policy loss for logging/ analysis\n",
                                                        
+        "                #policy_loss.item() extracts the numerical value of th
e loss tensor (detaching it from the computational graph).\n",
                                                        
+        "                #This value is added to total_policy_loss to compute t
he cumulative loss over all batches in the current PPO step.\n",
                                                        
+        "                #Result: tracks the total policy loss for the current
training epoch\n",
+        "                # The loss over the whole dataset is the sum of the lo
sses over all batches.\n",
+        "                #The training dataset is split into batches during the
 training process. Each batch represents a subset of the collected training data
 from one episode.\n",                                  
+        "                # Loss calculation is performed for each batch (policy
 loss and value loss)\n",
+        "                # for each batch, gradients are calculated with respec
t to the total loss for that batch and the optimizer then updates the network pa
rameters using these gradients.\n",                     
+        "                # this is because the surrogate loss is only calculate
d over a single batch of data\n",
                                                        
+        "                #look at the formula for surrogate loss.\n",
+        "                # It is written in terms of an expectation ˆ Et[. . .]
 that indicates the empirical average over a finite batch of samples.\n",
                                                        
+        "                # This means you have collected a set of data (time st
eps) from the environment, and you're averaging over these data points. The hat
symbol implies you're approximating the true expectation with a finite sample of
 data from the environment. This empirical average can be computed as the mean o
f values from the sampled transitions\n",
                                
+        "                # the expectation is taken over all the data you've co
llected\n",
+        "                #If you're training with multiple batches (i.e., colle
cting data in chunks), then you can think of the expectation as being computed o
ver each batch.\n",                                     
+        "                #The overall expectation can indeed be seen as the sum
 of expectations computed for each batch, but The expectation of the sum is gene
rally not exactly equal to the sum of the expectations unless the samples are in
dependent, but in practical reinforcement learning algorithms, it's typically a
good enough approximation\n",
                                
+        "                #For samples to be independent, the outcome of one sam
ple must not provide any information about the outcome of another. Specifically,
 in the context of reinforcement learning, this means that the states, actions,
rewards, and subsequent states observed in different time steps or different epi
sodes should be independent of each other.\n",
                                
+        "                total_value_loss += value_loss.item()\n",
+        "                #Notice that we are calculating an empirical average,
which is already an approximation on the true value (the true expectation would
be the average over an infinite amount of data, and the empirical average is the
 average over the finite amount of data that we have collected).\n",
    
+        "                #But furthermore, we are approximating even the empiri
cal average istelf. The empirical average is the average over all our collected
datal, but here we actually batch our data, calculate average over each batch an
d then sum these averages, which is not exaclty equal to the average of the sums
 (but is a decent approximation).\n",
                                
+        "        return total_policy_loss / self.PPO_steps, total_value_loss /
self.PPO_steps\n",
+        "\n",
+        "    def train(self):\n",
+        "        train_rewards = []\n",
+        "        # test_rewards = []\n",
+        "        # policy_losses = []\n",
+        "        # value_losses = []\n",
+        "        #lens = []\n",
+        "\n",
+        "        for episode in range(1, self.max_episodes + 1):\n",
+        "            # Perform a forward pass and collect experience\n",
+        "            train_reward, states, actions, actions_log_probability, ad
vantages, returns, violations = self.forward_pass()\n",
                                                        
+        "\n",
+        "            # Update the policy using the experience collected\n",
+        "            policy_loss, value_loss = self.update_policy(\n",
+        "                states,\n",
+        "                actions,\n",
+        "                actions_log_probability,\n",
+        "                advantages,\n",
+        "                returns)\n",
+        "            # test_reward = self.evaluate()\n",
+        "\n",
+        "            # # Visualize the environment if it supports rendering (cu
rrently this is done once each episode - might want to change to once every mult
iple of episodes)\n",                                   
+        "            # if hasattr(self.env, \"render\") and callable(getattr(se
lf.env, \"render\", None)):\n",
                                                        
+        "            #   self.env.render()\n",
+        "\n",
+        "            # Log the results\n",
+        "            # policy_losses.append(policy_loss)\n",
+        "            # value_losses.append(value_loss)\n",
+        "            train_rewards.append(train_reward)\n",
+        "            # # run these when back online\n",
+        "            # self.run[\"policy_loss\"].log(policy_loss)\n",
+        "            # self.run[\"value_loss\"].log(value_loss)\n",
+        "            self.run[\"train_reward\"].log(train_reward)\n",
+        "            self.run[\"total_violation\"].log(violations)\n",
+        "\n",
+        "            # Calculate the mean of recent rewards and losses for disp
lay\n",
+        "            mean_train_rewards = np.mean(train_rewards[-self.n_trials:
])\n",
+        "            #mean_test_rewards = np.mean(test_rewards[-self.n_trials:]
)\n",
+        "            # mean_abs_policy_loss = np.mean(np.abs(policy_losses[-sel
f.n_trials:]))\n",
+        "            # mean_abs_value_loss = np.mean(np.abs(value_losses[-self.
n_trials:]))\n",
+        "\n",
+        "            # Print results at specified intervals\n",
+        "            if episode % self.print_interval == 0:\n",
+        "                print(f'Episode: {episode:3} | \\\n",
+        "                    Train Rewards: {train_reward:3.1f} \\\n",
+        "                    Violations: {violations}\\\n",
+        "                    Mean Train Rewards: {mean_train_rewards:3.1f}' )\n
",
+        "                    # \\\n",
+        "                    # | Mean Abs Policy Loss: {mean_abs_policy_loss:2.
2f} \\\n",
+        "                    # | Mean Abs Value Loss: {mean_abs_value_loss:2.2f
} ')\n",
+        "\n",
+        "\n",
+        "\n",
+        "                                    # | Mean Test Rewards: {mean_test_
rewards:3.1f} \\\n",
+        "                                    #| \"Episode Len: {np.mean(lens[-s
elf.n_trials:])}\n",
+        "\n",
+        "\n",
+        "\n",
+        "            # # Check if reward threshold is reached\n",
+        "            # if mean_test_rewards >= self.reward_threshold:\n",
+        "            #     print(f'Reached reward threshold in {episode} episod
es')\n",
+        "            #     break\n",
+        "        # Check if the environment has a close method before calling i
t\n",
+        "        # if hasattr(self.env, \"close\") and callable(getattr(self.en
v, \"close\", None)):\n",
+        "        #   self.env.close() #Close environment visualisation after tr
aining is done.\n",
+        "        return train_rewards\n",
+        "\n",
+        "def plot_train_rewards(train_rewards, reward_threshold):\n",
+        "    plt.figure(figsize=(12, 8))\n",
+        "    plt.plot(train_rewards, label='Training Reward')\n",
+        "    plt.xlabel('Episode', fontsize=20)\n",
+        "    plt.ylabel('Training Reward', fontsize=20)\n",
+        "    plt.hlines(reward_threshold, 0, len(train_rewards), color='y')\n",
+        "    plt.legend(loc='lower right')\n",
+        "    plt.grid()\n",
+        "    plt.show()\n",
+        "\n",
+        "def plot_test_rewards(test_rewards, reward_threshold):\n",
+        "    plt.figure(figsize=(12, 8))\n",
+        "    plt.plot(test_rewards, label='Testing Reward')\n",
+        "    plt.xlabel('Episode', fontsize=20)\n",
+        "    plt.ylabel('Testing Reward', fontsize=20)\n",
+        "    plt.hlines(reward_threshold, 0, len(test_rewards), color='y')\n",
+        "    plt.legend(loc='lower right')\n",
+        "    plt.grid()\n",
+        "    plt.show()\n",
+        "\n",
+        "def plot_losses(policy_losses, value_losses):\n",
+        "    plt.figure(figsize=(12, 8))\n",
+        "    plt.plot(value_losses, label='Value Losses')\n",
+        "    plt.plot(policy_losses, label='Policy Losses')\n",
+        "    plt.xlabel('Episode', fontsize=20)\n",
+        "    plt.ylabel('Loss', fontsize=20)\n",
+        "    plt.legend(loc='lower right')\n",
+        "    plt.grid()\n",
+        "    plt.show()"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 17,
+      "id": "acf82e05",
+      "metadata": {
+        "id": "acf82e05"
+      },
+      "outputs": [],
+      "source": [
+        "class Env2Gen1LoadConstrReplacement(Env2Gen1LoadConstr):\n",
+        "    \"\"\"\n",
+        "    Environment using the Replacement reward method instead of Summati
on.\n",
+        "\n",
+        "    Inherits from Env2Gen1LoadConstr but modifies the reward calculati
on\n",
+        "    to implement the replacement method from the RL-OPF paper.\n",
+        "    \"\"\"\n",
+        "\n",
+        "    def __init__(self,network_file, episode_length=None, constraint_pe
nalty_factor=100, offset_k=2500):\n",
                                                        
+        "        super().__init__(network_file, episode_length=episode_length,
constraint_penalty_factor=constraint_penalty_factor)\n",
                                                        
+        "        self.offset_k = offset_k\n",
+        "\n",
+        "    def calculate_constrained_reward(self):\n",
+        "        \"\"\"\n",
+        "        Calculate reward using replacement method with pre-calculated
offset k.\n",
+        "\n",
+        "        Replacement method:\n",
+        "        - If all constraints satisfied: return -J(s) + k\n",
+        "        - If constraints violated: return -P(s)\n",
+        "\n",
+        "        Returns:\n",
+        "        --------\n",
+        "        tuple: (reward, constraint_results)\n",
+        "        \"\"\"\n",
+        "        try:\n",
+        "            # Get base reward from objective function (negative for mi
nimization)\n",
+        "            base_reward = self._calculate_reward()\n",
+        "\n",
+        "            # Evaluate constraints\n",
+        "            constraint_results = self._evaluate_all_constraints()\n",
+        "\n",
+        "            # Apply replacement method\n",
+        "            if constraint_results['all_satisfied']:\n",
+        "                # All constraints satisfied: return optimization rewar
d + offset k\n",
+        "                constrained_reward = base_reward + self.offset_k\n",
+        "            else:\n",
+        "                # Constraints violated: return only penalty (negative)
\n",
+        "                total_violation = float(constraint_results['total_viol
ation'])\n",
+        "                constrained_reward = -self.penalty_factor * total_viol
ation\n",
+        "\n",
+        "            # Ensure reward is a scalar\n",
+        "            if hasattr(constrained_reward, '__len__'):\n",
+        "                constrained_reward = float(constrained_reward)\n",
+        "\n",
+        "            return constrained_reward, constraint_results\n",
+        "\n",
+        "        except Exception as e:\n",
+        "            print(f\"Error calculating replacement reward: {e}\")\n",
+        "            # Fall back to base reward on error\n",
+        "            return self._calculate_reward(), {\n",
+        "                'all_satisfied': True,\n",
+        "                'violations': {},\n",
+        "                'total_violation': 0.0\n",
+        "            }\n",
+        "\n",
+        "    def get_reward_method_info(self):\n",
+        "        \"\"\"\n",
+        "        Get information about the reward method being used.\n",
+        "\n",
+        "        Returns:\n",
+        "        --------\n",
+        "        dict: Information about the reward method\n",
+        "        \"\"\"\n",
+        "        return {\n",
+        "            'method': 'replacement',\n",
+        "            'offset_k': self.offset_k,\n",
+        "            'k_method': self.k_method,\n",
+        "            'k_samples': self.k_samples,\n",
+        "            'penalty_factor': self.penalty_factor\n",
+        "        }"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "source": [
+        "# Add this to your main function or create a new sweep script\n",
+        "\n",
+        "import time\n",
+        "from datetime import datetime\n",
+        "\n",
+        "def run_sweep_replacement():\n",
+        "    # Base parameters (non-swept parameters)\n",
+        "    base_params = {\n",
+        "        \"optimizer_name\": \"Adam\",\n",
+        "        \"MAX_EPISODES\": 100,\n",
+        "        \"PRINT_INTERVAL\": 10,  # Print less frequently during sweep\
n",
+        "        \"N_TRIALS\": 2,\n",
+        "        \"DROPOUT\": 0,\n",
+        "        \"network_file\": \"elec_s_5_ec_lc1.0_3h.nc\",\n",
+        "    }\n",
+        "\n",
+        "    # Parameters to sweep (key parameters that affect learning)\n",
+        "    sweep_params = {\n",
+        "        \"LEARNING_RATE\": [1e-4],\n",
+        "        \"EPSILON\": [0.1],\n",
+        "        \"ENTROPY_COEFFICIENT\": [0.01],\n",
+        "        \"HIDDEN_DIMENSIONS\": [32],\n",
+        "        \"PPO_STEPS\": [2],\n",
+        "        \"BATCH_SIZE\": [64],\n",
+        "        \"DISCOUNT_FACTOR\": [0.99],\n",
+        "        \"constraint_penalty_factor\":[0,100],\n",
+        "        \"episode_length\": [1000],\n",
+        "        \"env_class\": [\"Env2Gen1LoadConstr\", \"Env2Gen1LoadConstrRe
placement\"] #Will need to change this if you change the env class names\n",
                                                        
+        "    }\n",
+        "\n",
+        "    replacement_env=Env2Gen1LoadConstrReplacement(network_file=base_pa
rams[\"network_file\"]) #this is used to calculate the offset k for the replacem
ent reward method\n",                                   
+        "\n",
+        "    # Seeds to use for each configuration\n",
+        "    #seeds = [42, 123, 7]  # Using 3 seeds for statistical significanc
e\n",
+        "    seeds = [42]\n",
+        "    #seeds= [123, 7]\n",
+        "\n",
+        "    # Generate a subset of combinations to keep the sweep manageable\n
",
+        "    # We'll use a more focused approach rather than a full grid search
\n",
+        "\n",
+        "    # Add some random combinations to explore the space more broadly\n
",
+        "    config_seed=42\n",
+        "    import random\n",
+        "    random.seed(config_seed)  # For reproducible random configs\n",
+        "\n",
+        "    #after setting the seed, perform sampling to determine k and store
 the result\n",
+        "\n",
+        "    num_random_configs = 1  # Adjust based on how many total runs you
want\n",
+        "    random_configs = []\n",
+        "\n",
+        "    for _ in range(num_random_configs):\n",
+        "        config = {param: random.choice(values) for param, values in sw
eep_params.items()}\n",
+        "        random_configs.append(config)\n",
+        "\n",
+        "    # Combine priority and random configs\n",
+        "    all_configs = random_configs\n",
+        "\n",
+        "    # Print sweep summary\n",
+        "    print(f\"Running sweep with {len(all_configs)} configurations and
{len(seeds)} seeds\")\n",
+        "    print(f\"Total runs: {len(all_configs) * len(seeds)}\")\n",
+        "\n",
+        "    # Run all configurations\n",
+        "    for seed in seeds:\n",
+        "        # Set all random seeds\n",
+        "        random.seed(seed)\n",
+        "        np.random.seed(seed)\n",
+        "        torch.manual_seed(seed)\n",
+        "        if torch.cuda.is_available():\n",
+        "            torch.cuda.manual_seed(seed)\n",
+        "            torch.cuda.manual_seed_all(seed)\n",
+        "\n",
+        "        #after setting the seed, perform sampling to determine k and s
tore the result\n",
+        "        replacement_reward_offset=calculate_offset_k_initialization(en
vClass=replacement_env)\n",
+        "        for config_idx, config in enumerate(all_configs):\n",
+        "              # Create a unique run ID\n",
+        "            run_id = f\"sweep_{datetime.now().strftime('%Y%m%d')}_{con
fig_idx}_{seed}\"\n",
+        "\n",
+        "            my_api=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuY
WkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZDg3ZjNlYi04M
WI3LTQ1ODctOGIxNS1iNTY3ZjgzMGYzMzYifQ==\"\n",           
+        "\n",
+        "            # Initialize Neptune run\n",
+        "            run = neptune.init_run(\n",
+        "                project='EnergyGridRL/elec-s-5-ec-lc10-3h-GenDispatch'
,\n",
+        "                api_token=my_api,\n",
+        "                name=f\"Sweep-Config{config_idx}-Seed{seed}\",\n",
+        "                tags=[\"hyperparameter_sweep\"]\n",
+        "            )\n",
+        "\n",
+        "            # Combine base params with this config\n",
+        "            params = {**base_params, **config}\n",
+        "            params[\"SEED\"] = seed\n",
+        "\n",
+        "            # Log all parameters\n",
+        "            for key, value in params.items():\n",
+        "                run[f\"parameters/{key}\"] = value\n",
+        "\n",
+        "            run[\"env_class\"]=params[\"env_class\"]\n",
+        "            print(f\"\\n{'='*50}\")\n",
+        "            print(f\"Starting run {config_idx+1}/{len(all_configs)}, s
eed {seed}\")\n",
+        "            print(f\"Parameters: {params}\")\n",
+        "            print(f\"{'='*50}\\n\")\n",
+        "\n",
+        "            try:\n",
+        "\n",
+        "                # Create environment and agent\n",
+        "                if params[\"env_class\"]==\"Env2Gen1LoadConstr\":\n",
+        "                    env = Env2Gen1LoadConstr(network_file=params[\"net
work_file\"], episode_length=params[\"episode_length\"], constraint_penalty_fact
or=params[\"constraint_penalty_factor\"])\n",           
+        "                elif params[\"env_class\"]==\"Env2Gen1LoadConstrReplac
ement\":\n",
+        "                    env = Env2Gen1LoadConstrReplacement(network_file=p
arams[\"network_file\"], episode_length=params[\"episode_length\"], constraint_p
enalty_factor=params[\"constraint_penalty_factor\"], offset_k=replacement_reward
_offset)\n",
    
+        "                env.seed(seed)\n",
+        "\n",
+        "                device = torch.device(\"cuda\" if torch.cuda.is_availa
ble() else \"cpu\")\n",
+        "\n",
+        "                agent = PPO_agent(\n",
+        "                    env=env,\n",
+        "                    run=run,\n",
+        "                    device=device,\n",
+        "                    hidden_dimensions=params[\"HIDDEN_DIMENSIONS\"],\n
",
+        "                    dropout=params[\"DROPOUT\"],\n",
+        "                    discount_factor=params[\"DISCOUNT_FACTOR\"],\n",
+        "                    optimizer_name=params[\"optimizer_name\"],\n",
+        "                    max_episodes=params[\"MAX_EPISODES\"],\n",
+        "                    print_interval=params[\"PRINT_INTERVAL\"],\n",
+        "                    PPO_steps=params[\"PPO_STEPS\"],\n",
+        "                    n_trials=params[\"N_TRIALS\"],\n",
+        "                    epsilon=params[\"EPSILON\"],\n",
+        "                    entropy_coefficient=params[\"ENTROPY_COEFFICIENT\"
],\n",
+        "                    learning_rate=params[\"LEARNING_RATE\"],\n",
+        "                    batch_size=params[\"BATCH_SIZE\"],\n",
+        "                    seed=seed\n",
+        "                )\n",
+        "\n",
+        "                run[\"replacement_reward\"].log(replacement_reward_off
set)\n",
+        "\n",
+        "                # Train the agent\n",
+        "                train_rewards = agent.train()\n",
+        "\n",
+        "                # Log final performance metrics\n",
+        "                run[\"results/final_reward\"] = train_rewards[-1]\n",
+        "                run[\"results/mean_last_100_reward\"] = np.mean(train_
rewards[-100:])\n",
+        "                run[\"results/best_reward\"] = np.max(train_rewards)\n
",
+        "                run[\"results/baseline_reward\"]= evaluate_baseline_re
ward(network_file=params[\"network_file\"], env=env, agent=agent)\n",
                                                        
+        "\n",
+        "            except Exception as e:\n",
+        "                print(f\"Error in run: {e}\")\n",
+        "                run[\"results/error\"] = str(e)\n",
+        "\n",
+        "            # Close the Neptune run\n",
+        "            run.stop()\n",
+        "\n",
+        "            # Small delay to avoid API rate limits\n",
+        "            time.sleep(1)"
+      ],
+      "metadata": {
+        "id": "EvvFGCFapPvW"
+      },
+      "id": "EvvFGCFapPvW",
+      "execution_count": 19,
+      "outputs": []
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 13,
+      "id": "7fb26605",
+      "metadata": {
+        "id": "7fb26605"
+      },
+      "outputs": [],
+      "source": [
+        "# # Add this to your main function or create a new sweep script\n",
+        "\n",
+        "# import time\n",
+        "# from datetime import datetime\n",
+        "\n",
+        "# def run_sweep_replacement():\n",
+        "#     # Base parameters (non-swept parameters)\n",
+        "#     base_params = {\n",
+        "#         \"optimizer_name\": \"Adam\",\n",
+        "#         \"MAX_EPISODES\": 1000,\n",
+        "#         \"PRINT_INTERVAL\": 10,  # Print less frequently during swee
p\n",
+        "#         \"N_TRIALS\": 100,\n",
+        "#         \"DROPOUT\": 0,\n",
+        "#         \"network_file\": \"elec_s_5_ec_lcopt_3h.nc\",\n",
+        "#     }\n",
+        "\n",
+        "#     # Parameters to sweep (key parameters that affect learning)\n",
+        "#     sweep_params = {\n",
+        "#         \"LEARNING_RATE\": [1e-4, 3e-4, 1e-3, 3e-3],\n",
+        "#         \"EPSILON\": [0.1, 0.2, 0.3],\n",
+        "#         \"ENTROPY_COEFFICIENT\": [0.01, 0.05, 0.1],\n",
+        "#         \"HIDDEN_DIMENSIONS\": [32, 64, 128],\n",
+        "#         \"PPO_STEPS\": [8, 16],\n",
+        "#         \"BATCH_SIZE\": [128, 256],\n",
+        "#         \"DISCOUNT_FACTOR\": [0.95, 0.99],\n",
+        "#         \"constraint_penalty_factor\":[0,25,50,100],\n",
+        "#         \"episode_length\": [2,4,6],\n",
+        "#         \"env_class\": [\"Env2Gen1LoadConstr\", \"Env2Gen1LoadConstr
Replacement\"] #Will need to change this if you change the env class names\n",
                                                        
+        "#     }\n",
+        "\n",
+        "#     replacement_env=Env2Gen1LoadConstrReplacement() #this is used to
 calculate the offset k for the replacement reward method\n",
                                                        
+        "\n",
+        "#     # Seeds to use for each configuration\n",
+        "#     #seeds = [42, 123, 7]  # Using 3 seeds for statistical significa
nce\n",
+        "#     #seeds = [42]\n",
+        "#     seeds= [123, 7]\n",
+        "\n",
+        "#     # Generate a subset of combinations to keep the sweep manageable
\n",
+        "#     # We'll use a more focused approach rather than a full grid sear
ch\n",
+        "\n",
+        "#     # Priority configurations based on most promising parameter valu
es\n",
+        "#     # Priority configurations based on most promising parameter valu
es\n",
+        "#     priority_configs = [\n",
+        "#         # Config 1: Higher learning rate, exploration focus\n",
+        "#         {\"LEARNING_RATE\": 1e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFI
CIENT\": 0.1,\n",
+        "#          \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\
": 256,\n",
+        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":0,
 \"episode_length\": 4},\n",
+        "\n",
+        "#         # Config 2: Medium learning rate, balanced approach\n",
+        "#         {\"LEARNING_RATE\": 3e-4, \"EPSILON\": 0.2, \"ENTROPY_COEFFI
CIENT\": 0.05,\n",
+        "#          \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\
": 256,\n",
+        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":0,
\"episode_length\": 4},\n",
+        "\n",
+        "#         # Config 3: Conservative updates, larger network\n",
+        "#         {\"LEARNING_RATE\": 1e-4, \"EPSILON\": 0.1, \"ENTROPY_COEFFI
CIENT\": 0.01,\n",
+        "#          \"HIDDEN_DIMENSIONS\": 128, \"PPO_STEPS\": 16, \"BATCH_SIZE
\": 256,\n",
+        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":0,
\"episode_length\": 4},\n",
+        "\n",
+        "#         # Config 4: Aggressive learning, smaller network\n",
+        "#         {\"LEARNING_RATE\": 3e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFI
CIENT\": 0.1,\n",
+        "#          \"HIDDEN_DIMENSIONS\": 32, \"PPO_STEPS\": 8, \"BATCH_SIZE\"
: 128,\n",
+        "#          \"DISCOUNT_FACTOR\": 0.95, \"constraint_penalty_factor\":0,
\"episode_length\": 4},\n",
+        "\n",
+        "#         # Config 1: Higher learning rate, exploration focus\n",
+        "#         {\"LEARNING_RATE\": 1e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFI
CIENT\": 0.1,\n",
+        "#          \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\
": 256,\n",
+        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":10
0,\"episode_length\": 4},\n",
                                                        
+        "\n",
+        "#         # Config 2: Medium learning rate, balanced approach\n",
+        "#         {\"LEARNING_RATE\": 3e-4, \"EPSILON\": 0.2, \"ENTROPY_COEFFI
CIENT\": 0.05,\n",
+        "#          \"HIDDEN_DIMENSIONS\": 64, \"PPO_STEPS\": 16, \"BATCH_SIZE\
": 256,\n",
+        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":10
0,\"episode_length\": 4},\n",
                                                        
+        "\n",
+        "#         # Config 3: Conservative updates, larger network\n",
+        "#         {\"LEARNING_RATE\": 1e-4, \"EPSILON\": 0.1, \"ENTROPY_COEFFI
CIENT\": 0.01,\n",
+        "#          \"HIDDEN_DIMENSIONS\": 128, \"PPO_STEPS\": 16, \"BATCH_SIZE
\": 256,\n",
+        "#          \"DISCOUNT_FACTOR\": 0.99, \"constraint_penalty_factor\":10
0,\"episode_length\": 4},\n",
                                                        
+        "\n",
+        "#         # Config 4: Aggressive learning, smaller network\n",
+        "#         {\"LEARNING_RATE\": 3e-3, \"EPSILON\": 0.3, \"ENTROPY_COEFFI
CIENT\": 0.1,\n",
+        "#          \"HIDDEN_DIMENSIONS\": 32, \"PPO_STEPS\": 8, \"BATCH_SIZE\"
: 128,\n",
+        "#          \"DISCOUNT_FACTOR\": 0.95, \"constraint_penalty_factor\":10
0,\"episode_length\": 4},\n",
                                                        
+        "#     ]\n",
+        "\n",
+        "#     # Add some random combinations to explore the space more broadly
\n",
+        "#     config_seed=42\n",
+        "#     import random\n",
+        "#     random.seed(config_seed)  # For reproducible random configs\n",
+        "\n",
+        "#     #after setting the seed, perform sampling to determine k and sto
re the result\n",
+        "\n",
+        "#     num_random_configs = 8  # Adjust based on how many total runs yo
u want\n",
+        "#     random_configs = []\n",
+        "\n",
+        "#     for _ in range(num_random_configs):\n",
+        "#         config = {param: random.choice(values) for param, values in
sweep_params.items()}\n",
+        "#         random_configs.append(config)\n",
+        "\n",
+        "#     # Combine priority and random configs\n",
+        "#     all_configs = priority_configs + random_configs\n",
+        "\n",
+        "#     # Print sweep summary\n",
+        "#     print(f\"Running sweep with {len(all_configs)} configurations an
d {len(seeds)} seeds\")\n",
+        "#     print(f\"Total runs: {len(all_configs) * len(seeds)}\")\n",
+        "\n",
+        "#     # Run all configurations\n",
+        "#     for seed in seeds:\n",
+        "#         # Set all random seeds\n",
+        "#         random.seed(seed)\n",
+        "#         np.random.seed(seed)\n",
+        "#         torch.manual_seed(seed)\n",
+        "#         if torch.cuda.is_available():\n",
+        "#             torch.cuda.manual_seed(seed)\n",
+        "#             torch.cuda.manual_seed_all(seed)\n",
+        "\n",
+        "#         #after setting the seed, perform sampling to determine k and
 store the result\n",
+        "#         replacement_reward_offset=calculate_offset_k_initialization(
envClass=replacement_env)\n",
                                                        
+        "#         for config_idx, config in enumerate(all_configs):\n",
+        "#               # Create a unique run ID\n",
+        "#             run_id = f\"sweep_{datetime.now().strftime('%Y%m%d')}_{c
onfig_idx}_{seed}\"\n",
+        "\n",
+        "#             my_api=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmU
uYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZDg3ZjNlYi0
4MWI3LTQ1ODctOGIxNS1iNTY3ZjgzMGYzMzYifQ==\"\n",         
+        "\n",
+        "#             # Initialize Neptune run\n",
+        "#             run = neptune.init_run(\n",
+        "#                 project='EnergyGridRL/PPO-2snapshots-replacement',\n
",
+        "#                 api_token=my_api,\n",
+        "#                 name=f\"Sweep-Config{config_idx}-Seed{seed}\",\n",
+        "#                 tags=[\"hyperparameter_sweep\"]\n",
+        "#             )\n",
+        "\n",
+        "#             # Combine base params with this config\n",
+        "#             params = {**base_params, **config}\n",
+        "#             params[\"SEED\"] = seed\n",
+        "\n",
+        "#             # Log all parameters\n",
+        "#             for key, value in params.items():\n",
+        "#                 run[f\"parameters/{key}\"] = value\n",
+        "\n",
+        "#             run[\"env_class\"]=params[\"env_class\"]\n",
+        "#             print(f\"\\n{'='*50}\")\n",
+        "#             print(f\"Starting run {config_idx+1}/{len(all_configs)},
 seed {seed}\")\n",
+        "#             print(f\"Parameters: {params}\")\n",
+        "#             print(f\"{'='*50}\\n\")\n",
+        "\n",
+        "#             try:\n",
+        "\n",
+        "#                 # Create environment and agent\n",
+        "#                 if params[\"env_class\"]==\"Env2Gen1LoadConstr\":\n"
,
+        "#                     env = Env2Gen1LoadConstr(network_file=params[\"n
etwork_file\"], episode_length=params[\"episode_length\"], constraint_penalty_fa
ctor=params[\"constraint_penalty_factor\"])\n",         
+        "#                 elif params[\"env_class\"]==\"Env2Gen1LoadConstrRepl
acement\":\n",
+        "#                     env = Env2Gen1LoadConstrReplacement(network_file
=params[\"network_file\"], episode_length=params[\"episode_length\"], constraint
_penalty_factor=params[\"constraint_penalty_factor\"], offset_k=replacement_rewa
rd_offset)\n",
    
+        "#                 else:\n",
+        "#                     raise ValueError(f\"Invalid environment class: {
params[\"env_class\"]}\")\n",
                                                        
+        "#                 env.seed(seed)\n",
+        "\n",
+        "#                 device = torch.device(\"cuda\" if torch.cuda.is_avai
lable() else \"cpu\")\n",
+        "\n",
+        "#                 agent = PPO_agent(\n",
+        "#                     env=env,\n",
+        "#                     run=run,\n",
+        "#                     device=device,\n",
+        "#                     hidden_dimensions=params[\"HIDDEN_DIMENSIONS\"],
\n",
+        "#                     dropout=params[\"DROPOUT\"],\n",
+        "#                     discount_factor=params[\"DISCOUNT_FACTOR\"],\n",
+        "#                     optimizer_name=params[\"optimizer_name\"],\n",
+        "#                     max_episodes=params[\"MAX_EPISODES\"],\n",
+        "#                     print_interval=params[\"PRINT_INTERVAL\"],\n",
+        "#                     PPO_steps=params[\"PPO_STEPS\"],\n",
+        "#                     n_trials=params[\"N_TRIALS\"],\n",
+        "#                     epsilon=params[\"EPSILON\"],\n",
+        "#                     entropy_coefficient=params[\"ENTROPY_COEFFICIENT
\"],\n",
+        "#                     learning_rate=params[\"LEARNING_RATE\"],\n",
+        "#                     batch_size=params[\"BATCH_SIZE\"],\n",
+        "#                     seed=seed\n",
+        "#                 )\n",
+        "\n",
+        "#                 run[\"replacement_reward\"].log(replacement_reward_o
ffset)\n",
+        "\n",
+        "#                 # Train the agent\n",
+        "#                 train_rewards = agent.train()\n",
+        "\n",
+        "#                 # Log final performance metrics\n",
+        "#                 run[\"results/final_reward\"] = train_rewards[-1]\n"
,
+        "#                 run[\"results/mean_last_100_reward\"] = np.mean(trai
n_rewards[-100:])\n",
+        "#                 run[\"results/best_reward\"] = np.max(train_rewards)
\n",
+        "#                 run[\"results/baseline_reward\"]= evaluate_baseline_
reward(network_file=params[\"network_file\"], env=env, agent=agent)\n",
                                                        
+        "\n",
+        "#             except Exception as e:\n",
+        "#                 print(f\"Error in run: {e}\")\n",
+        "#                 run[\"results/error\"] = str(e)\n",
+        "\n",
+        "#             # Close the Neptune run\n",
+        "#             run.stop()\n",
+        "\n",
+        "#             # Small delay to avoid API rate limits\n",
+        "#             time.sleep(1)"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 20,
+      "id": "99a34ee8",
+      "metadata": {
+        "id": "99a34ee8",
+        "colab": {
+          "base_uri": "https://localhost:8080/",
+          "height": 659
+        },
+        "outputId": "bb2c59c9-a1f2-4e7a-990e-17654a720a09"
+      },
+      "outputs": [
+        {
+          "metadata": {
+            "tags": null
+          },
+          "name": "stderr",
+          "output_type": "stream",
+          "text": [
+            "WARNING:pypsa.network.io:Importing network from PyPSA version v0.0
.0 while current version is v0.35.1. Read the release notes at https://pypsa.rea
dthedocs.io/en/latest/release_notes.html to prepare your network for import.\n",

    
+            "WARNING:pypsa.consistency:The following lines have carriers which
are not defined:\n",
+            "Index(['0', '1', '2', '3', '4', '5', '6'], dtype='object', name='L
ine')\n",
+            "WARNING:pypsa.consistency:The following buses have carriers which
are not defined:\n",
+            "Index(['ZA0 0', 'ZA0 1', 'ZA0 2', 'ZA0 3', 'ZA0 4', 'ZA1 0', 'ZA2
0', 'ZA3 0'], dtype='object', name='Bus')\n",
                                                        
+            "Writing constraints.: 100%|\u001b[38;2;128;191;255m██████████\u001
b[0m| 13/13 [00:01<00:00, 10.09it/s]\n",
                                                        
+            "Writing continuous variables.: 100%|\u001b[38;2;128;191;255m██████
████\u001b[0m| 6/6 [00:00<00:00, 21.45it/s]\n",
                                                        
+            "WARNING:linopy.constants:Optimization potentially failed: \n",
+            "Status: warning\n",
+            "Termination condition: infeasible\n",
+            "Solution: 0 primals, 0 duals\n",
+            "Objective: nan\n",
+            "Solver model: available\n",
+            "Solver message: Infeasible\n",
+            "\n",
+            "WARNING:pypsa.consistency:The following sub_networks have carriers
 which are not defined:\n",
+            "Index(['0'], dtype='object', name='SubNetwork')\n",
+            "WARNING:pypsa.consistency:The following lines have carriers which
are not defined:\n",
+            "Index(['0', '1', '2', '3', '4', '5', '6'], dtype='object', name='L
ine')\n",
+            "WARNING:pypsa.consistency:The following buses have carriers which
are not defined:\n",
+            "Index(['ZA0 0', 'ZA0 1', 'ZA0 2', 'ZA0 3', 'ZA0 4', 'ZA1 0', 'ZA2
0', 'ZA3 0'], dtype='object', name='Bus')\n"
                                                        
+          ]
+        },
+        {
+          "output_type": "error",
+          "ename": "UnboundLocalError",
+          "evalue": "cannot access local variable 'min_load' where it is not as
sociated with a value",
+          "traceback": [
+            "\u001b[0;31m------------------------------------------------------
---------------------\u001b[0m",
                                                        
+            "\u001b[0;31mUnboundLocalError\u001b[0m                         Tra
ceback (most recent call last)",
                                                        
+            "\u001b[0;32m/tmp/ipython-input-3330791089.py\u001b[0m in \u001b[0;
36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b
[0mrun_sweep_replacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u0
01b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
    
+            "\u001b[0;32m/tmp/ipython-input-1102575295.py\u001b[0m in \u001b[0;
36mrun_sweep_replacement\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m
  }\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--
-> 31\u001b[0;31m     \u001b[0mreplacement_env\u001b[0m\u001b[0;34m=\u001b[0m\u0
01b[0mEnv2Gen1LoadConstrReplacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwo
rk_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_params\u001b[0m\u001b[0;34m[
\u001b[0m\u001b[0;34m\"network_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m
)\u001b[0m \u001b[0;31m#this is used to calculate the offset k for the replaceme
nt reward method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u
001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m
     33\u001b[0m     \u001b[0;31m# Seeds to use for each configuration\u001b[0m\
u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            
+            "\u001b[0;32m/tmp/ipython-input-237244066.py\u001b[0m in \u001b[0;3
6m__init__\u001b[0;34m(self, network_file, episode_length, constraint_penalty_fa
ctor, offset_k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u00
1b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init
__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u0
01b[0mnetwork_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[
0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0
mconstraint_penalty_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0
m\u001b[0;34m,\u001b[0m \u001b[0moffset_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0
;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[
0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b
[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b
[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_file\u001b[0
m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m=\u001b[0m\u
001b[0mepisode_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint_penalty
_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint_penalty_factor\u001b[0
m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u00
1b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u0
01b[0m\u001b[0moffset_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset_k\u001b[
0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b
[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                                                
+            "\u001b[0;32m/tmp/ipython-input-416271505.py\u001b[0m in \u001b[0;3
6m__init__\u001b[0;34m(self, network_file, episode_length, constraint_penalty_fa
ctor)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u0
01b[1;32m     85\u001b[0m         \u001b[0;31m# Create observation space\u001b[0
m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[
0;31m         \u001b[0mlow_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh_b
ounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[
0m\u001b[0mcreate_observation_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)
\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;
32m     87\u001b[0m         self.observation_space = spaces.Box(\n\u001b[1;32m
   88\u001b[0m             \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml
ow_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[
0m\u001b[0m\n",
                                        
+            "\u001b[0;32m/tmp/ipython-input-416271505.py\u001b[0m in \u001b[0;3
6mcreate_observation_bounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001
b[0m                 \u001b[0mmax_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m
self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[
0m\u001b[0mloads_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m
.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u00
1b[0;34m.\u001b[0m\u001b[0msnapshot_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;3
4m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u0
01b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \
u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001
b[0mload_p_lower\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;
34m(\u001b[0m\u001b[0mmin_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[
0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m
    \u001b[0mload_p_upper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m
\u001b[0;34m(\u001b[0m\u001b[0mmax_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;3
4m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0
;34m\u001b[0m\u001b[0m\n",
                                            
+            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variab
le 'min_load' where it is not associated with a value"
                                                        
+          ]
+        }
+      ],
+      "source": [
+        "run_sweep_replacement()"
+      ]
+    }
+  ],
+  "metadata": {
+    "colab": {
+      "machine_shape": "hm",
+      "provenance": []
+    },
+    "kernelspec": {
+      "display_name": "Python 3",
+      "name": "python3"
+    },
+    "language_info": {
+      "codemirror_mode": {
+        "name": "ipython",
+        "version": 3
+      },
+      "file_extension": ".py",
+      "mimetype": "text/x-python",
+      "name": "python",
+      "nbconvert_exporter": "python",
+      "pygments_lexer": "ipython3",
+      "version": "3.10.17"
+    }
+  },
+  "nbformat": 4,
+  "nbformat_minor": 5
+}
\ No newline at end of file
```

---



---

